{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Earth Engine Data Preparation for FuseTS\n",
    "\n",
    "This notebook extracts Sentinel-1 and Sentinel-2 data from Google Earth Engine and prepares it for FuseTS MOGPR processing.\n",
    "\n",
    "## Temporal Compositing Strategy\n",
    "- **Total periods**: 31 periods from Nov 2024 - Oct 2025\n",
    "- **Period length**: 12 days each\n",
    "- **Start date**: November 1, 2024\n",
    "- **End date**: October 31, 2025\n",
    "- **Period 1**: Nov 1-12, 2024\n",
    "- **Period 2**: Nov 13-24, 2024  \n",
    "- **Period 3**: Nov 25 - Dec 6, 2024\n",
    "- **... and so on**\n",
    "\n",
    "## Indonesian Agricultural Calendar Coverage\n",
    "This date range perfectly captures:\n",
    "- **First planting season**: Nov 2024 - Mar 2025 (crosses year boundary)\n",
    "- **Second planting season**: Apr - Jun 2025\n",
    "- **Third planting season**: Jul - Sep 2025 (optional)\n",
    "- **Full cycle**: Complete agricultural year\n",
    "\n",
    "## Output Format\n",
    "Data will be exported in FuseTS-compatible xarray format with proper band naming:\n",
    "- S1: `VV`, `VH` bands\n",
    "- S2: `S2ndvi` band\n",
    "- Dimensions: `(time, y, x)` with `t` coordinate name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio earthengine-api geemap pandas numpy xarray matplotlib geopandas shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ee.Authenticate()\n",
    "ee.Initialize(project='ee-geodeticengineeringundip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Additional imports for mask processing\n",
    "import rasterio\n",
    "from rasterio.features import shapes\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# Initialize Earth Engine with authentication\n",
    "print(\"üîê Authenticating with Google Earth Engine...\")\n",
    "\n",
    "try:\n",
    "    # First time setup: authenticate\n",
    "    ee.Authenticate()\n",
    "    print(\"‚úÖ Authentication successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication note: {e}\")\n",
    "    print(\"If already authenticated, continuing...\")\n",
    "\n",
    "# Initialize with project\n",
    "try:\n",
    "    ee.Initialize(project='ee-geodeticengineeringundip')\n",
    "    print(\"‚úÖ Earth Engine initialized successfully!\")\n",
    "    print(f\"   Project: ee-geodeticengineeringundip\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing Earth Engine: {e}\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(\"  1. You have run ee.Authenticate() successfully\")\n",
    "    print(\"  2. You have access to project 'ee-geodeticengineeringundip'\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nüì¶ Package versions:\")\n",
    "print(f\"   Earth Engine API: {ee.__version__}\")\n",
    "print(f\"   geemap: {geemap.__version__}\")\n",
    "print(f\"   rasterio: {rasterio.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Study Area and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STUDY AREA SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "# Choose your study area:\n",
    "STUDY_AREA_TYPE = 'demak'  # Options: 'java_island' or 'demak'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìç STUDY AREA CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if STUDY_AREA_TYPE == 'demak':\n",
    "    # ========================================================================\n",
    "    # OPTION 1: KABUPATEN DEMAK (Small area - faster processing)\n",
    "    # ========================================================================\n",
    "    print(\"\\nüéØ Using Kabupaten Demak, Central Java\")\n",
    "    \n",
    "    # Demak administrative boundary (approximate coordinates)\n",
    "    # You can adjust these based on your specific area of interest\n",
    "    demak_bounds = {\n",
    "        'west': 110.35,   # Western boundary\n",
    "        'east': 110.75,   # Eastern boundary  \n",
    "        'south': -7.05,   # Southern boundary\n",
    "        'north': -6.75    # Northern boundary\n",
    "    }\n",
    "    \n",
    "    # Create rectangle geometry for Demak\n",
    "    study_area = ee.Geometry.Rectangle([\n",
    "        demak_bounds['west'], \n",
    "        demak_bounds['south'],\n",
    "        demak_bounds['east'], \n",
    "        demak_bounds['north']\n",
    "    ])\n",
    "    \n",
    "    # Alternative: Use GEE administrative boundaries (more accurate)\n",
    "    # Uncomment these lines to use official boundaries:\n",
    "    # admin_boundaries = ee.FeatureCollection(\"FAO/GAUL/2015/level2\")\n",
    "    # demak = admin_boundaries.filter(ee.Filter.eq('ADM2_NAME', 'Demak'))\n",
    "    # study_area = demak.geometry()\n",
    "    \n",
    "    print(f\"   Type: Administrative boundary (regency/kabupaten)\")\n",
    "    print(f\"   Location: Central Java Province\")\n",
    "    print(f\"   Approximate area: ~900 km¬≤\")\n",
    "    print(f\"   Bounds: {demak_bounds}\")\n",
    "    print(f\"   ‚úÖ Much smaller than Java Island ‚Üí faster export!\")\n",
    "    \n",
    "elif STUDY_AREA_TYPE == 'java_island':\n",
    "    # ========================================================================\n",
    "    # OPTION 2: FULL JAVA ISLAND (Large area - requires more storage)\n",
    "    # ========================================================================\n",
    "    print(\"\\nüèùÔ∏è  Using Full Java Island\")\n",
    "    \n",
    "    import rasterio\n",
    "    from rasterio.features import shapes\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import shape, mapping\n",
    "    \n",
    "    # Path to Java Island mask\n",
    "    MASK_FILE = 'java_island_mask.tif'\n",
    "    \n",
    "    print(f\"   Loading Java Island mask from: {MASK_FILE}\")\n",
    "    \n",
    "    # Read the mask file and extract geometry\n",
    "    with rasterio.open(MASK_FILE) as src:\n",
    "        # Read the mask (assuming mask values > 0 indicate valid areas)\n",
    "        mask_data = src.read(1)\n",
    "        mask_transform = src.transform\n",
    "        mask_crs = src.crs\n",
    "        \n",
    "        # Get bounds\n",
    "        bounds = src.bounds\n",
    "        print(f\"   Mask bounds: {bounds}\")\n",
    "        print(f\"   Mask CRS: {mask_crs}\")\n",
    "        print(f\"   Mask shape: {mask_data.shape}\")\n",
    "        \n",
    "        # Extract geometry from mask (vectorize the raster mask)\n",
    "        mask_geoms = []\n",
    "        for geom, val in shapes(mask_data, mask=mask_data > 0, transform=mask_transform):\n",
    "            mask_geoms.append(shape(geom))\n",
    "    \n",
    "    # Create a unified geometry for Java Island\n",
    "    if len(mask_geoms) > 0:\n",
    "        from shapely.ops import unary_union\n",
    "        java_geometry = unary_union(mask_geoms)\n",
    "        \n",
    "        # Add 5 km buffer to the Java Island geometry\n",
    "        BUFFER_DISTANCE_KM = 5\n",
    "        BUFFER_DISTANCE_DEGREES = BUFFER_DISTANCE_KM / 111.0  # Approximate conversion (1 degree ‚âà 111 km)\n",
    "        \n",
    "        print(f\"   Applying {BUFFER_DISTANCE_KM} km buffer to Java Island mask...\")\n",
    "        java_geometry_buffered = java_geometry.buffer(BUFFER_DISTANCE_DEGREES)\n",
    "        \n",
    "        # Convert to GeoJSON format for Earth Engine\n",
    "        java_geojson = mapping(java_geometry_buffered)\n",
    "        \n",
    "        # Upload to Earth Engine\n",
    "        study_area = ee.Geometry(java_geojson)\n",
    "        \n",
    "        print(f\"   ‚úÖ Java Island mask loaded successfully!\")\n",
    "        print(f\"   Number of geometries merged: {len(mask_geoms)}\")\n",
    "        print(f\"   Buffer applied: {BUFFER_DISTANCE_KM} km\")\n",
    "        print(f\"   Approximate area: ~150,000 km¬≤\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No valid mask areas found, falling back to bounding box\")\n",
    "        study_area = ee.Geometry.Rectangle([bounds.left, bounds.bottom, bounds.right, bounds.top])\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Invalid STUDY_AREA_TYPE: {STUDY_AREA_TYPE}. Use 'demak' or 'java_island'\")\n",
    "\n",
    "# Processing parameters\n",
    "START_DATE = '2024-11-01'  # November 1, 2024\n",
    "END_DATE = '2025-10-31'    # October 31, 2025\n",
    "SCALE = 50  # meters per pixel (50m resolution for both S1 and S2)\n",
    "CRS = 'EPSG:4326'  # WGS84 coordinate system\n",
    "MAX_CLOUD_COVER = 20  # Maximum cloud cover percentage for S2\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = 'gee_fusets_data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Display final configuration\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üìã FINAL CONFIGURATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"   Study Area: {STUDY_AREA_TYPE.upper()}\")\n",
    "print(f\"   Bounds: {study_area.bounds().getInfo()}\")\n",
    "print(f\"   Area size: {study_area.area().getInfo() / 1e6:.1f} km¬≤\")\n",
    "print(f\"   Processing period: {START_DATE} to {END_DATE}\")\n",
    "print(f\"   Temporal resolution: 12-day composites (31 periods)\")\n",
    "print(f\"   Spatial resolution: {SCALE}m\")\n",
    "print(f\"   Coordinate system: {CRS}\")\n",
    "print(f\"   Max cloud cover: {MAX_CLOUD_COVER}%\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Estimate data size\n",
    "area_km2 = study_area.area().getInfo() / 1e6\n",
    "pixels_per_period = (area_km2 * 1e6) / (SCALE * SCALE)  # Total pixels\n",
    "bands = 3  # VV, VH, S2ndvi\n",
    "bytes_per_pixel = 4  # Float32\n",
    "total_size_gb = (pixels_per_period * bands * bytes_per_pixel * 31) / 1e9\n",
    "\n",
    "print(f\"\\nüíæ Estimated data size:\")\n",
    "print(f\"   Per period: ~{total_size_gb/31:.2f} GB\")\n",
    "print(f\"   Total (31 periods): ~{total_size_gb:.1f} GB\")\n",
    "\n",
    "if total_size_gb > 250:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  WARNING: Exceeds GEE Asset quota (250GB)\")\n",
    "    print(f\"   ‚Üí Use Google Drive export instead\")\n",
    "elif total_size_gb > 100:\n",
    "    print(f\"\\n   ‚ö° Large dataset - GEE Assets recommended\")\n",
    "else:\n",
    "    print(f\"\\n   ‚úÖ Manageable size - Google Drive or Assets both work\")\n",
    "\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Load Paddy Shapefile Mask (Klambu-Glapan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD PADDY SHAPEFILE AND CREATE STUDY AREA FROM IT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìç LOADING PADDY SHAPEFILE MASK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the shapefile\n",
    "shapefile_path = 'data/klambu-glapan.shp'\n",
    "\n",
    "try:\n",
    "    paddy_gdf = gpd.read_file(shapefile_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Shapefile loaded successfully!\")\n",
    "    print(f\"   File: {shapefile_path}\")\n",
    "    print(f\"   Number of features: {len(paddy_gdf)}\")\n",
    "    print(f\"   CRS: {paddy_gdf.crs}\")\n",
    "    print(f\"   Total area: {paddy_gdf.area.sum() / 1e6:.2f} km¬≤\")\n",
    "    \n",
    "    # Get bounds in original CRS\n",
    "    minx, miny, maxx, maxy = paddy_gdf.total_bounds\n",
    "    print(f\"\\n   Original CRS Bounds:\")\n",
    "    print(f\"     West (MinX):  {minx:.2f}\")\n",
    "    print(f\"     South (MinY): {miny:.2f}\")\n",
    "    print(f\"     East (MaxX):  {maxx:.2f}\")\n",
    "    print(f\"     North (MaxY): {maxy:.2f}\")\n",
    "    print(f\"     Width:  {(maxx - minx):.2f} m ({(maxx - minx)/1000:.2f} km)\")\n",
    "    print(f\"     Height: {(maxy - miny):.2f} m ({(maxy - miny)/1000:.2f} km)\")\n",
    "    \n",
    "    # Add buffer around shapefile (500m buffer)\n",
    "    buffer_m = 500\n",
    "    print(f\"\\n   Applying {buffer_m}m buffer to paddy areas...\")\n",
    "    \n",
    "    # Buffer in the original CRS (should be meters)\n",
    "    paddy_buffered = paddy_gdf.copy()\n",
    "    paddy_buffered['geometry'] = paddy_gdf.buffer(buffer_m)\n",
    "    \n",
    "    # Convert to WGS84 (EPSG:4326) for GEE\n",
    "    paddy_wgs84 = paddy_buffered.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get bounds in WGS84\n",
    "    west, south, east, north = paddy_wgs84.total_bounds\n",
    "    \n",
    "    print(f\"\\n   WGS84 Bounds (for GEE):\")\n",
    "    print(f\"     West:  {west:.6f}¬∞\")\n",
    "    print(f\"     South: {south:.6f}¬∞\")\n",
    "    print(f\"     East:  {east:.6f}¬∞\")\n",
    "    print(f\"     North: {north:.6f}¬∞\")\n",
    "    \n",
    "    # Create GEE geometry from the buffered shapefile\n",
    "    # Convert to GeoJSON and upload to Earth Engine\n",
    "    from shapely.ops import unary_union\n",
    "    \n",
    "    # Merge all polygons into a single geometry\n",
    "    merged_geometry = unary_union(paddy_wgs84.geometry)\n",
    "    \n",
    "    # Convert to GeoJSON\n",
    "    paddy_geojson = mapping(merged_geometry)\n",
    "    \n",
    "    # Upload to Earth Engine\n",
    "    study_area = ee.Geometry(paddy_geojson)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Study area created from paddy shapefile!\")\n",
    "    print(f\"   Type: Paddy field boundaries with {buffer_m}m buffer\")\n",
    "    print(f\"   Location: Klambu-Glapan, Demak, Central Java\")\n",
    "    print(f\"   Area (GEE): {study_area.area().getInfo() / 1e6:.2f} km¬≤\")\n",
    "    \n",
    "    # Visualize the shapefile and buffer\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Plot 1: Original shapefile\n",
    "    paddy_gdf.plot(ax=axes[0], facecolor='lightgreen', edgecolor='darkgreen', linewidth=1.5, alpha=0.7)\n",
    "    axes[0].set_title('Original Paddy Shapefile\\n(Klambu-Glapan)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Easting (m)')\n",
    "    axes[0].set_ylabel('Northing (m)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: With buffer\n",
    "    paddy_buffered.plot(ax=axes[1], facecolor='yellow', edgecolor='orange', linewidth=1.5, alpha=0.5, label=f'{buffer_m}m buffer')\n",
    "    paddy_gdf.plot(ax=axes[1], facecolor='lightgreen', edgecolor='darkgreen', linewidth=1.5, alpha=0.7, label='Paddy areas')\n",
    "    axes[1].set_title(f'Paddy Areas with {buffer_m}m Buffer\\n(Study Area for GEE Download)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Easting (m)')\n",
    "    axes[1].set_ylabel('Northing (m)')\n",
    "    axes[1].legend(loc='best')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('paddy_shapefile_study_area.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n   Visualization saved: paddy_shapefile_study_area.png\")\n",
    "    \n",
    "    # Override the study area type\n",
    "    STUDY_AREA_TYPE = 'paddy_shapefile'\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå Shapefile not found: {shapefile_path}\")\n",
    "    print(f\"   Please ensure the shapefile exists in the data/ folder\")\n",
    "    print(f\"   Falling back to Demak bounding box...\")\n",
    "    \n",
    "    # Fall back to Demak bounds if shapefile not found\n",
    "    STUDY_AREA_TYPE = 'demak'\n",
    "    demak_bounds = {\n",
    "        'west': 110.35,\n",
    "        'east': 110.75,\n",
    "        'south': -7.05,\n",
    "        'north': -6.75\n",
    "    }\n",
    "    study_area = ee.Geometry.Rectangle([\n",
    "        demak_bounds['west'], \n",
    "        demak_bounds['south'],\n",
    "        demak_bounds['east'], \n",
    "        demak_bounds['north']\n",
    "    ])\n",
    "    west, south, east, north = demak_bounds['west'], demak_bounds['south'], demak_bounds['east'], demak_bounds['north']\n",
    "\n",
    "# Processing parameters\n",
    "START_DATE = '2023-11-01'  # November 1, 2023\n",
    "END_DATE = '2025-11-07'    # November 7, 2025\n",
    "SCALE = 10  # meters per pixel (10m resolution - native S2 resolution)\n",
    "CRS = 'EPSG:4326'  # WGS84 coordinate system\n",
    "MAX_CLOUD_COVER = 80  # Maximum cloud cover percentage for S2 (relaxed for better coverage)\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = 'gee_fusets_data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Display final configuration\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üìã FINAL CONFIGURATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"   Study Area: {STUDY_AREA_TYPE.upper()}\")\n",
    "print(f\"   Bounds: W={west:.6f}¬∞, S={south:.6f}¬∞, E={east:.6f}¬∞, N={north:.6f}¬∞\")\n",
    "print(f\"   Area size: {study_area.area().getInfo() / 1e6:.2f} km¬≤\")\n",
    "print(f\"   Processing period: {START_DATE} to {END_DATE}\")\n",
    "print(f\"   Temporal resolution: 12-day composites\")\n",
    "print(f\"   Spatial resolution: {SCALE}m\")\n",
    "print(f\"   Coordinate system: {CRS}\")\n",
    "print(f\"   Max cloud cover: {MAX_CLOUD_COVER}%\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Estimate data size\n",
    "area_km2 = study_area.area().getInfo() / 1e6\n",
    "pixels_per_period = (area_km2 * 1e6) / (SCALE * SCALE)  # Total pixels\n",
    "bands = 3  # VV, VH, S2ndvi\n",
    "bytes_per_pixel = 4  # Float32\n",
    "\n",
    "# Calculate number of periods from Nov 2023 to Nov 2025\n",
    "from datetime import datetime\n",
    "start = datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "end = datetime.strptime(END_DATE, '%Y-%m-%d')\n",
    "days = (end - start).days\n",
    "periods_count = int(np.ceil(days / 12))\n",
    "\n",
    "total_size_gb = (pixels_per_period * bands * bytes_per_pixel * periods_count) / 1e9\n",
    "\n",
    "print(f\"\\nüíæ Estimated data size:\")\n",
    "print(f\"   Number of periods: {periods_count}\")\n",
    "print(f\"   Per period: ~{total_size_gb/periods_count:.2f} GB\")\n",
    "print(f\"   Total ({periods_count} periods): ~{total_size_gb:.1f} GB\")\n",
    "\n",
    "if total_size_gb > 250:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  WARNING: Exceeds GEE Asset quota (250GB)\")\n",
    "    print(f\"   ‚Üí Use Google Drive export instead\")\n",
    "elif total_size_gb > 100:\n",
    "    print(f\"\\n   ‚ö° Large dataset - GEE Assets recommended\")\n",
    "else:\n",
    "    print(f\"\\n   ‚úÖ Manageable size - Google Drive or Assets both work\")\n",
    "\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate 12-Day Composite Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_12day_periods(start_date_str, end_date_str):\n",
    "    \"\"\"\n",
    "    Generate periods of 12 days each from start date to end date\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date_str : str\n",
    "        Start date in 'YYYY-MM-DD' format (e.g., '2023-11-01')\n",
    "    end_date_str : str\n",
    "        End date in 'YYYY-MM-DD' format (e.g., '2025-11-07')\n",
    "    \"\"\"\n",
    "    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
    "    \n",
    "    periods = []\n",
    "    period_num = 1\n",
    "    current_start = start_date\n",
    "    \n",
    "    while current_start <= end_date:\n",
    "        period_end = current_start + timedelta(days=11)  # 12 days inclusive\n",
    "        \n",
    "        # Ensure we don't go beyond the end date\n",
    "        if period_end > end_date:\n",
    "            period_end = end_date\n",
    "            \n",
    "        periods.append({\n",
    "            'period': period_num,\n",
    "            'start_date': current_start,\n",
    "            'end_date': period_end,\n",
    "            'start_str': current_start.strftime('%Y-%m-%d'),\n",
    "            'end_str': period_end.strftime('%Y-%m-%d'),\n",
    "            'center_date': current_start + timedelta(days=6),  # Middle of period\n",
    "            'doy_center': (current_start + timedelta(days=6)).timetuple().tm_yday,\n",
    "            'year': current_start.year,\n",
    "            'month': current_start.month\n",
    "        })\n",
    "        \n",
    "        if period_end >= end_date:\n",
    "            break\n",
    "        \n",
    "        current_start = period_end + timedelta(days=1)  # Start next period\n",
    "        period_num += 1\n",
    "            \n",
    "    return periods\n",
    "\n",
    "# Generate periods from Nov 2023 to Nov 2025\n",
    "periods = generate_12day_periods(START_DATE, END_DATE)\n",
    "\n",
    "print(f\"Generated {len(periods)} periods from {START_DATE} to {END_DATE}:\")\n",
    "print(\"\\nFirst 5 periods:\")\n",
    "for i, period in enumerate(periods[:5]):\n",
    "    print(f\"Period {period['period']:2d}: {period['start_str']} to {period['end_str']} (center: DOY {period['doy_center']:3d}, {period['year']})\")\n",
    "\n",
    "print(\"\\nPeriods covering 2-year span (Nov 2023 - Nov 2025):\")\n",
    "print(f\"  ‚Ä¢ 2023 periods: {len([p for p in periods if p['year'] == 2023])}\")\n",
    "print(f\"  ‚Ä¢ 2024 periods: {len([p for p in periods if p['year'] == 2024])}\")\n",
    "print(f\"  ‚Ä¢ 2025 periods: {len([p for p in periods if p['year'] == 2025])}\")\n",
    "\n",
    "print(\"\\nYear boundary crossings:\")\n",
    "year_boundary_periods = [p for p in periods if p['start_date'].year != p['end_date'].year]\n",
    "for period in year_boundary_periods:\n",
    "    print(f\"Period {period['period']:2d}: {period['start_str']} to {period['end_str']} ‚Üê CROSSES YEAR BOUNDARY\")\n",
    "\n",
    "print(\"\\nLast 5 periods:\")\n",
    "for i, period in enumerate(periods[-5:]):\n",
    "    print(f\"Period {period['period']:2d}: {period['start_str']} to {period['end_str']} (center: DOY {period['doy_center']:3d}, {period['year']})\")\n",
    "\n",
    "# Create a DataFrame for easier handling\n",
    "periods_df = pd.DataFrame(periods)\n",
    "print(f\"\\nTotal temporal coverage: {periods[0]['start_str']} to {periods[-1]['end_str']}\")\n",
    "print(f\"Covers {len(periods)} 12-day periods over 2 years\")\n",
    "print(f\"\\nIndonesian agricultural seasons covered:\")\n",
    "print(f\"  ‚Ä¢ 2023-2024 cycle: Nov 2023 - Oct 2024 (full year)\")\n",
    "print(f\"  ‚Ä¢ 2024-2025 cycle: Nov 2024 - Nov 2025 (full year + 1 week)\")\n",
    "print(f\"  ‚Ä¢ Total: ~6 growing seasons (3 per year √ó 2 years)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Data Loading Functions\n",
    "\n",
    "**üìå Configuration: Using Sentinel-2 Level-1C (TOA) without cloud masking**\n",
    "- **Coverage**: 99.9% (maximum)\n",
    "- **Trade-off**: TOA reflectance (not atmospherically corrected)\n",
    "- **Rationale**: Best for tropical rainy season (Indonesia Nov-Oct)\n",
    "- **Suitability**: Excellent for MOGPR fusion temporal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentinel1_data(geometry, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Load Sentinel-1 GRD data for a specific time period\n",
    "    \"\"\"\n",
    "    s1_collection = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "                    .filterBounds(geometry)\n",
    "                    .filterDate(start_date, end_date)\n",
    "                    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "                    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "                    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "                    .select(['VV', 'VH']))\n",
    "    \n",
    "    return s1_collection\n",
    "\n",
    "def load_sentinel2_data(geometry, start_date, end_date, max_cloud_cover=60):\n",
    "    \"\"\"\n",
    "    Load Sentinel-2 Level-1C (TOA) data without cloud masking\n",
    "    \n",
    "    ‚ö†Ô∏è  IMPORTANT TRADE-OFFS:\n",
    "    ‚úÖ Pros:\n",
    "       ‚Ä¢ Maximum coverage (99.9%)\n",
    "       ‚Ä¢ No data loss from cloud masking\n",
    "       ‚Ä¢ Works well in tropical rainy season\n",
    "    \n",
    "    ‚ùå Cons:\n",
    "       ‚Ä¢ NOT atmospherically corrected (TOA reflectance)\n",
    "       ‚Ä¢ May include some cloudy pixels\n",
    "       ‚Ä¢ NDVI values affected by atmosphere\n",
    "       ‚Ä¢ Suitable for temporal analysis but absolute values less accurate\n",
    "    \n",
    "    Collection: COPERNICUS/S2 (Level-1C TOA, not Level-2A SR)\n",
    "    \"\"\"\n",
    "    def calculate_ndvi_toa(image):\n",
    "        # B8 = NIR, B4 = Red (same as Level-2A)\n",
    "        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "        return image.addBands(ndvi)\n",
    "    \n",
    "    # Load Level-1C TOA data WITHOUT cloud masking\n",
    "    s2_collection = (ee.ImageCollection('COPERNICUS/S2')  # Note: S2, not S2_SR\n",
    "                    .filterBounds(geometry)\n",
    "                    .filterDate(start_date, end_date)\n",
    "                    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', max_cloud_cover))\n",
    "                    .map(calculate_ndvi_toa)\n",
    "                    .select(['NDVI']))\n",
    "    \n",
    "    return s2_collection\n",
    "\n",
    "def create_composite(collection, method='median'):\n",
    "    \"\"\"\n",
    "    Create a composite from an image collection\n",
    "    \"\"\"\n",
    "    if method == 'median':\n",
    "        return collection.median()\n",
    "    elif method == 'mean':\n",
    "        return collection.mean()\n",
    "    elif method == 'max':\n",
    "        return collection.max()\n",
    "    else:\n",
    "        return collection.median()\n",
    "\n",
    "print(\"Data loading functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Data for All Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_period(period_info, geometry, scale=10):\n",
    "    \"\"\"\n",
    "    Process S1 and S2 data for a single 12-day period\n",
    "    \"\"\"\n",
    "    start_date = period_info['start_str']\n",
    "    end_date = period_info['end_str']\n",
    "    period_num = period_info['period']\n",
    "    \n",
    "    print(f\"Processing Period {period_num}: {start_date} to {end_date}\")\n",
    "    \n",
    "    try:\n",
    "        # Load Sentinel-1 data\n",
    "        s1_collection = load_sentinel1_data(geometry, start_date, end_date)\n",
    "        s1_count = s1_collection.size().getInfo()\n",
    "        \n",
    "        # Load Sentinel-2 data\n",
    "        s2_collection = load_sentinel2_data(geometry, start_date, end_date, MAX_CLOUD_COVER)\n",
    "        s2_count = s2_collection.size().getInfo()\n",
    "        \n",
    "        print(f\"  Found {s1_count} S1 images, {s2_count} S2 images\")\n",
    "        \n",
    "        # Create composites\n",
    "        if s1_count > 0:\n",
    "            s1_composite = create_composite(s1_collection, 'median')\n",
    "        else:\n",
    "            # Create empty image with correct bands\n",
    "            s1_composite = ee.Image.constant([0, 0]).rename(['VV', 'VH']).updateMask(ee.Image.constant(0))\n",
    "            \n",
    "        if s2_count > 0:\n",
    "            s2_composite = create_composite(s2_collection, 'median')\n",
    "        else:\n",
    "            # Create empty NDVI image\n",
    "            s2_composite = ee.Image.constant(0).rename('NDVI').updateMask(ee.Image.constant(0))\n",
    "        \n",
    "        # Combine S1 and S2 data\n",
    "        combined_image = s1_composite.addBands(s2_composite.rename('S2ndvi'))\n",
    "        \n",
    "        # Add metadata\n",
    "        combined_image = combined_image.set({\n",
    "            'period': period_num,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'center_date': period_info['center_date'].strftime('%Y-%m-%d'),\n",
    "            'doy_center': period_info['doy_center'],\n",
    "            's1_count': s1_count,\n",
    "            's2_count': s2_count\n",
    "        })\n",
    "        \n",
    "        return combined_image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing period {period_num}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process all periods\n",
    "print(\"Starting data processing for all periods...\\n\")\n",
    "\n",
    "processed_images = []\n",
    "successful_periods = []\n",
    "\n",
    "for i, period in enumerate(periods):\n",
    "    result = process_single_period(period, study_area, SCALE)\n",
    "    if result is not None:\n",
    "        processed_images.append(result)\n",
    "        successful_periods.append(period)\n",
    "    \n",
    "    # Progress update every 5 periods\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Completed {i + 1}/{len(periods)} periods\\n\")\n",
    "\n",
    "print(f\"Successfully processed {len(processed_images)} out of {len(periods)} periods\")\n",
    "\n",
    "# Create ImageCollection from processed images\n",
    "if processed_images:\n",
    "    time_series_collection = ee.ImageCollection(processed_images)\n",
    "    print(f\"Created time series collection with {time_series_collection.size().getInfo()} images\")\n",
    "else:\n",
    "    print(\"No images were successfully processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Data from GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_timeseries_to_drive(collection, geometry, scale, output_name):\n",
    "    \"\"\"\n",
    "    Export the time series collection to Google Drive as a multi-band image\n",
    "    \"\"\"\n",
    "    # Convert collection to multi-band image\n",
    "    # Each period becomes a separate set of bands\n",
    "    image_list = collection.toList(collection.size())\n",
    "    \n",
    "    def rename_bands_with_period(image):\n",
    "        image = ee.Image(image)\n",
    "        period = ee.Number(image.get('period')).format('%02d')\n",
    "        \n",
    "        # Rename bands to include period number\n",
    "        old_names = image.bandNames()\n",
    "        new_names = old_names.map(lambda name: ee.String(name).cat('_P').cat(period))\n",
    "        \n",
    "        return image.rename(new_names)\n",
    "    \n",
    "    # Rename bands with period numbers\n",
    "    renamed_collection = collection.map(rename_bands_with_period)\n",
    "    \n",
    "    # Convert to single multi-band image\n",
    "    multi_band_image = renamed_collection.toBands()\n",
    "    \n",
    "    # Export task\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "        image=multi_band_image,\n",
    "        description=output_name,\n",
    "        folder='GEE_FuseTS_Data',\n",
    "        fileNamePrefix=output_name,\n",
    "        scale=scale,\n",
    "        region=geometry,\n",
    "        maxPixels=1e9,\n",
    "        crs='EPSG:4326',\n",
    "        fileFormat='GeoTIFF'\n",
    "    )\n",
    "    \n",
    "    return task\n",
    "\n",
    "def export_individual_periods_to_drive(collection, geometry, scale, base_name):\n",
    "    \"\"\"\n",
    "    Export each period as a separate GeoTIFF file to Google Drive\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    image_list = collection.toList(collection.size())\n",
    "    \n",
    "    for i in range(len(successful_periods)):\n",
    "        image = ee.Image(image_list.get(i))\n",
    "        period_num = successful_periods[i]['period']\n",
    "        \n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=image,\n",
    "            description=f'{base_name}_Period_{period_num:02d}',\n",
    "            folder='GEE_FuseTS_Data',\n",
    "            fileNamePrefix=f'{base_name}_Period_{period_num:02d}',\n",
    "            scale=scale,\n",
    "            region=geometry,\n",
    "            maxPixels=1e9,\n",
    "            crs='EPSG:4326',\n",
    "            fileFormat='GeoTIFF'\n",
    "        )\n",
    "        \n",
    "        tasks.append(task)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "# ============================================================================\n",
    "# NEW: GEE ASSETS EXPORT FUNCTIONS (Better for large datasets!)\n",
    "# ============================================================================\n",
    "\n",
    "def export_timeseries_to_asset(collection, geometry, scale, asset_id):\n",
    "    \"\"\"\n",
    "    Export the time series collection to GEE Assets as ImageCollection\n",
    "    \n",
    "    Advantages over Drive export:\n",
    "    - No size limits (up to 10TB per user)\n",
    "    - Data stays in GEE cloud (faster processing)\n",
    "    - Can be used immediately in other GEE scripts\n",
    "    - Better for large study areas\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    asset_id : str\n",
    "        Full path to asset, e.g., 'projects/ee-geodeticengineeringundip/assets/S1_S2_Nov2024_Oct2025'\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    image_list = collection.toList(collection.size())\n",
    "    \n",
    "    for i in range(len(successful_periods)):\n",
    "        image = ee.Image(image_list.get(i))\n",
    "        period_num = successful_periods[i]['period']\n",
    "        period_info = successful_periods[i]\n",
    "        \n",
    "        # Add comprehensive metadata\n",
    "        image_with_metadata = image.set({\n",
    "            'period': period_num,\n",
    "            'start_date': period_info['start_str'],\n",
    "            'end_date': period_info['end_str'],\n",
    "            'center_date': period_info['center_date'].strftime('%Y-%m-%d'),\n",
    "            'doy_center': period_info['doy_center'],\n",
    "            'year': period_info['year'],\n",
    "            'month': period_info['month'],\n",
    "            'system:time_start': ee.Date(period_info['start_str']).millis(),\n",
    "            'system:time_end': ee.Date(period_info['end_str']).millis()\n",
    "        })\n",
    "        \n",
    "        # Create asset ID for this period\n",
    "        period_asset_id = f'{asset_id}_Period_{period_num:02d}'\n",
    "        \n",
    "        task = ee.batch.Export.image.toAsset(\n",
    "            image=image_with_metadata,\n",
    "            description=f'Asset_Period_{period_num:02d}',\n",
    "            assetId=period_asset_id,\n",
    "            scale=scale,\n",
    "            region=geometry,\n",
    "            maxPixels=1e13,  # Higher limit for assets\n",
    "            crs='EPSG:4326',\n",
    "            pyramidingPolicy={'.default': 'mean'}  # Better for time series\n",
    "        )\n",
    "        \n",
    "        tasks.append(task)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "def export_imagecollection_to_asset(collection, asset_id, geometry, scale):\n",
    "    \"\"\"\n",
    "    Export entire ImageCollection to a single GEE Asset\n",
    "    \n",
    "    Note: For very large collections, individual image exports (above function) are more reliable\n",
    "    \"\"\"\n",
    "    # This exports the collection metadata structure\n",
    "    # Individual images still need to be exported separately\n",
    "    print(\"‚ö†Ô∏è  GEE doesn't support direct ImageCollection export.\")\n",
    "    print(\"    Use export_timeseries_to_asset() to export individual images.\")\n",
    "    print(\"    They will form an ImageCollection when all are in the same folder.\")\n",
    "    return None\n",
    "\n",
    "# Choose export method\n",
    "EXPORT_METHOD = 'individual'  # 'combined' or 'individual'\n",
    "EXPORT_DESTINATION = 'drive'  # 'drive' or 'asset' - CHANGED TO 'drive' due to asset quota limit\n",
    "\n",
    "# Your GEE Assets path (update this to your project!)\n",
    "ASSET_BASE_PATH = 'projects/ee-geodeticengineeringundip/assets/FuseTS'\n",
    "\n",
    "print(f\"\\nüì§ EXPORT CONFIGURATION:\")\n",
    "print(f\"   Destination: {EXPORT_DESTINATION.upper()}\")\n",
    "print(f\"   Method: {EXPORT_METHOD}\")\n",
    "if EXPORT_DESTINATION == 'asset':\n",
    "    print(f\"   Asset path: {ASSET_BASE_PATH}\")\n",
    "print(f\"\\nüí° Choose export destination:\")\n",
    "print(f\"   ‚Ä¢ 'drive': Google Drive (good for < 2GB, need to download)\")\n",
    "print(f\"   ‚Ä¢ 'asset': GEE Assets (recommended for large data, stays in cloud)\")\n",
    "\n",
    "if time_series_collection:\n",
    "    if EXPORT_DESTINATION == 'asset':\n",
    "        # ====================================================================\n",
    "        # EXPORT TO GEE ASSETS (Recommended for large datasets!)\n",
    "        # ====================================================================\n",
    "        print(\"\\nüöÄ Exporting to GEE Assets...\")\n",
    "        print(\"   ‚úÖ No size limits (up to 10TB)\")\n",
    "        print(\"   ‚úÖ Data stays in GEE cloud\")\n",
    "        print(\"   ‚úÖ Can use immediately in other scripts\")\n",
    "        \n",
    "        asset_id = f'{ASSET_BASE_PATH}/S1_S2_Nov2024_Oct2025'\n",
    "        \n",
    "        export_tasks = export_timeseries_to_asset(\n",
    "            time_series_collection,\n",
    "            study_area,\n",
    "            SCALE,\n",
    "            asset_id\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìã Starting {len(export_tasks)} asset export tasks...\")\n",
    "        \n",
    "        # Start first 10 tasks (GEE limits concurrent tasks)\n",
    "        for i, task in enumerate(export_tasks[:10]):\n",
    "            task.start()\n",
    "            print(f\"  ‚úÖ Started: Period {i+1:02d} ‚Üí {asset_id}_Period_{i+1:02d}\")\n",
    "        \n",
    "        if len(export_tasks) > 10:\n",
    "            print(f\"\\n‚è≥ Remaining {len(export_tasks) - 10} tasks queued\")\n",
    "            print(\"   Start them manually from: https://code.earthengine.google.com/tasks\")\n",
    "            print(\"   Or run this code to start next batch:\")\n",
    "            print(f\"   for task in export_tasks[10:20]: task.start()\")\n",
    "        \n",
    "        print(f\"\\nüìä After exports complete, load data in GEE with:\")\n",
    "        print(f\"   var collection = ee.ImageCollection('{ASSET_BASE_PATH}/S1_S2_Nov2024_Oct2025_Period_*');\")\n",
    "        \n",
    "    elif EXPORT_DESTINATION == 'drive':\n",
    "        # ====================================================================\n",
    "        # EXPORT TO GOOGLE DRIVE (Original method)\n",
    "        # ====================================================================\n",
    "        if EXPORT_METHOD == 'combined':\n",
    "            # Export as single multi-band file\n",
    "            print(\"\\nüì§ Preparing export as single multi-band GeoTIFF to Google Drive...\")\n",
    "            export_task = export_timeseries_to_drive(\n",
    "                time_series_collection, \n",
    "                study_area, \n",
    "                SCALE, \n",
    "                f'S1_S2_TimeSeries_Nov2024_Oct2025'\n",
    "            )\n",
    "            \n",
    "            print(f\"Starting export task: {export_task.config['description']}\")\n",
    "            export_task.start()\n",
    "            \n",
    "            print(f\"Export task submitted. Monitor progress at: https://code.earthengine.google.com/tasks\")\n",
    "            \n",
    "        else:\n",
    "            # Export individual period files\n",
    "            print(\"\\nüì§ Preparing export as individual period GeoTIFFs to Google Drive...\")\n",
    "            export_tasks = export_individual_periods_to_drive(\n",
    "                time_series_collection,\n",
    "                study_area,\n",
    "                SCALE,\n",
    "                f'S1_S2_Nov2024_Oct2025'\n",
    "            )\n",
    "            \n",
    "            print(f\"Starting {len(export_tasks)} export tasks...\")\n",
    "            for i, task in enumerate(export_tasks[:5]):  # Start first 5 tasks\n",
    "                task.start()\n",
    "                print(f\"  Started: {task.config['description']}\")\n",
    "            \n",
    "            if len(export_tasks) > 5:\n",
    "                print(f\"\\nRemaining {len(export_tasks) - 5} tasks can be started manually or in batches\")\n",
    "                print(\"Monitor all tasks at: https://code.earthengine.google.com/tasks\")\n",
    "\n",
    "else:\n",
    "    print(\"No data to export!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Local Processing Function (Alternative to Export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b. Load Data from GEE Assets (For Subsequent Processing)\n",
    "\n",
    "If you exported to GEE Assets, use this code to load the data later in GEE or download specific regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA FROM GEE ASSETS\n",
    "# ============================================================================\n",
    "\n",
    "def load_asset_collection(asset_base_path, pattern='*'):\n",
    "    \"\"\"\n",
    "    Load ImageCollection from GEE Assets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    asset_base_path : str\n",
    "        Base path to assets folder\n",
    "    pattern : str\n",
    "        Pattern to match asset names (e.g., 'S1_S2_Nov2024_Oct2025_Period_*')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ee.ImageCollection\n",
    "    \"\"\"\n",
    "    # Load all images matching the pattern\n",
    "    full_pattern = f'{asset_base_path}/{pattern}'\n",
    "    \n",
    "    try:\n",
    "        # Try loading as collection\n",
    "        collection = ee.ImageCollection(full_pattern)\n",
    "        count = collection.size().getInfo()\n",
    "        print(f\"‚úÖ Loaded {count} images from assets\")\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading assets: {e}\")\n",
    "        print(f\"   Make sure assets exist at: {full_pattern}\")\n",
    "        print(f\"   Check: https://code.earthengine.google.com/?asset={asset_base_path}\")\n",
    "        return None\n",
    "\n",
    "def download_region_from_assets(collection, region_geometry, scale, output_format='GeoTIFF'):\n",
    "    \"\"\"\n",
    "    Download a specific region from asset collection\n",
    "    \n",
    "    This is useful when you've exported large Java Island data but only want\n",
    "    a smaller region for analysis\n",
    "    \"\"\"\n",
    "    # Convert collection to multi-band image\n",
    "    def add_period_to_bands(image):\n",
    "        period = ee.Number(image.get('period')).format('%02d')\n",
    "        old_names = image.bandNames()\n",
    "        new_names = old_names.map(lambda name: ee.String(name).cat('_P').cat(period))\n",
    "        return image.rename(new_names)\n",
    "    \n",
    "    renamed_collection = collection.map(add_period_to_bands)\n",
    "    multi_band = renamed_collection.toBands()\n",
    "    \n",
    "    # Create download URL\n",
    "    url = multi_band.getDownloadURL({\n",
    "        'scale': scale,\n",
    "        'crs': 'EPSG:4326',\n",
    "        'region': region_geometry,\n",
    "        'format': output_format\n",
    "    })\n",
    "    \n",
    "    print(f\"üì• Download URL generated:\")\n",
    "    print(f\"   {url}\")\n",
    "    print(f\"\\n   Copy this URL to your browser to download\")\n",
    "    \n",
    "    return url\n",
    "\n",
    "# Example: Load your exported assets\n",
    "if EXPORT_DESTINATION == 'asset':\n",
    "    print(\"=\"*60)\n",
    "    print(\"üìñ LOADING DATA FROM GEE ASSETS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Wait a moment for exports to start (if just submitted)\n",
    "    import time\n",
    "    print(\"\\n‚è≥ Note: Asset exports take time. Check status at:\")\n",
    "    print(\"   https://code.earthengine.google.com/tasks\")\n",
    "    \n",
    "    # Example of how to load later (after exports complete)\n",
    "    print(f\"\\nüí° To load your exported data later, use:\")\n",
    "    print(f\"\\n```python\")\n",
    "    print(f\"# Load the asset collection\")\n",
    "    print(f\"asset_pattern = '{ASSET_BASE_PATH}/S1_S2_Nov2024_Oct2025_Period_*'\")\n",
    "    print(f\"collection = ee.ImageCollection(asset_pattern)\")\n",
    "    print(f\"\")\n",
    "    print(f\"# Check what was loaded\")\n",
    "    print(f\"print(f'Loaded {{collection.size().getInfo()}} images')\")\n",
    "    print(f\"\")\n",
    "    print(f\"# Download a specific region (optional)\")\n",
    "    print(f\"small_region = ee.Geometry.Rectangle([106.8, -6.3, 107.0, -6.1])  # Example: Jakarta area\")\n",
    "    print(f\"url = download_region_from_assets(collection, small_region, scale={SCALE})\")\n",
    "    print(f\"```\")\n",
    "    \n",
    "    print(f\"\\nüîÑ Or use directly in GEE Code Editor:\")\n",
    "    print(f\"```javascript\")\n",
    "    print(f\"// Load the collection\")\n",
    "    print(f\"var collection = ee.ImageCollection('{ASSET_BASE_PATH}/S1_S2_Nov2024_Oct2025_Period_*');\")\n",
    "    print(f\"\")\n",
    "    print(f\"// Sort by period\")\n",
    "    print(f\"var sorted = collection.sort('period');\")\n",
    "    print(f\"\")\n",
    "    print(f\"// Get first image\")\n",
    "    print(f\"var first = sorted.first();\")\n",
    "    print(f\"print('First period bands:', first.bandNames());\")\n",
    "    print(f\"\")\n",
    "    print(f\"// Process further or export to Drive from here\")\n",
    "    print(f\"```\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Assets allow you to:\")\n",
    "    print(f\"   ‚Ä¢ Process data entirely in GEE (no download needed)\")\n",
    "    print(f\"   ‚Ä¢ Download only specific regions when needed\")\n",
    "    print(f\"   ‚Ä¢ Share with collaborators\")\n",
    "    print(f\"   ‚Ä¢ Use in GEE Code Editor or Python API\")\n",
    "\n",
    "elif EXPORT_DESTINATION == 'drive':\n",
    "    print(\"\\nüí° For Google Drive exports:\")\n",
    "    print(\"   1. Monitor tasks at: https://code.earthengine.google.com/tasks\")\n",
    "    print(\"   2. Download files from Google Drive\")\n",
    "    print(\"   3. Use local processing (Section 7 below) or load in MOGPR notebook\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timeseries_locally(collection, geometry, scale, max_pixels=1e6):\n",
    "    \"\"\"\n",
    "    Extract time series data directly to memory for small areas\n",
    "    This is faster than export/download for small study areas\n",
    "    \"\"\"\n",
    "    print(\"Extracting time series data locally...\")\n",
    "    \n",
    "    # Get the region bounds\n",
    "    region = geometry.bounds()\n",
    "    \n",
    "    # Extract data for each period\n",
    "    image_list = collection.toList(collection.size())\n",
    "    \n",
    "    periods_data = []\n",
    "    \n",
    "    for i in range(len(successful_periods)):\n",
    "        print(f\"Extracting period {i+1}/{len(successful_periods)}...\")\n",
    "        \n",
    "        image = ee.Image(image_list.get(i))\n",
    "        period_info = successful_periods[i]\n",
    "        \n",
    "        try:\n",
    "            # Sample the image\n",
    "            if scale * scale * 10000 < max_pixels:  # Rough estimate\n",
    "                # Use geemap for efficient extraction\n",
    "                data_array = geemap.ee_to_xarray(\n",
    "                    image, \n",
    "                    region=region, \n",
    "                    scale=scale,\n",
    "                    crs='EPSG:4326'\n",
    "                )\n",
    "                \n",
    "                # Add period information\n",
    "                data_array = data_array.assign_coords(\n",
    "                    period=period_info['period'],\n",
    "                    center_date=period_info['center_date'],\n",
    "                    doy_center=period_info['doy_center']\n",
    "                )\n",
    "                \n",
    "                periods_data.append(data_array)\n",
    "                \n",
    "            else:\n",
    "                print(f\"  Area too large for local extraction, use export method instead\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error extracting period {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if periods_data:\n",
    "        # Combine all periods into a single xarray Dataset\n",
    "        print(\"Combining periods into time series...\")\n",
    "        \n",
    "        # Concatenate along a new time dimension\n",
    "        combined_data = xr.concat(periods_data, dim='time')\n",
    "        \n",
    "        # Create proper time coordinates\n",
    "        time_coords = [p['center_date'] for p in successful_periods[:len(periods_data)]]\n",
    "        combined_data = combined_data.assign_coords(time=time_coords)\n",
    "        \n",
    "        return combined_data\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Try local extraction for small areas\n",
    "area_size = study_area.area().getInfo()  # in square meters\n",
    "area_km2 = area_size / 1e6\n",
    "\n",
    "print(f\"Study area size: {area_km2:.2f} km¬≤\")\n",
    "\n",
    "if area_km2 < 100:  # Less than 100 km¬≤\n",
    "    print(\"Area is small enough for local extraction. Attempting direct download...\")\n",
    "    \n",
    "    try:\n",
    "        local_data = extract_timeseries_locally(\n",
    "            time_series_collection, \n",
    "            study_area, \n",
    "            SCALE, \n",
    "            max_pixels=1e6\n",
    "        )\n",
    "        \n",
    "        if local_data is not None:\n",
    "            print(\"Local extraction successful!\")\n",
    "            print(f\"Data shape: {local_data.dims}\")\n",
    "            print(f\"Variables: {list(local_data.data_vars)}\")\n",
    "            \n",
    "            # Save locally\n",
    "            output_file = os.path.join(OUTPUT_DIR, f'S1_S2_timeseries_Nov2024_Oct2025_local.nc')\n",
    "            local_data.to_netcdf(output_file)\n",
    "            print(f\"Data saved to: {output_file}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Local extraction failed, use export method instead\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Local extraction error: {e}\")\n",
    "        print(\"Use export method instead\")\n",
    "        \n",
    "else:\n",
    "    print(\"Area is too large for local extraction. Use the export method above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Metadata and Processing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processing summary\n",
    "processing_summary = {\n",
    "    'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'start_date': START_DATE,\n",
    "    'end_date': END_DATE,\n",
    "    'temporal_coverage': f'{START_DATE} to {END_DATE}',\n",
    "    'agricultural_year': 'Nov 2024 - Oct 2025',\n",
    "    'total_periods': len(periods),\n",
    "    'successful_periods': len(successful_periods),\n",
    "    'study_area_bounds': study_area.bounds().getInfo(),\n",
    "    'spatial_resolution': f'{SCALE}m',\n",
    "    'coordinate_system': CRS,\n",
    "    'max_cloud_cover': MAX_CLOUD_COVER,\n",
    "    'composite_method': 'median',\n",
    "    'output_bands': ['VV', 'VH', 'S2ndvi'],\n",
    "    'agricultural_seasons_covered': {\n",
    "        'season_1': 'Nov 2024 - Mar 2025 (first planting, crosses year boundary)',\n",
    "        'season_2': 'Apr - Jun 2025 (second planting, dry season)',\n",
    "        'season_3': 'Jul - Sep 2025 (third planting, optional intensive)',\n",
    "        'full_coverage': 'Through Oct 2025'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create detailed period information\n",
    "period_details = []\n",
    "for period in successful_periods:\n",
    "    period_details.append({\n",
    "        'period': period['period'],\n",
    "        'start_date': period['start_str'],\n",
    "        'end_date': period['end_str'],\n",
    "        'center_date': period['center_date'].strftime('%Y-%m-%d'),\n",
    "        'doy_center': period['doy_center'],\n",
    "        'year': period['year'],\n",
    "        'month': period['month']\n",
    "    })\n",
    "\n",
    "# Save metadata\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'summary': processing_summary,\n",
    "    'periods': period_details\n",
    "}\n",
    "\n",
    "metadata_file = os.path.join(OUTPUT_DIR, f'processing_metadata_Nov2024_Oct2025.json')\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(\"Processing Summary:\")\n",
    "print(f\"  Temporal coverage: {START_DATE} to {END_DATE}\")\n",
    "print(f\"  Agricultural year: Nov 2024 - Oct 2025\")\n",
    "print(f\"  Total periods: {len(periods)}\")\n",
    "print(f\"  Successful periods: {len(successful_periods)}\")\n",
    "print(f\"  Spatial resolution: {SCALE}m\")\n",
    "print(f\"  Coordinate system: {CRS}\")\n",
    "print(f\"  Output bands: {processing_summary['output_bands']}\")\n",
    "print(f\"\\nAgricultural Seasons Covered:\")\n",
    "print(f\"  Season 1 (Nov-Mar): First planting season (crosses 2024‚Üí2025 boundary)\")\n",
    "print(f\"  Season 2 (Apr-Jun): Second planting season (dry season)\")\n",
    "print(f\"  Season 3 (Jul-Sep): Third planting season (optional intensive)\")\n",
    "print(f\"  Full coverage: Through October 2025\")\n",
    "print(f\"\\nMetadata saved to: {metadata_file}\")\n",
    "\n",
    "# Create period visualization\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "# Plot period timeline\n",
    "period_dates = [p['center_date'] for p in successful_periods]\n",
    "period_numbers = [p['period'] for p in successful_periods]\n",
    "\n",
    "ax.scatter(period_dates, period_numbers, alpha=0.7, s=50)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Period Number', fontsize=12)\n",
    "ax.set_title(f'12-Day Composite Periods: {START_DATE} to {END_DATE}\\nIndonesian Agricultural Year Coverage ({SCALE}m resolution, {CRS})', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add month boundaries and labels for both years\n",
    "from matplotlib.dates import DateFormatter, MonthLocator\n",
    "ax.xaxis.set_major_locator(MonthLocator())\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%b\\n%Y'))\n",
    "\n",
    "# Highlight agricultural seasons with colored backgrounds\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import datetime\n",
    "\n",
    "# Season 1: Nov 2024 - Mar 2025 (first planting)\n",
    "season1_start = datetime(2024, 11, 1)\n",
    "season1_end = datetime(2025, 3, 31)\n",
    "ax.axvspan(season1_start, season1_end, alpha=0.15, color='green', label='Season 1: Nov-Mar (First Planting)')\n",
    "\n",
    "# Season 2: Apr - Jun 2025 (second planting)\n",
    "season2_start = datetime(2025, 4, 1)\n",
    "season2_end = datetime(2025, 6, 30)\n",
    "ax.axvspan(season2_start, season2_end, alpha=0.15, color='blue', label='Season 2: Apr-Jun (Second Planting)')\n",
    "\n",
    "# Season 3: Jul - Sep 2025 (third planting)\n",
    "season3_start = datetime(2025, 7, 1)\n",
    "season3_end = datetime(2025, 9, 30)\n",
    "ax.axvspan(season3_start, season3_end, alpha=0.15, color='orange', label='Season 3: Jul-Sep (Third Planting)')\n",
    "\n",
    "# Highlight year boundary\n",
    "year_boundary = datetime(2025, 1, 1)\n",
    "ax.axvline(year_boundary, color='red', linewidth=2, linestyle='--', label='Year Boundary (2024‚Üí2025)')\n",
    "\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, f'period_timeline_Nov2024_Oct2025.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPeriod timeline saved to: {os.path.join(OUTPUT_DIR, f'period_timeline_Nov2024_Oct2025.png')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Conversion for FuseTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fusets_format(data_path_or_array, metadata_path=None):\n",
    "    \"\"\"\n",
    "    Convert GEE-exported data to FuseTS-compatible format\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(data_path_or_array, str):\n",
    "        # Load from file\n",
    "        print(f\"Loading data from: {data_path_or_array}\")\n",
    "        \n",
    "        if data_path_or_array.endswith('.nc'):\n",
    "            data = xr.open_dataset(data_path_or_array)\n",
    "        else:\n",
    "            # Assume GeoTIFF\n",
    "            import rioxarray\n",
    "            data = rioxarray.open_rasterio(data_path_or_array)\n",
    "            \n",
    "    else:\n",
    "        # Use provided array\n",
    "        data = data_path_or_array\n",
    "    \n",
    "    print(\"Converting to FuseTS format...\")\n",
    "    \n",
    "    # Ensure proper dimension naming\n",
    "    if 'time' in data.dims:\n",
    "        data = data.rename({'time': 't'})\n",
    "    \n",
    "    # Ensure proper band naming for FuseTS\n",
    "    if 'NDVI' in data.data_vars:\n",
    "        data = data.rename({'NDVI': 'S2ndvi'})\n",
    "    \n",
    "    # Ensure coordinate order is (t, y, x)\n",
    "    expected_dims = ['t', 'y', 'x']\n",
    "    \n",
    "    for var in data.data_vars:\n",
    "        if set(data[var].dims) == set(expected_dims):\n",
    "            data[var] = data[var].transpose('t', 'y', 'x')\n",
    "    \n",
    "    # Add FuseTS-specific attributes\n",
    "    data.attrs.update({\n",
    "        'title': f'Sentinel-1/2 Time Series for FuseTS Processing',\n",
    "        'description': '12-day composite periods extracted from Google Earth Engine',\n",
    "        'bands': 'VV (S1), VH (S1), S2ndvi (S2 NDVI)',\n",
    "        'temporal_resolution': '12-day composites',\n",
    "        'processing_software': 'Google Earth Engine + Python',\n",
    "        'fusets_ready': True\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_example_usage_script():\n",
    "    \"\"\"\n",
    "    Create a script showing how to use the exported data with FuseTS\n",
    "    \"\"\"\n",
    "    \n",
    "    script_content = '''\n",
    "# Example script to use GEE-exported data with FuseTS\n",
    "# Run this after downloading the exported data from Google Drive\n",
    "# Temporal coverage: November 2024 - October 2025 (Indonesian agricultural year)\n",
    "\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from fusets.mogpr import MOGPRTransformer\n",
    "from fusets.analytics import phenology\n",
    "from fusets import whittaker\n",
    "\n",
    "# Load the exported data\n",
    "# Option 1: If you exported as individual periods\n",
    "# data_files = ['S1_S2_Nov2024_Oct2025_Period_01.tif', 'S1_S2_Nov2024_Oct2025_Period_02.tif', ...]\n",
    "# data = combine_period_files(data_files)  # You'll need to implement this\n",
    "\n",
    "# Option 2: If you exported as single multi-band file\n",
    "data_path = 'S1_S2_TimeSeries_Nov2024_Oct2025.tif'\n",
    "data = rioxarray.open_rasterio(data_path)\n",
    "\n",
    "# Convert to FuseTS format\n",
    "fusets_data = prepare_fusets_format(data)\n",
    "\n",
    "# Apply MOGPR fusion\n",
    "mogpr = MOGPRTransformer()\n",
    "fused_data = mogpr.fit_transform(fusets_data)\n",
    "\n",
    "# Extract phenological metrics for Indonesian agricultural seasons\n",
    "# Season 1: Nov 2024 - Mar 2025 (first planting, crosses year boundary)\n",
    "# Season 2: Apr - Jun 2025 (second planting, dry season)\n",
    "# Season 3: Jul - Sep 2025 (third planting, optional intensive)\n",
    "\n",
    "phenology_metrics = phenology(fused_data['S2ndvi'])\n",
    "\n",
    "# Access results\n",
    "sos_times = phenology_metrics.da_sos_times\n",
    "eos_times = phenology_metrics.da_eos_times\n",
    "\n",
    "print(\"FuseTS processing completed for Nov 2024 - Oct 2025!\")\n",
    "print(\"Captured full Indonesian agricultural calendar including year-boundary season\")\n",
    "'''\n",
    "    \n",
    "    script_file = os.path.join(OUTPUT_DIR, 'fusets_processing_example.py')\n",
    "    with open(script_file, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    return script_file\n",
    "\n",
    "# Create example script\n",
    "example_script = create_example_usage_script()\n",
    "print(f\"Example FuseTS processing script created: {example_script}\")\n",
    "\n",
    "# If we have local data, prepare it for FuseTS\n",
    "if 'local_data' in locals() and local_data is not None:\n",
    "    print(\"\\nPreparing local data for FuseTS...\")\n",
    "    fusets_ready_data = prepare_fusets_format(local_data)\n",
    "    \n",
    "    # Save FuseTS-ready data\n",
    "    fusets_output = os.path.join(OUTPUT_DIR, f'S1_S2_timeseries_Nov2024_Oct2025_fusets_ready.nc')\n",
    "    fusets_ready_data.to_netcdf(fusets_output)\n",
    "    print(f\"FuseTS-ready data saved to: {fusets_output}\")\n",
    "    \n",
    "    # Display data structure\n",
    "    print(\"\\nFuseTS-ready data structure:\")\n",
    "    print(fusets_ready_data)\n",
    "    \n",
    "    print(\"\\nThis data is now ready for the MOGPR fusion notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What this notebook accomplishes:\n",
    "\n",
    "1. **Temporal Strategy**: Creates exactly 31 periods of 12-day composites from **Nov 2024 to Oct 2025**\n",
    "2. **Data Collection**: Extracts S1 (VV, VH) and S2 (NDVI) data from Google Earth Engine\n",
    "3. **Cloud Processing**: Uses GEE's computational power for large-scale data processing\n",
    "4. **Flexible Export**: **GEE Assets (recommended)** or Google Drive\n",
    "5. **Local Processing**: For small areas, extracts data directly without export/download\n",
    "6. **FuseTS Preparation**: Converts data to the exact format needed for MOGPR processing\n",
    "\n",
    "### Export Options Comparison:\n",
    "\n",
    "| Feature | GEE Assets ‚≠ê RECOMMENDED | Google Drive |\n",
    "|---------|---------------------------|--------------|\n",
    "| **Size limit** | 10 TB per user | ~15 GB per file |\n",
    "| **Best for** | Large areas (Java Island) | Small test areas |\n",
    "| **Speed** | Fast (stays in cloud) | Slow (download required) |\n",
    "| **Usage** | Use directly in GEE | Must download first |\n",
    "| **Sharing** | Easy (asset permissions) | Manual file sharing |\n",
    "| **Cost** | Free (GEE quota) | Free (Drive quota) |\n",
    "| **Processing** | Process in GEE cloud | Local processing needed |\n",
    "\n",
    "### When to use GEE Assets:\n",
    "‚úÖ **Study area > 1000 km¬≤** (like Java Island with 5km buffer)  \n",
    "‚úÖ **Multiple people need access** to the same data  \n",
    "‚úÖ **Want to process in GEE** without downloading  \n",
    "‚úÖ **Need to reuse data** in multiple projects  \n",
    "‚úÖ **Data size > 2GB**  \n",
    "\n",
    "### When to use Google Drive:\n",
    "‚úÖ **Small test area** (< 100 km¬≤)  \n",
    "‚úÖ **Quick prototyping** with local tools  \n",
    "‚úÖ **One-time download** for offline work  \n",
    "‚úÖ **Prefer local storage** over cloud  \n",
    "\n",
    "### Temporal Coverage (Indonesian Agricultural Year):\n",
    "- **Period 1**: 2024-11-01 to 2024-11-12 ‚Üê **First planting season starts**\n",
    "- **Period 2**: 2024-11-13 to 2024-11-24  \n",
    "- **Period 3**: 2024-11-25 to 2024-12-06\n",
    "- **Period 6**: 2024-12-31 to 2025-01-11 ‚Üê **Crosses year boundary**\n",
    "- **...**\n",
    "- **Period 11**: 2025-03-09 to 2025-03-20 ‚Üê **First planting season ends**\n",
    "- **Period 12-18**: 2025-04-01 to 2025-06-30 ‚Üê **Second planting season**\n",
    "- **Period 19-25**: 2025-07-01 to 2025-09-30 ‚Üê **Third planting season (optional)**\n",
    "- **Period 31**: 2025-10-21 to 2025-10-31 ‚Üê **Full coverage complete**\n",
    "\n",
    "### Agricultural Seasons Captured:\n",
    "- **Season 1 (Nov-Mar)**: First planting season - **handles year boundary transition**\n",
    "  - Start: Nov 2024 (Period 1)\n",
    "  - Peak: Jan 2025 (crosses from 2024‚Üí2025)\n",
    "  - End: Mar 2025 (Period ~11)\n",
    "  \n",
    "- **Season 2 (Apr-Jun)**: Second planting season (dry season)\n",
    "  - Periods 12-18 in 2025\n",
    "  \n",
    "- **Season 3 (Jul-Sep)**: Third planting season (optional intensive)\n",
    "  - Periods 19-25 in 2025\n",
    "  \n",
    "- **Full Monitoring**: Through October 2025 (Period 31)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "#### If you exported to GEE Assets (Recommended):\n",
    "1. **Monitor exports**: https://code.earthengine.google.com/tasks\n",
    "2. **Use in GEE Code Editor**:\n",
    "   ```javascript\n",
    "   var collection = ee.ImageCollection('projects/ee-geodeticengineeringundip/assets/FuseTS/S1_S2_Nov2024_Oct2025_Period_*');\n",
    "   ```\n",
    "3. **Or download specific regions** when needed (see Section 6b)\n",
    "4. **Process in GEE** or download small regions for local analysis\n",
    "\n",
    "#### If you exported to Google Drive:\n",
    "1. **Download Data**: Monitor exports at https://code.earthengine.google.com/tasks\n",
    "2. **Load in FuseTS**: Use the exported GeoTIFF files with the MOGPR fusion notebook\n",
    "3. **Apply MOGPR**: Run the S1+S2 fusion using the prepared time series\n",
    "4. **Multi-Season Analysis**: Detect all three Indonesian agricultural seasons\n",
    "\n",
    "### File Outputs:\n",
    "- **Assets**: `projects/ee-geodeticengineeringundip/assets/FuseTS/S1_S2_Nov2024_Oct2025_Period_*`\n",
    "- **Or Drive**: S1_S2_TimeSeries_Nov2024_Oct2025.tif (or individual period files)\n",
    "- **Metadata**: processing_metadata_Nov2024_Oct2025.json\n",
    "- **Timeline**: period_timeline_Nov2024_Oct2025.png\n",
    "- **Example Script**: fusets_processing_example.py\n",
    "\n",
    "### Key Features:\n",
    "‚úÖ **Perfect alignment** with Indonesian agricultural calendar  \n",
    "‚úÖ **Year boundary handling** for Nov 2024 ‚Üí Mar 2025 first season  \n",
    "‚úÖ **Complete coverage** of all potential planting seasons  \n",
    "‚úÖ **31 periods** √ó 12 days = 365 days (full agricultural year)  \n",
    "‚úÖ **50m resolution** for efficient regional analysis  \n",
    "‚úÖ **GEE Assets support** for large-scale datasets  \n",
    "\n",
    "### For Large Datasets (Java Island):\n",
    "üí° **Recommended workflow**:\n",
    "1. Export to **GEE Assets** (no size limits)\n",
    "2. Process and analyze **entirely in GEE** using Code Editor or Python API\n",
    "3. Download **only final results** or specific regions of interest\n",
    "4. Use MOGPR fusion **on cloud-processed data** for maximum efficiency\n",
    "\n",
    "The exported data is now ready for the FuseTS MOGPR processing workflow with full Indonesian agricultural season detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ READY TO RUN\n",
    "\n",
    "**Current Configuration:**\n",
    "- **Study Area**: Kabupaten Demak (~900 km¬≤)\n",
    "- **Temporal Coverage**: 31 periods √ó 12 days (Nov 2024 - Oct 2025)\n",
    "- **Sentinel-1**: VV, VH bands (99.9% coverage)\n",
    "- **Sentinel-2**: Level-1C TOA NDVI without masking (99.9% coverage)\n",
    "- **Export**: Google Drive, 31 individual files\n",
    "- **Expected Size**: ~15-30GB total\n",
    "\n",
    "**Next Steps:**\n",
    "1. ‚úÖ **Run Section 5** to process all 31 periods (~10-30 min)\n",
    "2. ‚úÖ **Run Section 6** to export to Google Drive\n",
    "3. ‚úÖ **Monitor exports** at https://code.earthengine.google.com/tasks\n",
    "4. ‚úÖ **Download** from Google Drive after completion (~30-60 min)\n",
    "5. ‚úÖ **Run MOGPR fusion** in S1_S2_MOGPR_Fusion_Tutorial.ipynb\n",
    "\n",
    "**Ready to start? Run Section 5 now!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
