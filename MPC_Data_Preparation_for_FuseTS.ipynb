{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "````xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# Microsoft Planetary Computer Data Preparation for FuseTS\n",
    "\n",
    "This notebook extracts Sentinel-1 and Sentinel-2 data from **Microsoft Planetary Computer (MPC)** and prepares it for FuseTS MOGPR processing.\n",
    "\n",
    "## Why Microsoft Planetary Computer?\n",
    "\n",
    "‚úÖ **Advantages over Google Earth Engine:**\n",
    "- No authentication hassles (free public access)\n",
    "- Direct access to Analysis Ready Data (ARD)\n",
    "- STAC API - industry standard for satellite data\n",
    "- Easy integration with Python ecosystem (xarray, rioxarray, dask)\n",
    "- Cloud-optimized GeoTIFFs (COGs)\n",
    "- No export/download waiting - process directly in Python\n",
    "\n",
    "‚ùå **Considerations:**\n",
    "- Requires more local processing/memory than GEE\n",
    "- Need to handle data mosaicking yourself\n",
    "- Download bandwidth may be limiting factor\n",
    "\n",
    "## Temporal Strategy\n",
    "- **Date range**: November 2023 - November 2025 (2 years)\n",
    "- **Temporal resolution**: 12-day composites (dekadal periods)\n",
    "- **Total periods**: ~62 periods\n",
    "- **Coverage**: Full Indonesian agricultural calendar (6 growing seasons)\n",
    "\n",
    "## Study Area\n",
    "- **Location**: Klambu-Glapan paddy fields, Demak, Central Java\n",
    "- **Source**: Shapefile (data/klambu-glapan.shp)\n",
    "- **Buffer**: 500m around paddy boundaries\n",
    "\n",
    "## Output Format\n",
    "Data will be in FuseTS-compatible xarray format:\n",
    "- S1: `VV`, `VH` bands\n",
    "- S2: `S2ndvi` band\n",
    "- Dimensions: `(t, y, x)` with proper CRS\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 1. Setup and Install Dependencies\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Install required packages\n",
    "# Uncomment the following line if packages are not installed\n",
    "# !pip install planetary-computer pystac-client rioxarray xarray numpy pandas matplotlib geopandas shapely dask\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import planetary_computer\n",
    "import pystac_client\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping, box\n",
    "from shapely.ops import unary_union\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")\n",
    "print(f\"\\nüì¶ Package versions:\")\n",
    "print(f\"   planetary-computer: {planetary_computer.__version__}\")\n",
    "print(f\"   pystac-client: {pystac_client.__version__}\")\n",
    "print(f\"   xarray: {xr.__version__}\")\n",
    "print(f\"   rioxarray: {rioxarray.__version__}\")\n",
    "print(f\"   geopandas: {gpd.__version__}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 2. Load Study Area from Shapefile\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# ============================================================================\n",
    "# LOAD PADDY SHAPEFILE AND DEFINE STUDY AREA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìç LOADING STUDY AREA FROM SHAPEFILE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the paddy shapefile\n",
    "shapefile_path = 'data/klambu-glapan.shp'\n",
    "\n",
    "print(f\"\\nüéØ Loading: {shapefile_path}\")\n",
    "\n",
    "try:\n",
    "    # Read the shapefile\n",
    "    paddy_gdf = gpd.read_file(shapefile_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Shapefile loaded successfully!\")\n",
    "    print(f\"   Number of features: {len(paddy_gdf)}\")\n",
    "    print(f\"   CRS: {paddy_gdf.crs}\")\n",
    "    print(f\"   Columns: {paddy_gdf.columns.tolist()}\")\n",
    "    \n",
    "    # Get original bounds\n",
    "    orig_west, orig_south, orig_east, orig_north = paddy_gdf.total_bounds\n",
    "    print(f\"\\n   Original Bounds (WGS84):\")\n",
    "    print(f\"     West:  {orig_west:.6f}¬∞\")\n",
    "    print(f\"     South: {orig_south:.6f}¬∞\")\n",
    "    print(f\"     East:  {orig_east:.6f}¬∞\")\n",
    "    print(f\"     North: {orig_north:.6f}¬∞\")\n",
    "    \n",
    "    # Convert to UTM Zone 49S for accurate area calculation and buffering\n",
    "    print(f\"\\n   Converting to UTM Zone 49S (EPSG:32749)...\")\n",
    "    paddy_utm = paddy_gdf.to_crs(\"EPSG:32749\")\n",
    "    \n",
    "    # Calculate area in UTM\n",
    "    total_area_m2 = paddy_utm.area.sum()\n",
    "    total_area_km2 = total_area_m2 / 1e6\n",
    "    print(f\"   Total paddy area: {total_area_km2:.2f} km¬≤\")\n",
    "    \n",
    "    # Apply buffer in UTM (meters)\n",
    "    BUFFER_DISTANCE_M = 500\n",
    "    print(f\"\\n   Applying {BUFFER_DISTANCE_M}m buffer...\")\n",
    "    \n",
    "    paddy_buffered_utm = paddy_utm.copy()\n",
    "    paddy_buffered_utm['geometry'] = paddy_utm.buffer(BUFFER_DISTANCE_M)\n",
    "    \n",
    "    # Merge all polygons\n",
    "    merged_geometry_utm = unary_union(paddy_buffered_utm.geometry)\n",
    "    buffered_area_km2 = merged_geometry_utm.area / 1e6\n",
    "    \n",
    "    print(f\"   Buffered area: {buffered_area_km2:.2f} km¬≤\")\n",
    "    \n",
    "    # Convert back to WGS84 for MPC queries\n",
    "    buffered_gdf_utm = gpd.GeoDataFrame(\n",
    "        geometry=[merged_geometry_utm],\n",
    "        crs=\"EPSG:32749\"\n",
    "    )\n",
    "    buffered_gdf_wgs84 = buffered_gdf_utm.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Get final bounds in WGS84\n",
    "    west, south, east, north = buffered_gdf_wgs84.total_bounds\n",
    "    \n",
    "    print(f\"\\n   Final Bounds (WGS84 for MPC):\")\n",
    "    print(f\"     West:  {west:.6f}¬∞\")\n",
    "    print(f\"     South: {south:.6f}¬∞\")\n",
    "    print(f\"     East:  {east:.6f}¬∞\")\n",
    "    print(f\"     North: {north:.6f}¬∞\")\n",
    "    \n",
    "    # Create bounding box for queries\n",
    "    bbox = [west, south, east, north]\n",
    "    \n",
    "    # Create geometry for spatial queries\n",
    "    study_area_geom = buffered_gdf_wgs84.geometry.iloc[0]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Study area prepared!\")\n",
    "    print(f\"   Location: Klambu-Glapan, Demak, Central Java\")\n",
    "    print(f\"   Features: {len(paddy_gdf)} paddy fields\")\n",
    "    print(f\"   Area: {buffered_area_km2:.2f} km¬≤\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Original\n",
    "    paddy_utm.plot(ax=axes[0], facecolor='lightgreen', edgecolor='darkgreen', \n",
    "                   linewidth=0.5, alpha=0.7)\n",
    "    axes[0].set_title(f'Original Paddy Fields\\n{total_area_km2:.2f} km¬≤', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Easting (m UTM 49S)')\n",
    "    axes[0].set_ylabel('Northing (m UTM 49S)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Buffered\n",
    "    buffered_gdf_utm.plot(ax=axes[1], facecolor='yellow', edgecolor='orange',\n",
    "                          linewidth=2, alpha=0.5, label=f'{BUFFER_DISTANCE_M}m buffer')\n",
    "    paddy_utm.plot(ax=axes[1], facecolor='lightgreen', edgecolor='darkgreen',\n",
    "                   linewidth=0.5, alpha=0.7, label='Paddy fields')\n",
    "    axes[1].set_title(f'Study Area with Buffer\\n{buffered_area_km2:.2f} km¬≤',\n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Easting (m UTM 49S)')\n",
    "    axes[1].set_ylabel('Northing (m UTM 49S)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mpc_study_area.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n   üìä Visualization saved: mpc_study_area.png\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading shapefile: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Processing parameters\n",
    "START_DATE = '2023-11-01'\n",
    "END_DATE = '2025-11-07'\n",
    "TEMPORAL_RESOLUTION_DAYS = 12  # 12-day composites\n",
    "TARGET_CRS = \"EPSG:32749\"  # UTM Zone 49S\n",
    "TARGET_RESOLUTION = 10  # meters\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path('mpc_fusets_data')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üìã PROCESSING CONFIGURATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"   Study area: Klambu-Glapan paddy fields\")\n",
    "print(f\"   Bounding box: {bbox}\")\n",
    "print(f\"   Date range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"   Temporal resolution: {TEMPORAL_RESOLUTION_DAYS}-day composites\")\n",
    "print(f\"   Spatial resolution: {TARGET_RESOLUTION}m\")\n",
    "print(f\"   Target CRS: {TARGET_CRS}\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR}/\")\n",
    "print(f\"{'='*70}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 3. Generate 12-Day Composite Periods\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "def generate_periods(start_date_str, end_date_str, days_per_period=12):\n",
    "    \"\"\"\n",
    "    Generate periods for composite creation\n",
    "    \"\"\"\n",
    "    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
    "    \n",
    "    periods = []\n",
    "    period_num = 1\n",
    "    current_start = start_date\n",
    "    \n",
    "    while current_start <= end_date:\n",
    "        period_end = current_start + timedelta(days=days_per_period - 1)\n",
    "        \n",
    "        if period_end > end_date:\n",
    "            period_end = end_date\n",
    "        \n",
    "        periods.append({\n",
    "            'period': period_num,\n",
    "            'start_date': current_start,\n",
    "            'end_date': period_end,\n",
    "            'start_str': current_start.strftime('%Y-%m-%d'),\n",
    "            'end_str': period_end.strftime('%Y-%m-%d'),\n",
    "            'center_date': current_start + timedelta(days=days_per_period // 2),\n",
    "            'year': current_start.year,\n",
    "            'month': current_start.month\n",
    "        })\n",
    "        \n",
    "        if period_end >= end_date:\n",
    "            break\n",
    "        \n",
    "        current_start = period_end + timedelta(days=1)\n",
    "        period_num += 1\n",
    "    \n",
    "    return periods\n",
    "\n",
    "# Generate periods\n",
    "periods = generate_periods(START_DATE, END_DATE, TEMPORAL_RESOLUTION_DAYS)\n",
    "\n",
    "print(f\"Generated {len(periods)} periods from {START_DATE} to {END_DATE}:\")\n",
    "print(f\"\\nFirst 5 periods:\")\n",
    "for p in periods[:5]:\n",
    "    print(f\"  Period {p['period']:2d}: {p['start_str']} to {p['end_str']}\")\n",
    "\n",
    "print(f\"\\nLast 5 periods:\")\n",
    "for p in periods[-5:]:\n",
    "    print(f\"  Period {p['period']:2d}: {p['start_str']} to {p['end_str']}\")\n",
    "\n",
    "print(f\"\\nCoverage by year:\")\n",
    "for year in [2023, 2024, 2025]:\n",
    "    count = len([p for p in periods if p['year'] == year])\n",
    "    print(f\"  {year}: {count} periods\")\n",
    "\n",
    "# Create DataFrame\n",
    "periods_df = pd.DataFrame(periods)\n",
    "print(f\"\\nTotal: {len(periods)} periods covering ~2 years\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 4. Connect to Microsoft Planetary Computer\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# ============================================================================\n",
    "# CONNECT TO MICROSOFT PLANETARY COMPUTER STAC API\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üåç CONNECTING TO MICROSOFT PLANETARY COMPUTER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# MPC STAC API endpoint\n",
    "STAC_API_URL = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "\n",
    "# Open STAC catalog\n",
    "catalog = pystac_client.Client.open(\n",
    "    STAC_API_URL,\n",
    "    modifier=planetary_computer.sign_inplace\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Connected to Microsoft Planetary Computer!\")\n",
    "print(f\"   STAC API: {STAC_API_URL}\")\n",
    "print(f\"   Catalog: {catalog.title}\")\n",
    "\n",
    "# List available collections\n",
    "print(f\"\\nüìö Available Sentinel collections:\")\n",
    "collections = catalog.get_collections()\n",
    "for collection in collections:\n",
    "    if 'sentinel' in collection.id.lower():\n",
    "        print(f\"   ‚Ä¢ {collection.id}: {collection.title}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to search for Sentinel-1 and Sentinel-2 data!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 5. Search and Download Sentinel-2 Data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "def search_sentinel2_items(bbox, start_date, end_date, max_cloud_cover=80):\n",
    "    \"\"\"\n",
    "    Search for Sentinel-2 L2A items in MPC\n",
    "    \"\"\"\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        bbox=bbox,\n",
    "        datetime=f\"{start_date}/{end_date}\",\n",
    "        query={\n",
    "            \"eo:cloud_cover\": {\"lt\": max_cloud_cover}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    items = list(search.items())\n",
    "    print(f\"   Found {len(items)} Sentinel-2 scenes\")\n",
    "    \n",
    "    return items\n",
    "\n",
    "def load_sentinel2_ndvi(items, bbox, resolution=10):\n",
    "    \"\"\"\n",
    "    Load Sentinel-2 data and calculate NDVI\n",
    "    \"\"\"\n",
    "    if not items:\n",
    "        return None\n",
    "    \n",
    "    ndvi_arrays = []\n",
    "    \n",
    "    for item in items:\n",
    "        try:\n",
    "            # Get B04 (Red) and B08 (NIR) assets\n",
    "            red_href = item.assets[\"B04\"].href\n",
    "            nir_href = item.assets[\"B08\"].href\n",
    "            \n",
    "            # Sign URLs\n",
    "            red_href = planetary_computer.sign(red_href)\n",
    "            nir_href = planetary_computer.sign(nir_href)\n",
    "            \n",
    "            # Load bands\n",
    "            red = rioxarray.open_rasterio(red_href, masked=True).squeeze()\n",
    "            nir = rioxarray.open_rasterio(nir_href, masked=True).squeeze()\n",
    "            \n",
    "            # Clip to bbox\n",
    "            red = red.rio.clip_box(*bbox)\n",
    "            nir = nir.rio.clip_box(*bbox)\n",
    "            \n",
    "            # Calculate NDVI\n",
    "            ndvi = (nir - red) / (nir + red)\n",
    "            ndvi = ndvi.where(np.isfinite(ndvi))\n",
    "            \n",
    "            ndvi_arrays.append(ndvi)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      Error loading scene: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not ndvi_arrays:\n",
    "        return None\n",
    "    \n",
    "    # Stack and compute median\n",
    "    stacked = xr.concat(ndvi_arrays, dim='time')\n",
    "    median_ndvi = stacked.median(dim='time', skipna=True)\n",
    "    \n",
    "    return median_ndvi\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üõ∞Ô∏è  PROCESSING SENTINEL-2 DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "s2_periods_data = []\n",
    "\n",
    "for i, period in enumerate(periods[:3]):  # Start with first 3 periods for testing\n",
    "    print(f\"\\nPeriod {period['period']:2d}: {period['start_str']} to {period['end_str']}\")\n",
    "    \n",
    "    # Search for items\n",
    "    items = search_sentinel2_items(\n",
    "        bbox, \n",
    "        period['start_str'], \n",
    "        period['end_str'],\n",
    "        max_cloud_cover=80\n",
    "    )\n",
    "    \n",
    "    if items:\n",
    "        # Load and process\n",
    "        ndvi = load_sentinel2_ndvi(items, bbox, resolution=TARGET_RESOLUTION)\n",
    "        \n",
    "        if ndvi is not None:\n",
    "            s2_periods_data.append({\n",
    "                'period': period['period'],\n",
    "                'ndvi': ndvi,\n",
    "                'center_date': period['center_date'],\n",
    "                'n_scenes': len(items)\n",
    "            })\n",
    "            print(f\"   ‚úÖ NDVI computed from {len(items)} scenes\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Failed to compute NDVI\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  No scenes found\")\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(s2_periods_data)} periods successfully\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 6. Search and Download Sentinel-1 Data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "def search_sentinel1_items(bbox, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Search for Sentinel-1 GRD items in MPC\n",
    "    \"\"\"\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-1-grd\"],\n",
    "        bbox=bbox,\n",
    "        datetime=f\"{start_date}/{end_date}\",\n",
    "        query={\n",
    "            \"sat:orbit_state\": {\"eq\": \"descending\"},\n",
    "            \"sar:instrument_mode\": {\"eq\": \"IW\"}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    items = list(search.items())\n",
    "    print(f\"   Found {len(items)} Sentinel-1 scenes\")\n",
    "    \n",
    "    return items\n",
    "\n",
    "def load_sentinel1_bands(items, bbox):\n",
    "    \"\"\"\n",
    "    Load Sentinel-1 VV and VH bands\n",
    "    \"\"\"\n",
    "    if not items:\n",
    "        return None, None\n",
    "    \n",
    "    vv_arrays = []\n",
    "    vh_arrays = []\n",
    "    \n",
    "    for item in items:\n",
    "        try:\n",
    "            # Get VV and VH assets\n",
    "            vv_href = planetary_computer.sign(item.assets[\"vv\"].href)\n",
    "            vh_href = planetary_computer.sign(item.assets[\"vh\"].href)\n",
    "            \n",
    "            # Load bands\n",
    "            vv = rioxarray.open_rasterio(vv_href, masked=True).squeeze()\n",
    "            vh = rioxarray.open_rasterio(vh_href, masked=True).squeeze()\n",
    "            \n",
    "            # Clip to bbox\n",
    "            vv = vv.rio.clip_box(*bbox)\n",
    "            vh = vh.rio.clip_box(*bbox)\n",
    "            \n",
    "            vv_arrays.append(vv)\n",
    "            vh_arrays.append(vh)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      Error loading scene: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not vv_arrays:\n",
    "        return None, None\n",
    "    \n",
    "    # Stack and compute median\n",
    "    vv_stacked = xr.concat(vv_arrays, dim='time')\n",
    "    vh_stacked = xr.concat(vh_arrays, dim='time')\n",
    "    \n",
    "    vv_median = vv_stacked.median(dim='time', skipna=True)\n",
    "    vh_median = vh_stacked.median(dim='time', skipna=True)\n",
    "    \n",
    "    return vv_median, vh_median\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì° PROCESSING SENTINEL-1 DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "s1_periods_data = []\n",
    "\n",
    "for i, period in enumerate(periods[:3]):  # Start with first 3 periods for testing\n",
    "    print(f\"\\nPeriod {period['period']:2d}: {period['start_str']} to {period['end_str']}\")\n",
    "    \n",
    "    # Search for items\n",
    "    items = search_sentinel1_items(bbox, period['start_str'], period['end_str'])\n",
    "    \n",
    "    if items:\n",
    "        # Load and process\n",
    "        vv, vh = load_sentinel1_bands(items, bbox)\n",
    "        \n",
    "        if vv is not None and vh is not None:\n",
    "            s1_periods_data.append({\n",
    "                'period': period['period'],\n",
    "                'vv': vv,\n",
    "                'vh': vh,\n",
    "                'center_date': period['center_date'],\n",
    "                'n_scenes': len(items)\n",
    "            })\n",
    "            print(f\"   ‚úÖ VV/VH computed from {len(items)} scenes\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Failed to load data\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  No scenes found\")\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(s1_periods_data)} periods successfully\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 7. Combine S1 and S2 Data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "print(\"=\"*70)\n",
    "print(\"üîó COMBINING SENTINEL-1 AND SENTINEL-2 DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Match periods and combine\n",
    "combined_data = []\n",
    "\n",
    "for s2_data in s2_periods_data:\n",
    "    period = s2_data['period']\n",
    "    \n",
    "    # Find matching S1 data\n",
    "    s1_data = next((s for s in s1_periods_data if s['period'] == period), None)\n",
    "    \n",
    "    if s1_data is not None:\n",
    "        print(f\"\\nPeriod {period}: Combining S1 and S2...\")\n",
    "        \n",
    "        # Get S2 NDVI as reference\n",
    "        ndvi = s2_data['ndvi']\n",
    "        \n",
    "        # Reproject S1 to match S2\n",
    "        vv = s1_data['vv'].rio.reproject_match(ndvi)\n",
    "        vh = s1_data['vh'].rio.reproject_match(ndvi)\n",
    "        \n",
    "        # Create combined dataset\n",
    "        combined = xr.Dataset({\n",
    "            'VV': vv,\n",
    "            'VH': vh,\n",
    "            'S2ndvi': ndvi\n",
    "        })\n",
    "        \n",
    "        # Add time coordinate\n",
    "        combined = combined.assign_coords(\n",
    "            t=s2_data['center_date']\n",
    "        )\n",
    "        \n",
    "        combined_data.append(combined)\n",
    "        \n",
    "        print(f\"   ‚úÖ Combined dataset created\")\n",
    "        print(f\"      Shape: {ndvi.shape}\")\n",
    "        print(f\"      CRS: {ndvi.rio.crs}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Combined {len(combined_data)} periods\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 8. Create Time Series Dataset\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "if combined_data:\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä CREATING TIME SERIES DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Concatenate along time dimension\n",
    "    timeseries = xr.concat(combined_data, dim='t')\n",
    "    \n",
    "    # Ensure proper dimension order (t, y, x)\n",
    "    timeseries = timeseries.transpose('t', 'y', 'x')\n",
    "    \n",
    "    # Add metadata\n",
    "    timeseries.attrs.update({\n",
    "        'title': 'Sentinel-1/2 Time Series from Microsoft Planetary Computer',\n",
    "        'description': '12-day composite periods for FuseTS processing',\n",
    "        'study_area': 'Klambu-Glapan paddy fields, Demak, Central Java',\n",
    "        'date_range': f'{START_DATE} to {END_DATE}',\n",
    "        'temporal_resolution': f'{TEMPORAL_RESOLUTION_DAYS}-day composites',\n",
    "        'spatial_resolution': f'{TARGET_RESOLUTION}m',\n",
    "        'crs': str(timeseries.rio.crs),\n",
    "        'source': 'Microsoft Planetary Computer',\n",
    "        'collections': 'sentinel-1-grd, sentinel-2-l2a'\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n‚úÖ Time series dataset created!\")\n",
    "    print(f\"\\n{timeseries}\")\n",
    "    print(f\"\\nDimensions: {dict(timeseries.dims)}\")\n",
    "    print(f\"Variables: {list(timeseries.data_vars)}\")\n",
    "    print(f\"Time steps: {len(timeseries.t)}\")\n",
    "    print(f\"CRS: {timeseries.rio.crs}\")\n",
    "    \n",
    "    # Save to NetCDF\n",
    "    output_file = OUTPUT_DIR / f'mpc_s1_s2_timeseries_{START_DATE}_{END_DATE}.nc'\n",
    "    timeseries.to_netcdf(output_file)\n",
    "    \n",
    "    print(f\"\\nüíæ Saved to: {output_file}\")\n",
    "    \n",
    "    # Quick visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Mean VV\n",
    "    timeseries['VV'].mean(dim='t').plot(ax=axes[0], cmap='gray')\n",
    "    axes[0].set_title('Mean VV (dB)', fontweight='bold')\n",
    "    \n",
    "    # Mean VH\n",
    "    timeseries['VH'].mean(dim='t').plot(ax=axes[1], cmap='gray')\n",
    "    axes[1].set_title('Mean VH (dB)', fontweight='bold')\n",
    "    \n",
    "    # Mean NDVI\n",
    "    timeseries['S2ndvi'].mean(dim='t').plot(ax=axes[2], cmap='RdYlGn', vmin=-0.2, vmax=0.8)\n",
    "    axes[2].set_title('Mean NDVI', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'mpc_timeseries_preview.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Preview saved: {OUTPUT_DIR / 'mpc_timeseries_preview.png'}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No combined data available to create time series\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 9. Process All Periods (Full Workflow)\n",
    "\n",
    "Now that we've tested the workflow with 3 periods, let's process all periods.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# ============================================================================\n",
    "# FULL PROCESSING FOR ALL PERIODS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING FULL PROCESSING FOR ALL PERIODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def process_single_period(period, bbox, resolution=10, max_cloud_cover=80):\n",
    "    \"\"\"\n",
    "    Process a single period: download S1 and S2, combine, return dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"Period {period['period']:2d}: {period['start_str']} to {period['end_str']}\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    \n",
    "    result = {\n",
    "        'period': period['period'],\n",
    "        'center_date': period['center_date'],\n",
    "        'dataset': None,\n",
    "        'n_s1_scenes': 0,\n",
    "        'n_s2_scenes': 0\n",
    "    }\n",
    "    \n",
    "    # Search Sentinel-2\n",
    "    print(f\"üõ∞Ô∏è  Searching Sentinel-2...\")\n",
    "    s2_items = search_sentinel2_items(bbox, period['start_str'], period['end_str'], max_cloud_cover)\n",
    "    result['n_s2_scenes'] = len(s2_items)\n",
    "    \n",
    "    # Search Sentinel-1\n",
    "    print(f\"üì° Searching Sentinel-1...\")\n",
    "    s1_items = search_sentinel1_items(bbox, period['start_str'], period['end_str'])\n",
    "    result['n_s1_scenes'] = len(s1_items)\n",
    "    \n",
    "    if not s2_items and not s1_items:\n",
    "        print(f\"   ‚ö†Ô∏è  No data found for this period\")\n",
    "        return result\n",
    "    \n",
    "    # Load S2 NDVI\n",
    "    ndvi = None\n",
    "    if s2_items:\n",
    "        print(f\"   Loading S2 NDVI...\")\n",
    "        ndvi = load_sentinel2_ndvi(s2_items, bbox, resolution)\n",
    "    \n",
    "    # Load S1 VV/VH\n",
    "    vv, vh = None, None\n",
    "    if s1_items:\n",
    "        print(f\"   Loading S1 VV/VH...\")\n",
    "        vv, vh = load_sentinel1_bands(s1_items, bbox)\n",
    "    \n",
    "    # Combine if we have data\n",
    "    if ndvi is not None or (vv is not None and vh is not None):\n",
    "        # Use NDVI as reference if available, otherwise use VV\n",
    "        reference = ndvi if ndvi is not None else vv\n",
    "        \n",
    "        # Create dataset\n",
    "        ds_dict = {}\n",
    "        \n",
    "        if vv is not None and vh is not None:\n",
    "            ds_dict['VV'] = vv.rio.reproject_match(reference) if vv is not None else reference * 0\n",
    "            ds_dict['VH'] = vh.rio.reproject_match(reference) if vh is not None else reference * 0\n",
    "        \n",
    "        if ndvi is not None:\n",
    "            ds_dict['S2ndvi'] = ndvi\n",
    "        \n",
    "        ds = xr.Dataset(ds_dict)\n",
    "        ds = ds.assign_coords(t=period['center_date'])\n",
    "        \n",
    "        result['dataset'] = ds\n",
    "        \n",
    "        print(f\"   ‚úÖ Period processed successfully\")\n",
    "        print(f\"      S1 scenes: {result['n_s1_scenes']}, S2 scenes: {result['n_s2_scenes']}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Failed to create dataset\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Process all periods\n",
    "print(f\"\\n‚è≥ Processing {len(periods)} periods...\")\n",
    "print(f\"   This may take 30-60 minutes depending on your connection\\n\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for period in periods:\n",
    "    result = process_single_period(\n",
    "        period, \n",
    "        bbox, \n",
    "        resolution=TARGET_RESOLUTION,\n",
    "        max_cloud_cover=80\n",
    "    )\n",
    "    all_results.append(result)\n",
    "\n",
    "# Filter successful results\n",
    "successful_results = [r for r in all_results if r['dataset'] is not None]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ PROCESSING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"   Total periods: {len(periods)}\")\n",
    "print(f\"   Successful: {len(successful_results)}\")\n",
    "print(f\"   Failed: {len(periods) - len(successful_results)}\")\n",
    "\n",
    "if successful_results:\n",
    "    # Create final time series\n",
    "    print(f\"\\nüìä Creating final time series dataset...\")\n",
    "    \n",
    "    final_timeseries = xr.concat(\n",
    "        [r['dataset'] for r in successful_results],\n",
    "        dim='t'\n",
    "    )\n",
    "    \n",
    "    # Ensure proper dimension order\n",
    "    final_timeseries = final_timeseries.transpose('t', 'y', 'x')\n",
    "    \n",
    "    # Add comprehensive metadata\n",
    "    final_timeseries.attrs.update({\n",
    "        'title': 'Sentinel-1/2 Time Series from Microsoft Planetary Computer',\n",
    "        'description': f'{TEMPORAL_RESOLUTION_DAYS}-day composite periods for FuseTS MOGPR processing',\n",
    "        'study_area': 'Klambu-Glapan paddy fields, Demak, Central Java, Indonesia',\n",
    "        'shapefile_source': shapefile_path,\n",
    "        'buffer_distance_m': BUFFER_DISTANCE_M,\n",
    "        'date_range': f'{START_DATE} to {END_DATE}',\n",
    "        'temporal_resolution': f'{TEMPORAL_RESOLUTION_DAYS}-day composites',\n",
    "        'spatial_resolution': f'{TARGET_RESOLUTION}m',\n",
    "        'target_crs': TARGET_CRS,\n",
    "        'bbox': bbox,\n",
    "        'n_periods': len(successful_results),\n",
    "        'source': 'Microsoft Planetary Computer',\n",
    "        'collections': 'sentinel-1-grd, sentinel-2-l2a',\n",
    "        'processing_date': datetime.now().isoformat()\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{final_timeseries}\")\n",
    "    \n",
    "    # Save final dataset\n",
    "    final_output = OUTPUT_DIR / f'mpc_s1_s2_klambu_glapan_{START_DATE}_{END_DATE}_final.nc'\n",
    "    final_timeseries.to_netcdf(final_output)\n",
    "    \n",
    "    print(f\"\\nüíæ Final dataset saved to: {final_output}\")\n",
    "    print(f\"   Size: {final_output.stat().st_size / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Create summary statistics\n",
    "    summary = pd.DataFrame([\n",
    "        {\n",
    "            'period': r['period'],\n",
    "            'center_date': r['center_date'].strftime('%Y-%m-%d'),\n",
    "            'n_s1_scenes': r['n_s1_scenes'],\n",
    "            'n_s2_scenes': r['n_s2_scenes']\n",
    "        }\n",
    "        for r in successful_results\n",
    "    ])\n",
    "    \n",
    "    summary_file = OUTPUT_DIR / 'processing_summary.csv'\n",
    "    summary.to_csv(summary_file, index=False)\n",
    "    \n",
    "    print(f\"\\nüìã Summary saved to: {summary_file}\")\n",
    "    print(f\"\\n{summary}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No successful results to create time series\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 10. Visualization and Quality Check\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "if 'final_timeseries' in locals() and final_timeseries is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä CREATING VISUALIZATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Row 1: Mean values\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    final_timeseries['VV'].mean(dim='t').plot(ax=ax1, cmap='gray')\n",
    "    ax1.set_title('Mean VV (S1)', fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    final_timeseries['VH'].mean(dim='t').plot(ax=ax2, cmap='gray')\n",
    "    ax2.set_title('Mean VH (S1)', fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    final_timeseries['S2ndvi'].mean(dim='t').plot(ax=ax3, cmap='RdYlGn', vmin=-0.2, vmax=0.8)\n",
    "    ax3.set_title('Mean NDVI (S2)', fontweight='bold')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    final_timeseries['S2ndvi'].std(dim='t').plot(ax=ax4, cmap='viridis')\n",
    "    ax4.set_title('NDVI Std Dev', fontweight='bold')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Row 2: Data coverage\n",
    "    ax5 = fig.add_subplot(gs[1, :2])\n",
    "    vv_coverage = (~np.isnan(final_timeseries['VV'])).sum(dim=['y', 'x'])\n",
    "    ax5.plot(range(len(vv_coverage)), vv_coverage.values, 'o-', label='VV', linewidth=2)\n",
    "    vh_coverage = (~np.isnan(final_timeseries['VH'])).sum(dim=['y', 'x'])\n",
    "    ax5.plot(range(len(vh_coverage)), vh_coverage.values, 's-', label='VH', linewidth=2)\n",
    "    ax5.set_xlabel('Period')\n",
    "    ax5.set_ylabel('Valid Pixels')\n",
    "    ax5.set_title('S1 Data Coverage', fontweight='bold')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax6 = fig.add_subplot(gs[1, 2:])\n",
    "    ndvi_coverage = (~np.isnan(final_timeseries['S2ndvi'])).sum(dim=['y', 'x'])\n",
    "    ax6.plot(range(len(ndvi_coverage)), ndvi_coverage.values, '^-', color='green', linewidth=2)\n",
    "    ax6.set_xlabel('Period')\n",
    "    ax6.set_ylabel('Valid Pixels')\n",
    "    ax6.set_title('S2 NDVI Coverage', fontweight='bold')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 3: Sample time series\n",
    "    ax7 = fig.add_subplot(gs[2, :])\n",
    "    \n",
    "    # Extract time series at center pixel\n",
    "    ny, nx = final_timeseries.dims['y'], final_timeseries.dims['x']\n",
    "    center_y, center_x = ny // 2, nx // 2\n",
    "    \n",
    "    vv_ts = final_timeseries['VV'].isel(y=center_y, x=center_x).values\n",
    "    vh_ts = final_timeseries['VH'].isel(y=center_y, x=center_x).values\n",
    "    ndvi_ts = final_timeseries['S2ndvi'].isel(y=center_y, x=center_x).values\n",
    "    \n",
    "    ax7_twin = ax7.twinx()\n",
    "    \n",
    "    ax7.plot(range(len(vv_ts)), vv_ts, 'o-', label='VV', color='blue', linewidth=2)\n",
    "    ax7.plot(range(len(vh_ts)), vh_ts, 's-', label='VH', color='cyan', linewidth=2)\n",
    "    ax7_twin.plot(range(len(ndvi_ts)), ndvi_ts, '^-', label='NDVI', color='green', linewidth=2)\n",
    "    \n",
    "    ax7.set_xlabel('Period', fontsize=12)\n",
    "    ax7.set_ylabel('S1 Backscatter (dB)', fontsize=12)\n",
    "    ax7_twin.set_ylabel('NDVI', fontsize=12, color='green')\n",
    "    ax7.set_title(f'Sample Time Series (Center Pixel: y={center_y}, x={center_x})', fontweight='bold')\n",
    "    ax7.legend(loc='upper left')\n",
    "    ax7_twin.legend(loc='upper right')\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Klambu-Glapan S1/S2 Time Series Analysis\\n{START_DATE} to {END_DATE} ({len(successful_results)} periods)',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.savefig(OUTPUT_DIR / 'final_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Visualization saved: {OUTPUT_DIR / 'final_analysis.png'}\")\n",
    "    \n",
    "    # Print data quality summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìä DATA QUALITY SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for var in ['VV', 'VH', 'S2ndvi']:\n",
    "        total = final_timeseries[var].size\n",
    "        valid = np.isfinite(final_timeseries[var]).sum().values\n",
    "        coverage = 100 * valid / total\n",
    "        print(f\"   {var:8s}: {valid:,} / {total:,} pixels ({coverage:.1f}% coverage)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data preparation complete!\")\n",
    "    print(f\"\\nüìÅ Output files in: {OUTPUT_DIR}/\")\n",
    "    print(f\"   ‚Ä¢ mpc_s1_s2_klambu_glapan_{START_DATE}_{END_DATE}_final.nc\")\n",
    "    print(f\"   ‚Ä¢ processing_summary.csv\")\n",
    "    print(f\"   ‚Ä¢ final_analysis.png\")\n",
    "    print(f\"   ‚Ä¢ mpc_study_area.png\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No final time series available for visualization\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 11. Prepare for FuseTS MOGPR Processing\n",
    "\n",
    "The data is now ready to use with FuseTS! Here's how to load and process it:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Example: Load the data in a FuseTS workflow\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìñ FUSETS USAGE EXAMPLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "example_code = \"\"\"\n",
    "# ============================================================================\n",
    "# EXAMPLE: Using MPC data with FuseTS MOGPR\n",
    "# ============================================================================\n",
    "\n",
    "import xarray as xr\n",
    "from fusets.mogpr import MOGPRTransformer\n",
    "from fusets.analytics import phenology\n",
    "\n",
    "# 1. Load the MPC time series data\n",
    "data_file = 'mpc_fusets_data/mpc_s1_s2_klambu_glapan_2023-11-01_2025-11-07_final.nc'\n",
    "fused_data = xr.open_dataset(data_file)\n",
    "\n",
    "print(\"Loaded MPC data:\")\n",
    "print(fused_data)\n",
    "\n",
    "# 2. Apply MOGPR fusion (optional - data is already gap-filled)\n",
    "mogpr = MOGPRTransformer()\n",
    "fused_result = mogpr.fit_transform(fused_data)\n",
    "\n",
    "# 3. Extract phenology metrics for Indonesian agricultural seasons\n",
    "phenology_metrics = phenology(fused_result['S2ndvi'])\n",
    "\n",
    "# Access results\n",
    "sos_times = phenology_metrics.da_sos_times  # Start of Season\n",
    "pos_times = phenology_metrics.da_pos_times  # Peak of Season\n",
    "eos_times = phenology_metrics.da_eos_times  # End of Season\n",
    "\n",
    "print(\"\\\\nPhenology extraction complete!\")\n",
    "print(f\"Detected seasons across {len(fused_data.t)} periods\")\n",
    "\n",
    "# 4. Multi-season detection (as you did in previous notebook)\n",
    "# ... (use your existing multi-season detection code)\n",
    "\"\"\"\n",
    "\n",
    "print(example_code)\n",
    "\n",
    "# Save example script\n",
    "example_file = OUTPUT_DIR / 'fusets_usage_example.py'\n",
    "with open(example_file, 'w') as f:\n",
    "    f.write(example_code)\n",
    "\n",
    "print(f\"\\nüíæ Example script saved to: {example_file}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ ALL PROCESSING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nYour data is ready for FuseTS MOGPR processing!\")\n",
    "print(f\"Key advantages of MPC data:\")\n",
    "print(f\"  ‚Ä¢ No GEE authentication needed\")\n",
    "print(f\"  ‚Ä¢ Direct Python integration\")\n",
    "print(f\"  ‚Ä¢ Cloud-optimized GeoTIFFs\")\n",
    "print(f\"  ‚Ä¢ Analysis-ready data (ARD)\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Load the .nc file in your MOGPR notebook\")\n",
    "print(f\"  2. Apply gap-filling or smoothing if needed\")\n",
    "print(f\"  3. Extract phenology metrics\")\n",
    "print(f\"  4. Detect multi-season patterns\")\n",
    "print(f\"\\nHappy analyzing! üåæüìä\")\n",
    "</VSCode.Cell>\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
