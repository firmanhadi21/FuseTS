{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Installation for Google Colab\n",
    "\n",
    "**Run this cell first if using Google Colab:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Install FuseTS from GitHub repository (for Google Colab)\n",
    "# ============================================================================\n",
    "# Only needed in Google Colab - comment out if running locally\n",
    "\n",
    "# Step 1: Install compatible dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install --upgrade pip setuptools wheel -q\n",
    "\n",
    "# Install core dependencies with compatible versions for Python 3.10+\n",
    "!pip install \"numpy>=1.23.5,<2.0\" cython matplotlib scipy>=1.14.0 -q\n",
    "!pip install GPy vam.whittaker xarray openeo shapely rioxarray geemap earthengine-api -q\n",
    "!pip install lcmap-pyccd -q\n",
    "\n",
    "# Step 2: Clone FuseTS and patch both setup.cfg and pyproject.toml\n",
    "print(\"\\nüîß Setting up FuseTS...\")\n",
    "!rm -rf FuseTS_temp  # Clean up any previous attempts\n",
    "!git clone https://github.com/firmanhadi21/FuseTS.git FuseTS_temp -q\n",
    "\n",
    "# Patch setup.cfg to allow newer numpy versions\n",
    "setup_cfg_path = 'FuseTS_temp/setup.cfg'\n",
    "with open(setup_cfg_path, 'r') as f:\n",
    "    content = f.read()\n",
    "content = content.replace('numpy==1.23.5', 'numpy>=1.23.5,<2.0')\n",
    "with open(setup_cfg_path, 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"‚úÖ Patched setup.cfg\")\n",
    "\n",
    "# Patch pyproject.toml to allow newer numpy versions (this is the key fix!)\n",
    "pyproject_path = 'FuseTS_temp/pyproject.toml'\n",
    "with open(pyproject_path, 'r') as f:\n",
    "    content = f.read()\n",
    "content = content.replace('numpy==1.23.5', 'numpy>=1.23.5')\n",
    "with open(pyproject_path, 'w') as f:\n",
    "    f.write(content)\n",
    "print(\"‚úÖ Patched pyproject.toml\")\n",
    "\n",
    "# Step 3: Install FuseTS\n",
    "print(\"\\nüöÄ Installing FuseTS...\")\n",
    "import os\n",
    "os.chdir('FuseTS_temp')\n",
    "!pip install -e . -q\n",
    "os.chdir('..')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ FuseTS installation complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: Please restart the runtime now:\")\n",
    "print(\"   Runtime > Restart runtime\")\n",
    "print(\"\\n   Then skip this cell and continue from the next cell.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-1 and Sentinel-2 Data Fusion using MOGPR\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load Sentinel-1 (SAR) and Sentinel-2 (optical) time series data\n",
    "2. Apply MOGPR (Multi-Output Gaussian Process Regression) fusion\n",
    "3. Extract phenological metrics from fused data\n",
    "\n",
    "## üìù Important Notes\n",
    "\n",
    "**For Google Colab users:**\n",
    "- ‚úÖ Run the installation cell first (Cell 2)\n",
    "- ‚úÖ Restart runtime after installation\n",
    "- ‚úÖ All code in this notebook is **Python** (not JavaScript!)\n",
    "\n",
    "**If you see JavaScript errors:**\n",
    "- ‚ùå Don't run JavaScript code from `GEE_Data_Preparation_for_FuseTS_Assets.ipynb` here\n",
    "- ‚úÖ That notebook has JavaScript examples for GEE Code Editor (use in browser)\n",
    "- ‚úÖ This notebook only uses Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation and Loading\n",
    "\n",
    "For this tutorial, we'll create synthetic S1 and S2 time series data. In practice, you would load your actual GeoTIFF stacks or data from other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FuseTS imports\n",
    "from fusets.mogpr import MOGPRTransformer\n",
    "from fusets.analytics import phenology\n",
    "from fusets import whittaker\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data (Option B - For Testing Only)\n",
    "\n",
    "**Skip this section if you loaded GEE Assets above!**\n",
    "\n",
    "This section generates synthetic time series for demonstration purposes. Only run this if `USE_GEE_ASSETS=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for MOGPR Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if NOT using GEE Assets\n",
    "if not USE_GEE_ASSETS or gee_dataset is None:\n",
    "    \n",
    "    def generate_synthetic_timeseries(start_date='2023-01-01', end_date='2023-12-31', \n",
    "                                    spatial_size=(50, 50), temporal_freq='5D'):\n",
    "        \"\"\"\n",
    "        Generate synthetic S1 and S2 time series data for demonstration\n",
    "        \"\"\"\n",
    "        # Create time index\n",
    "        time_index = pd.date_range(start_date, end_date, freq=temporal_freq)\n",
    "        n_times = len(time_index)\n",
    "        \n",
    "        # Spatial coordinates\n",
    "        y_coords = np.arange(spatial_size[0])\n",
    "        x_coords = np.arange(spatial_size[1])\n",
    "        \n",
    "        # Generate synthetic seasonal patterns\n",
    "        day_of_year = np.array([t.dayofyear for t in time_index])\n",
    "        \n",
    "        # Base seasonal cycle (simulating vegetation growth)\n",
    "        seasonal_cycle = 0.5 * (1 + np.sin(2 * np.pi * (day_of_year - 80) / 365))\n",
    "        \n",
    "        # Generate S2 NDVI (optical, weather dependent - more gaps)\n",
    "        s2_ndvi = np.zeros((n_times, spatial_size[0], spatial_size[1]))\n",
    "        for i, y in enumerate(y_coords):\n",
    "            for j, x in enumerate(x_coords):\n",
    "                # Add spatial variability\n",
    "                spatial_factor = 0.3 + 0.7 * np.sin(y/10) * np.cos(x/10)\n",
    "                \n",
    "                # Base NDVI with seasonal pattern\n",
    "                base_ndvi = 0.2 + 0.6 * seasonal_cycle * spatial_factor\n",
    "                \n",
    "                # Add noise\n",
    "                noise = np.random.normal(0, 0.05, n_times)\n",
    "                s2_ndvi[:, i, j] = base_ndvi + noise\n",
    "                \n",
    "                # Simulate cloud gaps (20% missing data)\n",
    "                cloud_mask = np.random.random(n_times) < 0.2\n",
    "                s2_ndvi[cloud_mask, i, j] = np.nan\n",
    "        \n",
    "        # Generate S1 VV data (SAR - weather independent, correlated with vegetation)\n",
    "        s1_vv = np.zeros((n_times, spatial_size[0], spatial_size[1]))\n",
    "        for i, y in enumerate(y_coords):\n",
    "            for j, x in enumerate(x_coords):\n",
    "                spatial_factor = 0.3 + 0.7 * np.sin(y/10) * np.cos(x/10)\n",
    "                \n",
    "                # VV decreases with vegetation growth (volume scattering)\n",
    "                base_vv = -15 - 5 * seasonal_cycle * spatial_factor\n",
    "                noise = np.random.normal(0, 1.0, n_times)\n",
    "                s1_vv[:, i, j] = base_vv + noise\n",
    "        \n",
    "        # Generate S1 VH data (cross-polarization)\n",
    "        s1_vh = np.zeros((n_times, spatial_size[0], spatial_size[1]))\n",
    "        for i, y in enumerate(y_coords):\n",
    "            for j, x in enumerate(x_coords):\n",
    "                spatial_factor = 0.3 + 0.7 * np.sin(y/10) * np.cos(x/10)\n",
    "                \n",
    "                # VH increases with vegetation growth\n",
    "                base_vh = -25 + 3 * seasonal_cycle * spatial_factor\n",
    "                noise = np.random.normal(0, 1.5, n_times)\n",
    "                s1_vh[:, i, j] = base_vh + noise\n",
    "        \n",
    "        return time_index, y_coords, x_coords, s1_vv, s1_vh, s2_ndvi\n",
    "\n",
    "    # Generate synthetic data\n",
    "    print(\"Generating synthetic time series data...\")\n",
    "    time_idx, y_coords, x_coords, vv_data, vh_data, ndvi_data = generate_synthetic_timeseries()\n",
    "\n",
    "    print(f\"Generated data shapes:\")\n",
    "    print(f\"Time series length: {len(time_idx)} observations\")\n",
    "    print(f\"Spatial dimensions: {len(y_coords)} x {len(x_coords)} pixels\")\n",
    "    print(f\"VV data shape: {vv_data.shape}\")\n",
    "    print(f\"VH data shape: {vh_data.shape}\")\n",
    "    print(f\"NDVI data shape: {ndvi_data.shape}\")\n",
    "else:\n",
    "    print(\"‚úÖ Using GEE Assets data - skipping synthetic data generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data from GEE Assets (Real Data Required)\n",
    "\n",
    "‚ö†Ô∏è **STOP! Assets don't exist yet?** Follow these steps first:\n",
    "\n",
    "### üìã Prerequisites: Export Data to GEE Assets First\n",
    "\n",
    "**Before running this notebook, you must:**\n",
    "\n",
    "1. **Open** `GEE_Data_Preparation_for_FuseTS_Assets.ipynb` \n",
    "2. **Run all cells** to export S1/S2 data to GEE Assets\n",
    "3. **Wait for exports to complete** (check https://code.earthengine.google.com/tasks)\n",
    "4. **Verify assets exist** at: `projects/ee-geodeticengineeringundip/assets/FuseTS/`\n",
    "\n",
    "### üîÑ Export Status Check\n",
    "\n",
    "Go to https://code.earthengine.google.com/tasks and ensure:\n",
    "- ‚úÖ All 31 period exports show \"Completed\" status\n",
    "- ‚úÖ Assets are visible in your GEE asset folder\n",
    "- ‚úÖ You have read permissions for the assets\n",
    "\n",
    "### üì• After Exports Complete\n",
    "\n",
    "Once data is exported to GEE Assets:\n",
    "1. Come back to this notebook\n",
    "2. Set `USE_GEE_ASSETS = True` below\n",
    "3. Run the data loading cell\n",
    "4. Continue with MOGPR analysis\n",
    "\n",
    "**Estimated export time:** 2-6 hours for full Java Island (depends on GEE queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Quick Check: Do Assets Exist?\n",
    "\n",
    "Run this cell to verify your GEE Assets are ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: Verify if GEE Assets exist\n",
    "import ee\n",
    "\n",
    "try:\n",
    "    # Initialize GEE\n",
    "    try:\n",
    "        ee.Initialize(project='ee-geodeticengineeringundip')\n",
    "    except:\n",
    "        ee.Authenticate()\n",
    "        ee.Initialize(project='ee-geodeticengineeringundip')\n",
    "    \n",
    "    # Check for individual assets (they're not a collection, but separate images)\n",
    "    asset_base = 'projects/ee-geodeticengineeringundip/assets/FuseTS/S1_S2_Nov2024_Oct2025_Period_'\n",
    "    \n",
    "    # Try to load first few periods to verify\n",
    "    found_periods = []\n",
    "    missing_periods = []\n",
    "    \n",
    "    print(\"üîç Checking for individual asset images...\")\n",
    "    for period in range(1, 32):  # Check all 31 periods\n",
    "        asset_id = f'{asset_base}{period:02d}'\n",
    "        try:\n",
    "            img = ee.Image(asset_id)\n",
    "            # Try to get info to verify it exists\n",
    "            img.bandNames().getInfo()\n",
    "            found_periods.append(period)\n",
    "        except:\n",
    "            missing_periods.append(period)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    if len(found_periods) > 0:\n",
    "        print(\"‚úÖ SUCCESS! GEE Assets found!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"üìä Found {len(found_periods)}/31 period images\")\n",
    "        print(f\"üìÅ Asset base: {asset_base}\")\n",
    "        print(f\"   Example: {asset_base}01, {asset_base}02, ...\")\n",
    "        \n",
    "        if len(missing_periods) > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  Missing {len(missing_periods)} periods: {missing_periods[:5]}{'...' if len(missing_periods) > 5 else ''}\")\n",
    "            print(f\"   Check export status at: https://code.earthengine.google.com/tasks\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ All 31 periods complete!\")\n",
    "        \n",
    "        print(\"\\n‚úÖ You can proceed with data loading below!\")\n",
    "        print(\"   The code will load these individual assets and combine them\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        raise Exception(\"No assets found\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"=\"*70)\n",
    "    print(\"‚ùå ASSETS NOT FOUND\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nüîß NEXT STEPS:\")\n",
    "    print(\"1. Open GEE_Data_Preparation_for_FuseTS_Assets.ipynb\")\n",
    "    print(\"2. Run all cells to export data to GEE Assets\")\n",
    "    print(\"3. Monitor exports at: https://code.earthengine.google.com/tasks\")\n",
    "    print(\"4. Wait for all 31 period exports to complete\")\n",
    "    print(\"5. Come back here and run this check again\")\n",
    "    print(\"\\n‚è±Ô∏è  Export typically takes 2-6 hours for Java Island\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Diagnostic: Check Asset Coverage\n",
    "\n",
    "Run this to see the actual bounds of your exported assets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the geographic extent of your assets\n",
    "import ee\n",
    "\n",
    "try:\n",
    "    ee.Initialize(project='ee-geodeticengineeringundip')\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project='ee-geodeticengineeringundip')\n",
    "\n",
    "# Load first period to check extent\n",
    "asset_id = 'projects/ee-geodeticengineeringundip/assets/FuseTS/S1_S2_Nov2024_Oct2025_Period_01'\n",
    "img = ee.Image(asset_id)\n",
    "\n",
    "# Get bounds\n",
    "bounds = img.geometry().bounds().getInfo()\n",
    "coords = bounds['coordinates'][0]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìç ASSET GEOGRAPHIC EXTENT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Asset: {asset_id}\")\n",
    "print(f\"\\nBounds (WGS84):\")\n",
    "print(f\"  West:  {min([c[0] for c in coords]):.4f}¬∞E\")\n",
    "print(f\"  East:  {max([c[0] for c in coords]):.4f}¬∞E\")\n",
    "print(f\"  South: {min([c[1] for c in coords]):.4f}¬∞N\")\n",
    "print(f\"  North: {max([c[1] for c in coords]):.4f}¬∞N\")\n",
    "\n",
    "print(f\"\\nüìä Java Island Reference Coordinates:\")\n",
    "print(f\"  Full Java: 105.0¬∞E to 115.0¬∞E, -8.8¬∞N to -5.6¬∞N\")\n",
    "print(f\"  Western Java (Banten): ~105.0¬∞E to 106.5¬∞E\")\n",
    "print(f\"  Central Java: ~106.5¬∞E to 111.0¬∞E\")\n",
    "print(f\"  Eastern Java: ~111.0¬∞E to 115.0¬∞E\")\n",
    "\n",
    "# Check if western Java is included\n",
    "west_bound = min([c[0] for c in coords])\n",
    "if west_bound > 105.5:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Western boundary at {west_bound:.4f}¬∞E\")\n",
    "    print(f\"   Missing western Java (should start around 105.0¬∞E)\")\n",
    "    print(f\"\\nüîß SOLUTION:\")\n",
    "    print(f\"   1. Check java_island_mask.tif covers western regions\")\n",
    "    print(f\"   2. Re-export with correct study area bounds\")\n",
    "    print(f\"   3. Ensure export region includes longitude >= 105.0¬∞E\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Coverage looks good - includes western Java\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize bounds on map\n",
    "import folium\n",
    "center_lat = (min([c[1] for c in coords]) + max([c[1] for c in coords])) / 2\n",
    "center_lon = (min([c[0] for c in coords]) + max([c[0] for c in coords])) / 2\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=7)\n",
    "\n",
    "# Add asset bounds\n",
    "folium.Polygon(\n",
    "    locations=[(c[1], c[0]) for c in coords],\n",
    "    color='blue',\n",
    "    fill=True,\n",
    "    fillColor='blue',\n",
    "    fillOpacity=0.2,\n",
    "    popup='Asset Coverage'\n",
    ").add_to(m)\n",
    "\n",
    "# Add reference points for Java regions\n",
    "folium.Marker([center_lat, 105.5], popup='Western Java (Banten)', icon=folium.Icon(color='red')).add_to(m)\n",
    "folium.Marker([center_lat, 109.0], popup='Central Java', icon=folium.Icon(color='green')).add_to(m)\n",
    "folium.Marker([center_lat, 113.0], popup='Eastern Java', icon=folium.Icon(color='orange')).add_to(m)\n",
    "\n",
    "print(\"\\nüó∫Ô∏è  Interactive map generated below:\")\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Check Actual Data Coverage (Not Just Bounds)\n",
    "\n",
    "This will sample actual pixel values across Java to find missing data areas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample actual pixel data across Java to check for missing regions\n",
    "import ee\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    ee.Initialize(project='ee-geodeticengineeringundip')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî¨ CHECKING ACTUAL DATA COVERAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load first period\n",
    "asset_id = 'projects/ee-geodeticengineeringundip/assets/FuseTS/S1_S2_Nov2024_Oct2025_Period_01'\n",
    "img = ee.Image(asset_id)\n",
    "\n",
    "# Define sample points across Java (west to east)\n",
    "sample_points = [\n",
    "    {'name': 'Western Java (Banten)', 'lon': 105.5, 'lat': -6.5},\n",
    "    {'name': 'West Java', 'lon': 107.0, 'lat': -6.8},\n",
    "    {'name': 'Central Java', 'lon': 109.5, 'lat': -7.2},\n",
    "    {'name': 'Central Java (Coast)', 'lon': 110.5, 'lat': -6.9},\n",
    "    {'name': 'Eastern Java', 'lon': 112.5, 'lat': -7.5},\n",
    "    {'name': 'East Java (Tip)', 'lon': 114.0, 'lat': -7.8},\n",
    "]\n",
    "\n",
    "print(\"\\nüìç Sampling pixel values at key locations:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "missing_regions = []\n",
    "valid_regions = []\n",
    "\n",
    "for point in sample_points:\n",
    "    try:\n",
    "        # Sample at this location\n",
    "        sample = img.sample(\n",
    "            region=ee.Geometry.Point([point['lon'], point['lat']]),\n",
    "            scale=50,\n",
    "            projection='EPSG:4326',\n",
    "            geometries=True\n",
    "        ).first()\n",
    "        \n",
    "        # Get values\n",
    "        vv = sample.get('VV').getInfo()\n",
    "        vh = sample.get('VH').getInfo()\n",
    "        ndvi = sample.get('S2ndvi').getInfo()\n",
    "        \n",
    "        # Check if data exists\n",
    "        if vv is None or vh is None or ndvi is None:\n",
    "            status = \"‚ùå NO DATA\"\n",
    "            missing_regions.append(point['name'])\n",
    "            print(f\"{status:15} {point['name']:25} ({point['lon']:.2f}¬∞E, {point['lat']:.2f}¬∞N)\")\n",
    "        else:\n",
    "            status = \"‚úÖ HAS DATA\"\n",
    "            valid_regions.append(point['name'])\n",
    "            print(f\"{status:15} {point['name']:25} ({point['lon']:.2f}¬∞E, {point['lat']:.2f}¬∞N)\")\n",
    "            print(f\"                   VV={vv:.2f} dB, VH={vh:.2f} dB, NDVI={ndvi:.3f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        status = \"‚ùå ERROR\"\n",
    "        missing_regions.append(point['name'])\n",
    "        print(f\"{status:15} {point['name']:25} ({point['lon']:.2f}¬∞E, {point['lat']:.2f}¬∞N)\")\n",
    "        print(f\"                   Error: {str(e)[:50]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Regions with data: {len(valid_regions)}/{len(sample_points)}\")\n",
    "if valid_regions:\n",
    "    for region in valid_regions:\n",
    "        print(f\"   ‚úì {region}\")\n",
    "\n",
    "if missing_regions:\n",
    "    print(f\"\\n‚ùå Regions WITHOUT data: {len(missing_regions)}/{len(sample_points)}\")\n",
    "    for region in missing_regions:\n",
    "        print(f\"   ‚úó {region}\")\n",
    "    \n",
    "    print(f\"\\nüîß ISSUE IDENTIFIED:\")\n",
    "    print(f\"   The assets exist but don't contain actual pixel data for:\")\n",
    "    for region in missing_regions:\n",
    "        print(f\"   ‚Ä¢ {region}\")\n",
    "    \n",
    "    print(f\"\\nüí° SOLUTION:\")\n",
    "    print(f\"   This means the export process didn't capture data for these regions.\")\n",
    "    print(f\"   Possible causes:\")\n",
    "    print(f\"   1. Java Island mask doesn't cover these areas\")\n",
    "    print(f\"   2. Export 'region' parameter was too restrictive\")\n",
    "    print(f\"   3. No Sentinel data available for these locations during export\")\n",
    "    print(f\"\\n   Fix in GEE_Data_Preparation_for_FuseTS_Assets.ipynb:\")\n",
    "    print(f\"   ‚Ä¢ Verify java_island_mask.tif covers ALL of Java\")\n",
    "    print(f\"   ‚Ä¢ Check study_area geometry includes missing regions\")\n",
    "    print(f\"   ‚Ä¢ Re-export with corrected mask/region\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All sampled regions have data!\")\n",
    "    print(f\"   If you still see missing areas, they may be:\")\n",
    "    print(f\"   ‚Ä¢ Between sample points (try more samples)\")\n",
    "    print(f\"   ‚Ä¢ Areas with genuine no-data (water, clouds, etc.)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è CRITICAL ISSUE: Most of Java Missing Data\n",
    "\n",
    "Your assets exist but contain **NO ACTUAL PIXEL DATA** for most of Java Island. Only 2 out of 6 regions have data.\n",
    "\n",
    "### üîç Root Cause Analysis\n",
    "\n",
    "The export process created the assets with correct bounds, but didn't actually export the pixel values. This typically happens when:\n",
    "\n",
    "1. **Java Island mask has holes** - The mask might have gaps where it should be solid\n",
    "2. **Mask value is wrong** - Export used `mask > 0` but mask values might be different\n",
    "3. **Region vs Mask conflict** - Both `region` and `mask` parameters were used incorrectly\n",
    "4. **No data in source** - Sentinel data genuinely missing (unlikely for whole regions)\n",
    "\n",
    "### üîß How to Fix This\n",
    "\n",
    "**You MUST re-export the data. Follow these steps:**\n",
    "\n",
    "#### Step 1: Verify the Mask File\n",
    "Open `java_island_mask.tif` in QGIS/ArcGIS and check:\n",
    "- ‚úì Does it cover ALL of Java (no gaps in middle)?\n",
    "- ‚úì What are the pixel values? (should be 1 for Java, 0 for non-Java)\n",
    "- ‚úì Are there holes in the middle of the island?\n",
    "\n",
    "#### Step 2: Fix the Export Code\n",
    "Go to `GEE_Data_Preparation_for_FuseTS_Assets.ipynb` and modify the export:\n",
    "\n",
    "**Current export likely has:**\n",
    "```python\n",
    "task = ee.batch.Export.image.toAsset(\n",
    "    image=image_with_metadata.updateMask(java_mask),  # ‚Üê Problem here\n",
    "    region=geometry,  # ‚Üê Or problem here\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "**Should be:**\n",
    "```python\n",
    "# Option A: Use mask properly\n",
    "java_mask_binary = java_mask.gt(0)  # Ensure binary mask\n",
    "masked_image = image_with_metadata.updateMask(java_mask_binary)\n",
    "\n",
    "task = ee.batch.Export.image.toAsset(\n",
    "    image=masked_image,\n",
    "    region=java_geometry.bounds(),  # Use mask bounds, not separate region\n",
    "    ...\n",
    ")\n",
    "\n",
    "# Option B: Export full region without mask (recommended for testing)\n",
    "task = ee.batch.Export.image.toAsset(\n",
    "    image=image_with_metadata,  # No mask - export everything\n",
    "    region=ee.Geometry.Rectangle([105.0, -8.8, 115.0, -5.6]),  # Full Java bounds\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "#### Step 3: Delete Old Assets and Re-export\n",
    "```javascript\n",
    "// In GEE Code Editor or Python, delete the bad assets:\n",
    "// Go to: https://code.earthengine.google.com\n",
    "// Assets tab ‚Üí FuseTS folder ‚Üí Delete all Period_XX assets\n",
    "```\n",
    "\n",
    "Then re-run the export with fixed code.\n",
    "\n",
    "### üìã Quick Fix Checklist\n",
    "\n",
    "- [ ] Open `java_island_mask.tif` and verify it's complete\n",
    "- [ ] Open `GEE_Data_Preparation_for_FuseTS_Assets.ipynb`\n",
    "- [ ] Find the export section (Cell 13)\n",
    "- [ ] Check how mask is applied in export\n",
    "- [ ] Delete old assets from GEE\n",
    "- [ ] Re-export with corrected code\n",
    "- [ ] Come back here and run the diagnostic again\n",
    "\n",
    "**Do NOT continue with MOGPR analysis until this is fixed!** The current assets are unusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION A: Load Real Data from GEE Assets\n",
    "# ============================================================================\n",
    "\n",
    "import ee\n",
    "import geemap\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "# Configuration\n",
    "USE_GEE_ASSETS = True  # Set to True to use GEE Assets, False for synthetic data\n",
    "ASSET_BASE_PATH = 'projects/ee-geodeticengineeringundip/assets/FuseTS'\n",
    "ASSET_NAME_PREFIX = 'S1_S2_Nov2024_Oct2025_Period_'  # Changed from pattern to prefix\n",
    "NUM_PERIODS = 31  # Total number of periods to load\n",
    "OUTPUT_LOCAL_DIR = 'gee_assets_download'\n",
    "\n",
    "# Region of interest (optional - download specific area)\n",
    "# Set to None to use full extent from assets\n",
    "# Your assets contain: Kabupaten Demak (~900 km¬≤)\n",
    "USE_SMALL_REGION = False  # Set False to use full asset extent (Demak), True for subset only\n",
    "if USE_SMALL_REGION:\n",
    "    # Example: Even smaller test region within Demak\n",
    "    REGION = ee.Geometry.Rectangle([110.40, -6.95, 110.50, -6.85])  # ~10x10 km\n",
    "    REGION_NAME = 'Demak_Test'\n",
    "else:\n",
    "    # Use full extent from assets (Kabupaten Demak)\n",
    "    REGION = None\n",
    "    REGION_NAME = 'Demak_Full'\n",
    "\n",
    "SCALE = 50  # meters (must match your asset export resolution)\n",
    "\n",
    "def initialize_gee():\n",
    "    \"\"\"Initialize Google Earth Engine\"\"\"\n",
    "    try:\n",
    "        ee.Initialize(project='ee-geodeticengineeringundip')\n",
    "        print(\"‚úÖ Earth Engine initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"Initializing Earth Engine...\")\n",
    "        try:\n",
    "            ee.Authenticate()\n",
    "            ee.Initialize(project='ee-geodeticengineeringundip')\n",
    "            print(\"‚úÖ Earth Engine authenticated and initialized\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Error: {e2}\")\n",
    "            raise\n",
    "\n",
    "def load_gee_assets_to_xarray(asset_base_path, name_prefix, num_periods=31, region=None, scale=50):\n",
    "    \"\"\"\n",
    "    Load individual GEE Asset images and convert to xarray Dataset compatible with FuseTS\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    asset_base_path : str\n",
    "        Base path to GEE assets folder\n",
    "    name_prefix : str\n",
    "        Prefix for asset names (e.g., 'S1_S2_Nov2024_Oct2025_Period_')\n",
    "    num_periods : int\n",
    "        Number of periods to load (default 31)\n",
    "    region : ee.Geometry, optional\n",
    "        Region to download. If None, uses full asset extent\n",
    "    scale : int\n",
    "        Resolution in meters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    xr.Dataset with proper FuseTS format\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    print(f\"üîç Loading {num_periods} individual assets from: {asset_base_path}/\")\n",
    "    print(f\"   Asset name pattern: {name_prefix}01, {name_prefix}02, ...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_LOCAL_DIR, exist_ok=True)\n",
    "    \n",
    "    # Storage for time series data\n",
    "    all_vv = []\n",
    "    all_vh = []\n",
    "    all_ndvi = []\n",
    "    time_coords = []\n",
    "    \n",
    "    # Load first asset to get spatial dimensions\n",
    "    first_asset_id = f'{asset_base_path}/{name_prefix}01'\n",
    "    first_img = ee.Image(first_asset_id)\n",
    "    \n",
    "    # Determine region from first asset if not specified\n",
    "    if region is None:\n",
    "        region = first_img.geometry().bounds()\n",
    "        print(f\"   Using full asset extent\")\n",
    "    \n",
    "    # Get first image info\n",
    "    bands = first_img.bandNames().getInfo()\n",
    "    print(f\"   Bands per period: {bands}\")\n",
    "    print(f\"\\nüì• Downloading {num_periods} periods at {scale}m resolution...\")\n",
    "    print(f\"   This will take several minutes...\")\n",
    "    \n",
    "    # Download each period individually\n",
    "    for period in range(1, num_periods + 1):\n",
    "        asset_id = f'{asset_base_path}/{name_prefix}{period:02d}'\n",
    "        \n",
    "        try:\n",
    "            # Load asset\n",
    "            img = ee.Image(asset_id)\n",
    "            \n",
    "            # Download this period\n",
    "            output_file = os.path.join(OUTPUT_LOCAL_DIR, f'period_{period:02d}.tif')\n",
    "            \n",
    "            if not os.path.exists(output_file):\n",
    "                print(f\"   Downloading period {period}/{num_periods}...\", end=' ')\n",
    "                \n",
    "                try:\n",
    "                    geemap.download_ee_image(\n",
    "                        img,\n",
    "                        filename=output_file,\n",
    "                        region=region,\n",
    "                        scale=scale,\n",
    "                        crs='EPSG:4326'\n",
    "                    )\n",
    "                    print(\"‚úì\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚úó Failed: {e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"   Period {period}/{num_periods}: Using cached file ‚úì\")\n",
    "            \n",
    "            # Load the GeoTIFF\n",
    "            period_data = rioxarray.open_rasterio(output_file)\n",
    "            \n",
    "            # Extract bands (assuming order: VV, VH, S2ndvi or NDVI)\n",
    "            if len(period_data.band) >= 3:\n",
    "                vv = period_data.isel(band=0).values  # First band = VV\n",
    "                vh = period_data.isel(band=1).values  # Second band = VH\n",
    "                ndvi = period_data.isel(band=2).values  # Third band = NDVI/S2ndvi\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Period {period} has only {len(period_data.band)} bands, expected 3\")\n",
    "                continue\n",
    "            \n",
    "            all_vv.append(vv)\n",
    "            all_vh.append(vh)\n",
    "            all_ndvi.append(ndvi)\n",
    "            \n",
    "            # Calculate time coordinate (center of 12-day period starting Nov 1, 2024)\n",
    "            start_date = datetime(2024, 11, 1)\n",
    "            period_center = start_date + timedelta(days=(period-1)*12 + 6)\n",
    "            time_coords.append(period_center)\n",
    "            \n",
    "            # Store spatial coords from first period\n",
    "            if period == 1:\n",
    "                y_coords = period_data.y.values\n",
    "                x_coords = period_data.x.values\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Warning: Could not load period {period}: {e}\")\n",
    "    \n",
    "    # Check if we got any data\n",
    "    if len(all_vv) == 0:\n",
    "        print(\"\\n‚ùå No data loaded!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully loaded {len(all_vv)} periods\")\n",
    "    \n",
    "    # Stack into numpy arrays\n",
    "    vv_stack = np.stack(all_vv, axis=0)  # Shape: (time, y, x)\n",
    "    vh_stack = np.stack(all_vh, axis=0)\n",
    "    ndvi_stack = np.stack(all_ndvi, axis=0)\n",
    "    \n",
    "    # Create FuseTS-compatible xarray Dataset\n",
    "    ds = xr.Dataset({\n",
    "        'VV': (['t', 'y', 'x'], vv_stack),\n",
    "        'VH': (['t', 'y', 'x'], vh_stack),\n",
    "        'S2ndvi': (['t', 'y', 'x'], ndvi_stack)\n",
    "    }, coords={\n",
    "        't': time_coords,\n",
    "        'y': y_coords,\n",
    "        'x': x_coords\n",
    "    })\n",
    "    \n",
    "    # Add metadata\n",
    "    ds.attrs.update({\n",
    "        'title': 'Sentinel-1/2 Time Series from GEE Assets',\n",
    "        'source': f'{asset_base_path}/{name_prefix}',\n",
    "        'temporal_resolution': '12-day composites',\n",
    "        'spatial_resolution': f'{scale}m',\n",
    "        'date_range': f'{time_coords[0].strftime(\"%Y-%m-%d\")} to {time_coords[-1].strftime(\"%Y-%m-%d\")}',\n",
    "        'num_periods': len(time_coords),\n",
    "        'region': 'Kabupaten Demak',\n",
    "        'crs': 'EPSG:4326',\n",
    "        'fusets_ready': True\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Summary:\")\n",
    "    print(f\"   Shape: {ds.VV.shape} (time, y, x)\")\n",
    "    print(f\"   Time range: {time_coords[0].strftime('%Y-%m-%d')} to {time_coords[-1].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Spatial extent: {len(y_coords)} x {len(x_coords)} pixels\")\n",
    "    print(f\"   Bands: VV, VH, S2ndvi\")\n",
    "    \n",
    "    return ds\n",
    "    \n",
    "    # Create ImageCollection from list\n",
    "    collection = ee.ImageCollection(image_list)\n",
    "    \n",
    "    # Get collection info\n",
    "    count = len(image_list)\n",
    "    print(f\"   Successfully loaded {count}/{num_periods} images\")\n",
    "    \n",
    "    if count == 0:\n",
    "        print(\"‚ùå No images found! Check:\")\n",
    "        print(f\"   1. Asset path: {full_pattern}\")\n",
    "        print(f\"   2. Asset permissions (must be readable)\")\n",
    "        print(f\"   3. Export tasks completed successfully\")\n",
    "        return None\n",
    "    \n",
    "    # Get first image to check bands\n",
    "    first = ee.Image(collection.first())\n",
    "    bands = first.bandNames().getInfo()\n",
    "    print(f\"   Bands found: {bands}\")\n",
    "    \n",
    "    # Determine region\n",
    "    if region is None:\n",
    "        region = collection.geometry().bounds()\n",
    "        print(f\"   Using full asset extent\")\n",
    "    else:\n",
    "        print(f\"   Using custom region: {region.bounds().getInfo()}\")\n",
    "    \n",
    "    # Download collection as multi-band image\n",
    "    print(f\"\\nüì• Downloading data at {scale}m resolution...\")\n",
    "    print(f\"   This may take a few minutes depending on region size...\")\n",
    "    \n",
    "    # Convert collection to multi-band image with period labels\n",
    "    image_list = collection.toList(count)\n",
    "    \n",
    "    def add_period_suffix(image):\n",
    "        img = ee.Image(image)\n",
    "        period = ee.Number(img.get('period')).format('%02d')\n",
    "        old_names = img.bandNames()\n",
    "        new_names = old_names.map(lambda name: ee.String(name).cat('_P').cat(period))\n",
    "        return img.rename(new_names).copyProperties(img, img.propertyNames())\n",
    "    \n",
    "    renamed_collection = collection.map(add_period_suffix)\n",
    "    multi_band_image = renamed_collection.toBands()\n",
    "    \n",
    "    # Use geemap for efficient download\n",
    "    import os\n",
    "    os.makedirs(OUTPUT_LOCAL_DIR, exist_ok=True)\n",
    "    \n",
    "    output_file = os.path.join(OUTPUT_LOCAL_DIR, f'{REGION_NAME}_S1_S2_timeseries.tif')\n",
    "    \n",
    "    # Download using geemap\n",
    "    try:\n",
    "        geemap.download_ee_image(\n",
    "            multi_band_image,\n",
    "            filename=output_file,\n",
    "            region=region,\n",
    "            scale=scale,\n",
    "            crs='EPSG:4326'\n",
    "        )\n",
    "        print(f\"‚úÖ Downloaded to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Direct download failed: {e}\")\n",
    "        print(f\"   Trying alternative download method...\")\n",
    "        \n",
    "        # Alternative: get download URL\n",
    "        url = multi_band_image.getDownloadURL({\n",
    "            'scale': scale,\n",
    "            'crs': 'EPSG:4326',\n",
    "            'region': region,\n",
    "            'format': 'GEO_TIFF'\n",
    "        })\n",
    "        print(f\"\\nüìé Download URL generated:\")\n",
    "        print(f\"   {url}\")\n",
    "        print(f\"\\n   Copy this URL to your browser to download the file\")\n",
    "        print(f\"   Then save it as: {output_file}\")\n",
    "        print(f\"   After download completes, re-run this cell to load the data\")\n",
    "        \n",
    "        # Check if file exists from previous download\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"\\n‚úÖ Found existing file: {output_file}\")\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # Load the downloaded GeoTIFF\n",
    "    print(f\"\\nüìñ Loading GeoTIFF into xarray...\")\n",
    "    data_raster = rioxarray.open_rasterio(output_file)\n",
    "    \n",
    "    # Parse band names to extract periods and create proper dataset\n",
    "    band_names = [str(b) for b in data_raster.band.values]\n",
    "    print(f\"   Total bands: {len(band_names)}\")\n",
    "    \n",
    "    # Parse periods from band names (format: VV_P01, VH_P01, S2ndvi_P01, etc.)\n",
    "    periods = sorted(list(set([int(b.split('_P')[1]) for b in band_names if '_P' in b])))\n",
    "    n_periods = len(periods)\n",
    "    print(f\"   Periods found: {n_periods} ({min(periods)} to {max(periods)})\")\n",
    "    \n",
    "    # Reorganize into proper time series format\n",
    "    vv_data = []\n",
    "    vh_data = []\n",
    "    ndvi_data = []\n",
    "    \n",
    "    for period in periods:\n",
    "        # Extract bands for this period\n",
    "        vv_band = f'VV_P{period:02d}'\n",
    "        vh_band = f'VH_P{period:02d}'\n",
    "        ndvi_band = f'S2ndvi_P{period:02d}'\n",
    "        \n",
    "        # Find band indices\n",
    "        vv_idx = [i for i, b in enumerate(band_names) if b.endswith(vv_band)]\n",
    "        vh_idx = [i for i, b in enumerate(band_names) if b.endswith(vh_band)]\n",
    "        ndvi_idx = [i for i, b in enumerate(band_names) if b.endswith(ndvi_band)]\n",
    "        \n",
    "        if vv_idx and vh_idx and ndvi_idx:\n",
    "            vv_data.append(data_raster.isel(band=vv_idx[0]))\n",
    "            vh_data.append(data_raster.isel(band=vh_idx[0]))\n",
    "            ndvi_data.append(data_raster.isel(band=ndvi_idx[0]))\n",
    "    \n",
    "    # Stack into time dimension\n",
    "    vv_array = xr.concat(vv_data, dim='time')\n",
    "    vh_array = xr.concat(vh_data, dim='time')\n",
    "    ndvi_array = xr.concat(ndvi_data, dim='time')\n",
    "    \n",
    "    # Create time coordinates (assuming 12-day periods starting Nov 1, 2024)\n",
    "    from datetime import datetime, timedelta\n",
    "    start_date = datetime(2024, 11, 1)\n",
    "    time_coords = [start_date + timedelta(days=(p-1)*12 + 6) for p in periods]  # Center of each period\n",
    "    \n",
    "    # Create proper FuseTS-compatible dataset\n",
    "    ds = xr.Dataset({\n",
    "        'VV': (['time', 'y', 'x'], vv_array.values),\n",
    "        'VH': (['time', 'y', 'x'], vh_array.values),\n",
    "        'S2ndvi': (['time', 'y', 'x'], ndvi_array.values)\n",
    "    }, coords={\n",
    "        'time': time_coords,\n",
    "        'y': vv_array.y.values,\n",
    "        'x': vv_array.x.values\n",
    "    })\n",
    "    \n",
    "    # Rename time dimension to 't' for FuseTS compatibility\n",
    "    ds = ds.rename({'time': 't'})\n",
    "    \n",
    "    # Add metadata\n",
    "    ds.attrs.update({\n",
    "        'title': 'Sentinel-1/2 Time Series from GEE Assets',\n",
    "        'source': f'{asset_base_path}/{name_prefix}',\n",
    "        'temporal_resolution': '12-day composites',\n",
    "        'spatial_resolution': f'{scale}m',\n",
    "        'date_range': f'{time_coords[0].strftime(\"%Y-%m-%d\")} to {time_coords[-1].strftime(\"%Y-%m-%d\")}',\n",
    "        'region': REGION_NAME,\n",
    "        'crs': 'EPSG:4326',\n",
    "        'fusets_ready': True\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset created successfully!\")\n",
    "    print(f\"   Shape: {ds.VV.shape}\")\n",
    "    print(f\"   Time range: {time_coords[0]} to {time_coords[-1]}\")\n",
    "    print(f\"   Spatial extent: {len(ds.y)} x {len(ds.x)} pixels\")\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# Execute if using GEE Assets\n",
    "if USE_GEE_ASSETS:\n",
    "    print(\"=\"*70)\n",
    "    print(\"üåç LOADING DATA FROM GEE ASSETS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize GEE\n",
    "    initialize_gee()\n",
    "    \n",
    "    # Load assets\n",
    "    gee_dataset = load_gee_assets_to_xarray(\n",
    "        ASSET_BASE_PATH,\n",
    "        ASSET_NAME_PREFIX,\n",
    "        num_periods=NUM_PERIODS,\n",
    "        region=REGION,\n",
    "        scale=SCALE\n",
    "    )\n",
    "    \n",
    "    if gee_dataset is not None:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä DATASET SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(gee_dataset)\n",
    "        print(\"\\n‚úÖ Data ready for MOGPR processing!\")\n",
    "        print(\"   Skip the synthetic data generation below and use 'gee_dataset'\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Failed to load GEE Assets\")\n",
    "        print(\"   Falling back to synthetic data generation...\")\n",
    "        USE_GEE_ASSETS = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Using synthetic data (USE_GEE_ASSETS=False)\")\n",
    "    print(\"   To use GEE Assets, set USE_GEE_ASSETS=True above\")\n",
    "    gee_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Alternative: Load GeoTIFF files directly\n",
    "\n",
    "If you exported your data to GEE Drive as individual GeoTIFF files (period_01.tif, period_02.tif, etc.), use this function instead of the GEE Asset loader above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geotiff_periods_to_xarray(geotiff_dir, num_periods=31, file_pattern='period_{:02d}.tif'):\n",
    "    \"\"\"\n",
    "    Load individual GeoTIFF files (period_01.tif, period_02.tif, etc.) into xarray Dataset\n",
    "    \n",
    "    This function is for when you already have GeoTIFF files downloaded locally,\n",
    "    either from GEE Drive export or downloaded manually.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    geotiff_dir : str\n",
    "        Directory containing the GeoTIFF files\n",
    "    num_periods : int\n",
    "        Number of periods to load (default: 31)\n",
    "    file_pattern : str\n",
    "        Pattern for GeoTIFF filenames with {:02d} for zero-padded period number\n",
    "        (default: 'period_{:02d}.tif')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset with dimensions (t, y, x) and variables (VV, VH, S2ndvi)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import rioxarray\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    print(f\"\\nüìÅ Loading GeoTIFF files from: {geotiff_dir}\")\n",
    "    print(f\"   Pattern: {file_pattern}\")\n",
    "    print(f\"   Expected periods: {num_periods}\")\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(geotiff_dir):\n",
    "        print(f\"‚ùå Directory not found: {geotiff_dir}\")\n",
    "        print(f\"\\nüí° Did you:\")\n",
    "        print(f\"   1. Export from GEE to Drive?\")\n",
    "        print(f\"   2. Download the files to your computer?\")\n",
    "        print(f\"   3. Upload to HPC or Colab?\")\n",
    "        return None\n",
    "    \n",
    "    # Storage for time series data\n",
    "    all_vv = []\n",
    "    all_vh = []\n",
    "    all_ndvi = []\n",
    "    time_coords = []\n",
    "    y_coords = None\n",
    "    x_coords = None\n",
    "    \n",
    "    print(f\"\\nüì• Loading {num_periods} GeoTIFF files...\")\n",
    "    \n",
    "    # Load each period\n",
    "    loaded_count = 0\n",
    "    for period in range(1, num_periods + 1):\n",
    "        filename = file_pattern.format(period)\n",
    "        filepath = os.path.join(geotiff_dir, filename)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"   ‚ö†Ô∏è  Period {period:02d}: File not found: {filename}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load the GeoTIFF\n",
    "            period_data = rioxarray.open_rasterio(filepath)\n",
    "            \n",
    "            # Extract bands (assuming order: VV, VH, S2ndvi or NDVI)\n",
    "            num_bands = len(period_data.band)\n",
    "            if num_bands >= 3:\n",
    "                vv = period_data.isel(band=0).values  # First band = VV\n",
    "                vh = period_data.isel(band=1).values  # Second band = VH\n",
    "                ndvi = period_data.isel(band=2).values  # Third band = NDVI/S2ndvi\n",
    "            elif num_bands == 1:\n",
    "                # Single band - might be just NDVI\n",
    "                print(f\"   ‚ö†Ô∏è  Period {period:02d}: Only 1 band found, treating as NDVI\")\n",
    "                ndvi = period_data.isel(band=0).values\n",
    "                vv = np.full_like(ndvi, np.nan)\n",
    "                vh = np.full_like(ndvi, np.nan)\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Period {period:02d}: {num_bands} bands found, expected 3\")\n",
    "                continue\n",
    "            \n",
    "            all_vv.append(vv)\n",
    "            all_vh.append(vh)\n",
    "            all_ndvi.append(ndvi)\n",
    "            \n",
    "            # Calculate time coordinate (center of 12-day period starting Nov 1, 2024)\n",
    "            start_date = datetime(2024, 11, 1)\n",
    "            period_center = start_date + timedelta(days=(period-1)*12 + 6)\n",
    "            time_coords.append(period_center)\n",
    "            \n",
    "            # Store spatial coords from first period\n",
    "            if period == 1:\n",
    "                y_coords = period_data.y.values\n",
    "                x_coords = period_data.x.values\n",
    "            \n",
    "            loaded_count += 1\n",
    "            if loaded_count % 5 == 0:\n",
    "                print(f\"   Loaded {loaded_count}/{num_periods} periods...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Period {period:02d}: Error loading {filename}: {e}\")\n",
    "    \n",
    "    # Check if we got any data\n",
    "    if len(all_vv) == 0:\n",
    "        print(\"\\n‚ùå No data loaded!\")\n",
    "        print(f\"\\nüí° Make sure GeoTIFF files are in: {geotiff_dir}\")\n",
    "        print(f\"   Expected files: {file_pattern.format(1)}, {file_pattern.format(2)}, ...\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully loaded {loaded_count}/{num_periods} periods\")\n",
    "    \n",
    "    # Stack into numpy arrays\n",
    "    vv_stack = np.stack(all_vv, axis=0)  # Shape: (time, y, x)\n",
    "    vh_stack = np.stack(all_vh, axis=0)\n",
    "    ndvi_stack = np.stack(all_ndvi, axis=0)\n",
    "    \n",
    "    # Create FuseTS-compatible xarray Dataset\n",
    "    ds = xr.Dataset({\n",
    "        'VV': (['t', 'y', 'x'], vv_stack),\n",
    "        'VH': (['t', 'y', 'x'], vh_stack),\n",
    "        'S2ndvi': (['t', 'y', 'x'], ndvi_stack)\n",
    "    }, coords={\n",
    "        't': time_coords,\n",
    "        'y': y_coords,\n",
    "        'x': x_coords\n",
    "    })\n",
    "    \n",
    "    # Add metadata\n",
    "    ds.attrs.update({\n",
    "        'title': 'Sentinel-1/2 Time Series from GeoTIFF files',\n",
    "        'source': geotiff_dir,\n",
    "        'temporal_resolution': '12-day composites',\n",
    "        'spatial_resolution': '50m',\n",
    "        'date_range': f'{time_coords[0].strftime(\"%Y-%m-%d\")} to {time_coords[-1].strftime(\"%Y-%m-%d\")}',\n",
    "        'num_periods': loaded_count,\n",
    "        'region': 'Kabupaten Demak',\n",
    "        'crs': 'EPSG:4326',\n",
    "        'fusets_ready': True\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Summary:\")\n",
    "    print(f\"   Shape: {ds.VV.shape} (time, y, x)\")\n",
    "    print(f\"   Time range: {time_coords[0].strftime('%Y-%m-%d')} to {time_coords[-1].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Spatial extent: {len(y_coords)} x {len(x_coords)} pixels\")\n",
    "    print(f\"   Bands: VV, VH, S2ndvi\")\n",
    "    print(f\"   Total size: {ds.nbytes / 1e6:.1f} MB\")\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Example: Load from Local GeoTIFF Files\n",
    "\n",
    "If you have downloaded the GeoTIFF files to your local machine or HPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION 1: Load from local GeoTIFF directory\n",
    "# ============================================================================\n",
    "# Use this if you have period_01.tif, period_02.tif, etc. downloaded locally\n",
    "\n",
    "# Specify the directory containing your GeoTIFF files\n",
    "GEOTIFF_DIR = '/path/to/your/geotiff/files'  # Change this to your actual path\n",
    "\n",
    "# Examples:\n",
    "# On HPC: GEOTIFF_DIR = '/home/username/data/demak_s1_s2'\n",
    "# On local: GEOTIFF_DIR = '/Users/username/Downloads/demak_geotiffs'\n",
    "# On Colab: GEOTIFF_DIR = '/content/drive/MyDrive/FuseTS_Data/demak'\n",
    "\n",
    "# Load the data\n",
    "combined_dataset = load_geotiff_periods_to_xarray(\n",
    "    geotiff_dir=GEOTIFF_DIR,\n",
    "    num_periods=31,\n",
    "    file_pattern='period_{:02d}.tif'  # Adjust if your files have different naming\n",
    ")\n",
    "\n",
    "# Display the dataset\n",
    "if combined_dataset is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    display(combined_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìò Quick Start Guide: Using GEE Assets\n",
    "\n",
    "**Step 1: Export data to GEE Assets** (one-time setup)\n",
    "- Run `GEE_Data_Preparation_for_FuseTS_Assets.ipynb`\n",
    "- Set `EXPORT_DESTINATION = 'asset'`\n",
    "- Monitor exports at https://code.earthengine.google.com/tasks\n",
    "- Wait for all exports to complete (may take hours for Java Island)\n",
    "\n",
    "**Step 2: Load data in this notebook**\n",
    "- In Section 2 above, set `USE_GEE_ASSETS = True`\n",
    "- Configure `ASSET_BASE_PATH` and `ASSET_PATTERN` to match your exports\n",
    "- For testing: Set `USE_SMALL_REGION = True` to download only a small area\n",
    "- For full analysis: Set `USE_SMALL_REGION = False` to use entire Java Island\n",
    "\n",
    "**Step 3: Run MOGPR analysis**\n",
    "- Execute cells below normally\n",
    "- All code automatically uses either GEE or synthetic data\n",
    "- Results will reflect the actual Indonesian agricultural calendar (Nov 2024 - Oct 2025)\n",
    "\n",
    "**üí° Tips:**\n",
    "- Start with small region for testing (faster download, ~1-2 minutes)\n",
    "- Once validated, process full Java Island (may take longer, but stays in memory)\n",
    "- Downloaded data is cached in `gee_assets_download/` folder for reuse\n",
    "- You can change `REGION` to focus on specific areas of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mogpr_dataset(s1_vv, s1_vh, s2_ndvi, time_coords, y_coords, x_coords):\n",
    "    \"\"\"\n",
    "    Prepare properly formatted xarray Dataset for MOGPR processing\n",
    "    (Only used for synthetic data)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create individual DataArrays with proper naming and coordinates\n",
    "    vv_da = xr.DataArray(\n",
    "        s1_vv,\n",
    "        dims=['t', 'y', 'x'],  # Note: 't' dimension name is required by FuseTS\n",
    "        coords={\n",
    "            't': time_coords,\n",
    "            'y': y_coords,\n",
    "            'x': x_coords\n",
    "        },\n",
    "        name='VV',\n",
    "        attrs={'long_name': 'Sentinel-1 VV backscatter', 'units': 'dB'}\n",
    "    )\n",
    "    \n",
    "    vh_da = xr.DataArray(\n",
    "        s1_vh,\n",
    "        dims=['t', 'y', 'x'],\n",
    "        coords={\n",
    "            't': time_coords,\n",
    "            'y': y_coords,\n",
    "            'x': x_coords\n",
    "        },\n",
    "        name='VH',\n",
    "        attrs={'long_name': 'Sentinel-1 VH backscatter', 'units': 'dB'}\n",
    "    )\n",
    "    \n",
    "    ndvi_da = xr.DataArray(\n",
    "        s2_ndvi,\n",
    "        dims=['t', 'y', 'x'],\n",
    "        coords={\n",
    "            't': time_coords,\n",
    "            'y': y_coords,\n",
    "            'x': x_coords\n",
    "        },\n",
    "        name='S2ndvi',  # Specific naming required by MOGPR\n",
    "        attrs={'long_name': 'Sentinel-2 NDVI', 'units': 'dimensionless'}\n",
    "    )\n",
    "    \n",
    "    # Combine into Dataset\n",
    "    dataset = xr.Dataset({\n",
    "        'VV': vv_da,\n",
    "        'VH': vh_da,\n",
    "        'S2ndvi': ndvi_da\n",
    "    })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Prepare the dataset\n",
    "print(\"Preparing dataset for MOGPR...\")\n",
    "\n",
    "if USE_GEE_ASSETS and gee_dataset is not None:\n",
    "    # Use GEE Assets data (already in proper format)\n",
    "    combined_dataset = gee_dataset\n",
    "    print(\"‚úÖ Using GEE Assets dataset\")\n",
    "else:\n",
    "    # Use synthetic data\n",
    "    combined_dataset = prepare_mogpr_dataset(\n",
    "        vv_data, vh_data, ndvi_data,\n",
    "        time_idx, y_coords, x_coords\n",
    "    )\n",
    "    print(\"‚úÖ Using synthetic dataset\")\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "print(combined_dataset)\n",
    "\n",
    "# Check for missing data\n",
    "print(\"\\nMissing data summary:\")\n",
    "for var in combined_dataset.data_vars:\n",
    "    missing_pct = (combined_dataset[var].isnull().sum() / combined_dataset[var].size * 100).values\n",
    "    print(f\"{var}: {missing_pct:.1f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply MOGPR Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 GPU Detection and Configuration\n",
    "\n",
    "Configure whether to use GPU-accelerated MOGPR (if available) or CPU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GPU Detection and Configuration\n",
    "# ============================================================================\n",
    "\n",
    "USE_GPU = True  # Set to True if you have GPU and installed torch + gpytorch\n",
    "\n",
    "if USE_GPU:\n",
    "    try:\n",
    "        import torch\n",
    "        from fusets.mogpr_gpu import MOGPRTransformerGPU\n",
    "        \n",
    "        # Check GPU availability\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            print(f\"‚úÖ GPU detected: {gpu_name}\")\n",
    "            print(f\"   VRAM: {gpu_memory:.1f} GB\")\n",
    "            print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "            print(\"‚úÖ Apple Silicon GPU (MPS) detected\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"‚ö†Ô∏è  No GPU detected, falling back to CPU\")\n",
    "            USE_GPU = False\n",
    "            \n",
    "        if USE_GPU:\n",
    "            print(f\"\\nüöÄ GPU-accelerated MOGPR enabled!\")\n",
    "            print(f\"   Expected speedup: 10-100x faster than CPU\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è  GPU libraries not available: {e}\")\n",
    "        print(\"   Falling back to CPU version\")\n",
    "        USE_GPU = False\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Using CPU version of MOGPR\")\n",
    "    print(\"   To enable GPU: Set USE_GPU = True and install torch + gpytorch\")\n",
    "\n",
    "print(f\"\\nFinal configuration: {'GPU' if USE_GPU else 'CPU'} mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series for a sample pixel\n",
    "# Get dataset dimensions\n",
    "n_time, n_y, n_x = combined_dataset['S2ndvi'].shape\n",
    "sample_y, sample_x = n_y // 2, n_x // 2  # Center pixel\n",
    "\n",
    "# Get time coordinate\n",
    "time_coord = combined_dataset.t.values\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(f'Input Time Series at Pixel ({sample_y}, {sample_x})', fontsize=16)\n",
    "\n",
    "# S2 NDVI\n",
    "axes[0, 0].plot(time_coord, combined_dataset['S2ndvi'][:, sample_y, sample_x], 'go-', alpha=0.7, label='S2 NDVI')\n",
    "axes[0, 0].set_title('Sentinel-2 NDVI (with gaps)')\n",
    "axes[0, 0].set_ylabel('NDVI')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# S1 VV\n",
    "axes[0, 1].plot(time_coord, combined_dataset['VV'][:, sample_y, sample_x], 'bo-', alpha=0.7, label='S1 VV')\n",
    "axes[0, 1].set_title('Sentinel-1 VV Backscatter')\n",
    "axes[0, 1].set_ylabel('VV (dB)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# S1 VH\n",
    "axes[1, 0].plot(time_coord, combined_dataset['VH'][:, sample_y, sample_x], 'ro-', alpha=0.7, label='S1 VH')\n",
    "axes[1, 0].set_title('Sentinel-1 VH Backscatter')\n",
    "axes[1, 0].set_ylabel('VH (dB)')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# All variables together\n",
    "# Normalize for comparison\n",
    "vv_norm = (combined_dataset['VV'][:, sample_y, sample_x] - combined_dataset['VV'][:, sample_y, sample_x].min()) / \\\n",
    "          (combined_dataset['VV'][:, sample_y, sample_x].max() - combined_dataset['VV'][:, sample_y, sample_x].min())\n",
    "vh_norm = (combined_dataset['VH'][:, sample_y, sample_x] - combined_dataset['VH'][:, sample_y, sample_x].min()) / \\\n",
    "          (combined_dataset['VH'][:, sample_y, sample_x].max() - combined_dataset['VH'][:, sample_y, sample_x].min())\n",
    "ndvi_norm = combined_dataset['S2ndvi'][:, sample_y, sample_x]\n",
    "\n",
    "axes[1, 1].plot(time_coord, vv_norm, 'b-', alpha=0.7, label='VV (normalized)')\n",
    "axes[1, 1].plot(time_coord, vh_norm, 'r-', alpha=0.7, label='VH (normalized)')\n",
    "axes[1, 1].plot(time_coord, ndvi_norm, 'go-', alpha=0.7, label='NDVI')\n",
    "axes[1, 1].set_title('All Variables (Normalized)')\n",
    "axes[1, 1].set_ylabel('Normalized Value')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show spatial patterns at a specific date\n",
    "mid_date_idx = len(time_coord) // 2\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle(f'Spatial Patterns on {time_coord[mid_date_idx].astype(\"datetime64[D]\")}', fontsize=14)\n",
    "\n",
    "im1 = axes[0].imshow(combined_dataset['S2ndvi'][mid_date_idx], cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[0].set_title('S2 NDVI')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(combined_dataset['VV'][mid_date_idx], cmap='viridis')\n",
    "axes[1].set_title('S1 VV (dB)')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "im3 = axes[2].imshow(combined_dataset['VH'][mid_date_idx], cmap='plasma')\n",
    "axes[2].set_title('S1 VH (dB)')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Fusion Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: MOGPR is VERY SLOW on large areas!\n",
    "# Create a small test subset to speed up processing\n",
    "# This will reduce processing time from hours to minutes\n",
    "\n",
    "print(\"Creating small test subset for faster MOGPR processing...\")\n",
    "\n",
    "# Extract a 50x50 pixel subset from the center\n",
    "center_y = combined_dataset.dims['y'] // 2\n",
    "center_x = combined_dataset.dims['x'] // 2\n",
    "subset_size = 50\n",
    "\n",
    "y_start = max(0, center_y - subset_size // 2)\n",
    "y_end = min(combined_dataset.dims['y'], center_y + subset_size // 2)\n",
    "x_start = max(0, center_x - subset_size // 2)\n",
    "x_end = min(combined_dataset.dims['x'], center_x + subset_size // 2)\n",
    "\n",
    "# Create subset\n",
    "test_subset = combined_dataset.isel(y=slice(y_start, y_end), x=slice(x_start, x_end))\n",
    "\n",
    "print(f\"Original dataset size: {combined_dataset.dims}\")\n",
    "print(f\"Test subset size: {test_subset.dims}\")\n",
    "print(f\"Processing will be {(combined_dataset.dims['y'] * combined_dataset.dims['x']) / (test_subset.dims['y'] * test_subset.dims['x']):.0f}x faster!\")\n",
    "print(\"\\n‚úÖ Ready for MOGPR fusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Data Quality Check (Run this BEFORE MOGPR!)\n",
    "\n",
    "**CRITICAL**: Check if your data is properly loaded and has valid values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è URGENT: Check if combined_dataset exists and has data\n",
    "\n",
    "**Run this BEFORE creating test_subset!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CRITICAL: Verify combined_dataset exists and has data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç Checking if combined_dataset is loaded...\\n\")\n",
    "\n",
    "# Check if variable exists\n",
    "try:\n",
    "    print(f\"‚úÖ combined_dataset exists\")\n",
    "    print(f\"   Type: {type(combined_dataset)}\")\n",
    "    print(f\"\\nüìä Dataset structure:\")\n",
    "    print(combined_dataset)\n",
    "    \n",
    "    # Check data variables\n",
    "    print(f\"\\nüìà Data variables:\")\n",
    "    for var in ['VV', 'VH', 'S2ndvi']:\n",
    "        if var in combined_dataset:\n",
    "            values = combined_dataset[var].values\n",
    "            total = values.size\n",
    "            nans = np.isnan(values).sum()\n",
    "            valid = total - nans\n",
    "            pct_valid = (valid / total) * 100\n",
    "            \n",
    "            print(f\"\\n{var}:\")\n",
    "            print(f\"  Shape: {combined_dataset[var].shape}\")\n",
    "            print(f\"  Valid: {valid:,} / {total:,} ({pct_valid:.1f}%)\")\n",
    "            \n",
    "            if pct_valid == 0:\n",
    "                print(f\"  ‚ùå ALL NaN - NO DATA!\")\n",
    "            elif pct_valid < 10:\n",
    "                print(f\"  ‚ö†Ô∏è  Very sparse data\")\n",
    "            else:\n",
    "                valid_vals = values[~np.isnan(values)]\n",
    "                print(f\"  Range: [{valid_vals.min():.4f}, {valid_vals.max():.4f}]\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå {var}: NOT FOUND in dataset!\")\n",
    "    \n",
    "    # Final verdict\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    all_vars_empty = all(\n",
    "        (combined_dataset[var].values if var in combined_dataset else np.array([np.nan])).size == np.isnan(\n",
    "            combined_dataset[var].values if var in combined_dataset else np.array([np.nan])\n",
    "        ).sum()\n",
    "        for var in ['VV', 'VH', 'S2ndvi']\n",
    "    )\n",
    "    \n",
    "    if all_vars_empty:\n",
    "        print(\"‚ùå CRITICAL ERROR: Dataset is EMPTY (all NaN)!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nüîß YOU NEED TO:\")\n",
    "        print(\"1. Make sure GEOTIFF_DIR is set correctly\")\n",
    "        print(\"2. Re-run the data loading cell:\")\n",
    "        print(\"   combined_dataset = load_geotiff_periods_to_xarray(...)\")\n",
    "        print(\"3. Check that GeoTIFF files exist and have data\")\n",
    "    else:\n",
    "        print(\"‚úÖ Dataset has valid data - Ready to proceed!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "except NameError:\n",
    "    print(\"‚ùå combined_dataset is NOT DEFINED!\")\n",
    "    print(\"\\nüîß YOU NEED TO:\")\n",
    "    print(\"1. Set GEOTIFF_DIR to your GeoTIFF directory\")\n",
    "    print(\"2. Run the cell that calls:\")\n",
    "    print(\"   combined_dataset = load_geotiff_periods_to_xarray(...)\")\n",
    "    print(\"\\nExample:\")\n",
    "    print(\"   GEOTIFF_DIR = '/home/username/data/demak'\")\n",
    "    print(\"   combined_dataset = load_geotiff_periods_to_xarray(\")\n",
    "    print(\"       geotiff_dir=GEOTIFF_DIR,\")\n",
    "    print(\"       num_periods=31,\")\n",
    "    print(\"       file_pattern='period_{:02d}.tif'\")\n",
    "    print(\"   )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Fix: Replace -inf and invalid values with NaN\n",
    "\n",
    "Your data contains `-inf` (negative infinity) values which break MOGPR. This cell will clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Clean the dataset: Replace -inf, +inf, and zeros with NaN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîß Cleaning dataset: Replacing invalid values with NaN...\\n\")\n",
    "\n",
    "# Make a copy to avoid modifying original\n",
    "combined_dataset_clean = combined_dataset.copy()\n",
    "\n",
    "for var in ['VV', 'VH', 'S2ndvi']:\n",
    "    print(f\"Processing {var}...\")\n",
    "    \n",
    "    # Get values\n",
    "    values = combined_dataset_clean[var].values\n",
    "    \n",
    "    # Count invalid values\n",
    "    n_inf = np.isinf(values).sum()\n",
    "    n_zero = (values == 0).sum()\n",
    "    total = values.size\n",
    "    \n",
    "    print(f\"  -inf/+inf values: {n_inf:,} ({n_inf/total*100:.1f}%)\")\n",
    "    print(f\"  Zero values: {n_zero:,} ({n_zero/total*100:.1f}%)\")\n",
    "    \n",
    "    # Replace -inf, +inf with NaN\n",
    "    values_clean = np.where(np.isinf(values), np.nan, values)\n",
    "    \n",
    "    # For S2ndvi: Keep zeros (valid NDVI)\n",
    "    # For VV/VH: Zeros might be valid backscatter, but check if they're fill values\n",
    "    if var in ['VV', 'VH']:\n",
    "        # If ALL non-inf values are zero, they're likely fill values\n",
    "        non_inf = values_clean[~np.isnan(values_clean)]\n",
    "        if non_inf.size > 0 and np.all(non_inf == 0):\n",
    "            print(f\"  ‚ö†Ô∏è  All non-inf values are zero - treating as missing data\")\n",
    "            values_clean = np.where(values_clean == 0, np.nan, values_clean)\n",
    "    \n",
    "    # Update dataset\n",
    "    combined_dataset_clean[var].values = values_clean\n",
    "    \n",
    "    # Report after cleaning\n",
    "    valid_after = (~np.isnan(values_clean)).sum()\n",
    "    print(f\"  ‚úÖ Valid values after cleaning: {valid_after:,} ({valid_after/total*100:.1f}%)\\n\")\n",
    "\n",
    "# Replace the original dataset\n",
    "combined_dataset = combined_dataset_clean\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Dataset cleaned successfully!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show final statistics\n",
    "print(\"\\nüìä Final Data Quality:\")\n",
    "for var in ['VV', 'VH', 'S2ndvi']:\n",
    "    values = combined_dataset[var].values\n",
    "    valid = (~np.isnan(values)).sum()\n",
    "    total = values.size\n",
    "    pct_valid = valid / total * 100\n",
    "    \n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"  Valid: {valid:,} / {total:,} ({pct_valid:.1f}%)\")\n",
    "    \n",
    "    if pct_valid > 0:\n",
    "        valid_vals = values[~np.isnan(values)]\n",
    "        print(f\"  Range: [{valid_vals.min():.4f}, {valid_vals.max():.4f}]\")\n",
    "        print(f\"  Mean: {valid_vals.mean():.4f}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Still no valid data!\")\n",
    "\n",
    "# Check if we can proceed\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "any_valid = any(\n",
    "    (~np.isnan(combined_dataset[var].values)).sum() > 0\n",
    "    for var in ['VV', 'VH', 'S2ndvi']\n",
    ")\n",
    "\n",
    "if any_valid:\n",
    "    print(\"‚úÖ Dataset has valid data after cleaning!\")\n",
    "    print(\"   You can now proceed with MOGPR fusion.\")\n",
    "else:\n",
    "    print(\"‚ùå CRITICAL: Still no valid data after cleaning!\")\n",
    "    print(\"\\nüí° This means your GEE download failed or is corrupted.\")\n",
    "    print(\"   You need to:\")\n",
    "    print(\"   1. Check the GEE Assets in GEE Code Editor\")\n",
    "    print(\"   2. Re-download/re-export from GEE\")\n",
    "    print(\"   3. Or use the GeoTIFF loading method if you have local files\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CRITICAL: Check data before running MOGPR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç Checking data quality in test_subset...\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(test_subset)\n",
    "\n",
    "print(f\"\\nüìä Data Statistics:\")\n",
    "for var in ['VV', 'VH', 'S2ndvi']:\n",
    "    values = test_subset[var].values\n",
    "    valid_values = values[~np.isnan(values)]\n",
    "    \n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"  Shape: {values.shape}\")\n",
    "    print(f\"  Total elements: {values.size:,}\")\n",
    "    print(f\"  Valid values: {valid_values.size:,} ({valid_values.size/values.size*100:.1f}%)\")\n",
    "    print(f\"  NaN values: {np.isnan(values).sum():,} ({np.isnan(values).sum()/values.size*100:.1f}%)\")\n",
    "    \n",
    "    if valid_values.size > 0:\n",
    "        print(f\"  Range: [{valid_values.min():.4f}, {valid_values.max():.4f}]\")\n",
    "        print(f\"  Mean: {valid_values.mean():.4f}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå ERROR: No valid values! All NaN!\")\n",
    "\n",
    "# Check if data is usable for MOGPR\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MOGPR Requirements Check:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_nan = {}\n",
    "for var in ['VV', 'VH', 'S2ndvi']:\n",
    "    values = test_subset[var].values\n",
    "    pct_valid = (~np.isnan(values)).sum() / values.size * 100\n",
    "    all_nan[var] = pct_valid == 0\n",
    "    \n",
    "    if pct_valid == 0:\n",
    "        print(f\"‚ùå {var}: 100% NaN - CANNOT RUN MOGPR!\")\n",
    "    elif pct_valid < 10:\n",
    "        print(f\"‚ö†Ô∏è  {var}: Only {pct_valid:.1f}% valid - Poor quality\")\n",
    "    elif pct_valid < 50:\n",
    "        print(f\"‚ö†Ô∏è  {var}: {pct_valid:.1f}% valid - Marginal quality\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {var}: {pct_valid:.1f}% valid - Good quality\")\n",
    "\n",
    "if any(all_nan.values()):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ùå CRITICAL ERROR: Some variables are completely empty!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüí° Possible causes:\")\n",
    "    print(\"   1. GeoTIFF files not properly loaded\")\n",
    "    print(\"   2. Wrong directory path in GEOTIFF_DIR\")\n",
    "    print(\"   3. Files exist but have no data\")\n",
    "    print(\"   4. Band indexing is wrong\")\n",
    "    print(\"\\nüîß Solutions:\")\n",
    "    print(\"   1. Check: print(list(GEOTIFF_DIR.glob('*.tif')))\")\n",
    "    print(\"   2. Verify GeoTIFF files have 3 bands (VV, VH, NDVI)\")\n",
    "    print(\"   3. Re-run the data loading cell\")\n",
    "    print(\"   4. Try loading one file manually to inspect\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Data quality check passed! Ready for MOGPR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Manual GeoTIFF Inspection\n",
    "\n",
    "If the data quality check fails, use this to manually inspect your GeoTIFF files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Manual inspection of GeoTIFF files (for debugging)\n",
    "# ============================================================================\n",
    "# Run this ONLY if data quality check fails\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if GEOTIFF_DIR is defined and exists\n",
    "try:\n",
    "    geotiff_path = Path(GEOTIFF_DIR)\n",
    "    print(f\"üìÅ Checking directory: {geotiff_path}\")\n",
    "    \n",
    "    if not geotiff_path.exists():\n",
    "        print(f\"‚ùå Directory does not exist!\")\n",
    "        print(f\"\\nüí° Update GEOTIFF_DIR to point to your GeoTIFF files\")\n",
    "    else:\n",
    "        # List all TIFF files\n",
    "        tiff_files = sorted(geotiff_path.glob('*.tif'))\n",
    "        print(f\"\\n‚úÖ Found {len(tiff_files)} TIFF files:\")\n",
    "        for f in tiff_files[:5]:  # Show first 5\n",
    "            print(f\"   {f.name}\")\n",
    "        if len(tiff_files) > 5:\n",
    "            print(f\"   ... and {len(tiff_files)-5} more\")\n",
    "        \n",
    "        # Load and inspect first file\n",
    "        if len(tiff_files) > 0:\n",
    "            print(f\"\\nüîç Inspecting first file: {tiff_files[0].name}\")\n",
    "            test_data = rioxarray.open_rasterio(tiff_files[0])\n",
    "            print(f\"   Shape: {test_data.shape}\")\n",
    "            print(f\"   Bands: {len(test_data.band)}\")\n",
    "            print(f\"   Data type: {test_data.dtype}\")\n",
    "            print(f\"   CRS: {test_data.rio.crs}\")\n",
    "            \n",
    "            for i in range(len(test_data.band)):\n",
    "                band_data = test_data.isel(band=i).values\n",
    "                valid_data = band_data[~np.isnan(band_data)]\n",
    "                print(f\"\\n   Band {i+1}:\")\n",
    "                print(f\"      Valid pixels: {valid_data.size:,} / {band_data.size:,}\")\n",
    "                if valid_data.size > 0:\n",
    "                    print(f\"      Range: [{valid_data.min():.4f}, {valid_data.max():.4f}]\")\n",
    "                else:\n",
    "                    print(f\"      ‚ùå All NaN!\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå No TIFF files found!\")\n",
    "            print(\"\\nüí° Make sure you:\")\n",
    "            print(\"   1. Downloaded GeoTIFF files from Google Drive\")\n",
    "            print(\"   2. Uploaded them to HPC in the correct directory\")\n",
    "            print(\"   3. Updated GEOTIFF_DIR to the correct path\")\n",
    "            \n",
    "except NameError:\n",
    "    print(\"‚ùå GEOTIFF_DIR is not defined!\")\n",
    "    print(\"\\nüí° You need to:\")\n",
    "    print(\"   1. Set GEOTIFF_DIR = '/path/to/your/geotiff/files'\")\n",
    "    print(\"   2. Run the load_geotiff_periods_to_xarray() function\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MOGPR fusion to the test subset\n",
    "# Supports both CPU and GPU versions\n",
    "\n",
    "if USE_GPU:\n",
    "    print(\"üöÄ Initializing GPU-accelerated MOGPR transformer...\")\n",
    "    mogpr = MOGPRTransformerGPU(device=device, batch_size=64)\n",
    "    print(f\"Applying GPU MOGPR fusion to {test_subset.dims['y']}√ó{test_subset.dims['x']} pixel subset...\")\n",
    "    print(\"Expected time: ~1-2 minutes (10-100x faster than CPU!)\")\n",
    "else:\n",
    "    print(\"Initializing MOGPR transformer (CPU version)...\")\n",
    "    mogpr = MOGPRTransformer()\n",
    "    print(f\"Applying MOGPR fusion to {test_subset.dims['y']}√ó{test_subset.dims['x']} pixel subset...\")\n",
    "    print(\"MOGPR builds Gaussian Process models to learn correlations between S1 and S2 variables...\")\n",
    "    print(\"This should take 5-15 minutes on the small subset...\")\n",
    "\n",
    "try:\n",
    "    # Apply MOGPR fusion\n",
    "    fused_result = mogpr.fit_transform(test_subset)\n",
    "    print(\"MOGPR fusion completed successfully!\")\n",
    "    \n",
    "    print(\"\\nFused result structure:\")\n",
    "    print(fused_result)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during MOGPR processing: {e}\")\n",
    "    print(\"This might be due to the data structure or missing dependencies.\")\n",
    "    \n",
    "    # For demonstration, we'll use the test subset as a fallback\n",
    "    print(\"Using test subset as fallback for demonstration...\")\n",
    "    fused_result = test_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Apply Whittaker smoothing first (recommended for noisy data)\n",
    "print(\"Applying Whittaker smoothing preprocessing...\")\n",
    "smoothed_dataset = combined_dataset.copy()\n",
    "\n",
    "for var in combined_dataset.data_vars:\n",
    "    print(f\"Smoothing {var}...\")\n",
    "    # Apply Whittaker smoothing to each variable\n",
    "    smoothed_dataset[var] = whittaker(combined_dataset[var], lmbd=10000)\n",
    "\n",
    "print(\"Smoothing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Phenological Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs fused data\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "fig.suptitle(f'Original vs Fused Data at Pixel ({sample_y}, {sample_x})', fontsize=16)\n",
    "\n",
    "variables = ['S2ndvi', 'VV', 'VH']\n",
    "colors = ['green', 'blue', 'red']\n",
    "units = ['NDVI', 'dB', 'dB']\n",
    "\n",
    "for i, (var, color, unit) in enumerate(zip(variables, colors, units)):\n",
    "    # Original data\n",
    "    axes[i, 0].plot(time_idx, combined_dataset[var][:, sample_y, sample_x], \n",
    "                   'o-', color=color, alpha=0.7, label=f'Original {var}')\n",
    "    axes[i, 0].set_title(f'Original {var}')\n",
    "    axes[i, 0].set_ylabel(f'{var} ({unit})')\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    axes[i, 0].legend()\n",
    "    \n",
    "    # Fused data\n",
    "    axes[i, 1].plot(time_idx, fused_result[var][:, sample_y, sample_x], \n",
    "                   'o-', color=color, alpha=0.7, label=f'Fused {var}')\n",
    "    # Overlay original for comparison\n",
    "    axes[i, 1].plot(time_idx, combined_dataset[var][:, sample_y, sample_x], \n",
    "                   's', color='gray', alpha=0.3, label='Original', markersize=3)\n",
    "    axes[i, 1].set_title(f'Fused {var}')\n",
    "    axes[i, 1].set_ylabel(f'{var} ({unit})')\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "    axes[i, 1].legend()\n",
    "\n",
    "axes[2, 0].set_xlabel('Date')\n",
    "axes[2, 1].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display gap-filling performance\n",
    "original_gaps = combined_dataset['S2ndvi'].isnull().sum().values\n",
    "fused_gaps = fused_result['S2ndvi'].isnull().sum().values\n",
    "\n",
    "print(f\"\\nGap-filling performance:\")\n",
    "print(f\"Original NDVI gaps: {original_gaps} pixels\")\n",
    "print(f\"Remaining gaps after fusion: {fused_gaps} pixels\")\n",
    "print(f\"Gaps filled: {original_gaps - fused_gaps} pixels ({(original_gaps - fused_gaps)/original_gaps*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Pixel Start of Season Analysis\n",
    "\n",
    "**Important**: This workflow provides **Start of Season (SOS) information for every pixel** in your study area!\n",
    "\n",
    "### What you get for each pixel:\n",
    "- **SOS Timing**: Day of year when Start of Season occurs (1-365)\n",
    "- **SOS Values**: NDVI value at the Start of Season\n",
    "- **Spatial Coverage**: Complete coverage for your entire study area\n",
    "- **Resolution**: Same spatial resolution as your input data (e.g., 10m pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply phenology analysis to fused NDVI\n",
    "print(\"Extracting phenological metrics from fused NDVI...\")\n",
    "\n",
    "try:\n",
    "    # Extract phenological metrics using FuseTS phenology function\n",
    "    phenology_metrics = phenology(fused_result['S2ndvi'])\n",
    "    \n",
    "    print(\"Phenological analysis completed!\")\n",
    "    print(\"\\nAvailable phenological metrics:\")\n",
    "    for var in phenology_metrics.data_vars:\n",
    "        print(f\"- {var}\")\n",
    "    \n",
    "    # Extract key metrics\n",
    "    sos_times = phenology_metrics.da_sos_times      # Start of Season (day of year)\n",
    "    eos_times = phenology_metrics.da_eos_times      # End of Season (day of year)\n",
    "    sos_values = phenology_metrics.da_sos_values    # Vegetation values at SOS\n",
    "    eos_values = phenology_metrics.da_eos_values    # Vegetation values at EOS\n",
    "    \n",
    "    print(f\"\\nSample phenological metrics at pixel ({sample_y}, {sample_x}):\")\n",
    "    print(f\"Start of Season (day of year): {sos_times[sample_y, sample_x].values}\")\n",
    "    print(f\"End of Season (day of year): {eos_times[sample_y, sample_x].values}\")\n",
    "    print(f\"NDVI at Start of Season: {sos_values[sample_y, sample_x].values:.3f}\")\n",
    "    print(f\"NDVI at End of Season: {eos_values[sample_y, sample_x].values:.3f}\")\n",
    "    \n",
    "    # Calculate growing season length\n",
    "    season_length = eos_times - sos_times\n",
    "    print(f\"Growing season length: {season_length[sample_y, sample_x].values} days\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during phenological analysis: {e}\")\n",
    "    print(\"This might be due to the synthetic data characteristics.\")\n",
    "    \n",
    "    # Create dummy metrics for visualization\n",
    "    print(\"Creating dummy phenological metrics for demonstration...\")\n",
    "    sos_times = xr.DataArray(\n",
    "        np.random.randint(60, 120, (len(y_coords), len(x_coords))),\n",
    "        dims=['y', 'x'], coords={'y': y_coords, 'x': x_coords}\n",
    "    )\n",
    "    eos_times = xr.DataArray(\n",
    "        np.random.randint(250, 310, (len(y_coords), len(x_coords))),\n",
    "        dims=['y', 'x'], coords={'y': y_coords, 'x': x_coords}\n",
    "    )\n",
    "    sos_values = xr.DataArray(\n",
    "        np.random.uniform(0.2, 0.4, (len(y_coords), len(x_coords))),\n",
    "        dims=['y', 'x'], coords={'y': y_coords, 'x': x_coords}\n",
    "    )\n",
    "    eos_values = xr.DataArray(\n",
    "        np.random.uniform(0.3, 0.5, (len(y_coords), len(x_coords))),\n",
    "        dims=['y', 'x'], coords={'y': y_coords, 'x': x_coords}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Per-Pixel Analysis and Export Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate per-pixel SOS information access\n",
    "print(\"üå± START OF SEASON INFORMATION FOR EVERY PIXEL üå±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDataset spatial dimensions:\")\n",
    "print(f\"- Y (rows): {len(y_coords)} pixels\")\n",
    "print(f\"- X (cols): {len(x_coords)} pixels\") \n",
    "print(f\"- Total pixels: {len(y_coords) * len(x_coords):,} pixels\")\n",
    "print(f\"- SOS information available for ALL pixels!\")\n",
    "\n",
    "print(f\"\\nSOS timing data structure:\")\n",
    "print(f\"- Shape: {sos_times.shape}\")\n",
    "print(f\"- Data type: {sos_times.dtype}\")\n",
    "print(f\"- Value range: Day {sos_times.min().values:.0f} to Day {sos_times.max().values:.0f}\")\n",
    "\n",
    "print(f\"\\nExample: SOS information for different pixels:\")\n",
    "sample_pixels = [(10, 15), (25, 25), (40, 35), (15, 40)]\n",
    "\n",
    "for i, (y, x) in enumerate(sample_pixels):\n",
    "    sos_day = sos_times[y, x].values\n",
    "    sos_val = sos_values[y, x].values\n",
    "    eos_day = eos_times[y, x].values\n",
    "    season_len = eos_day - sos_day\n",
    "    \n",
    "    print(f\"Pixel ({y:2d}, {x:2d}): SOS on Day {sos_day:3.0f}, NDVI={sos_val:.3f}, Season={season_len:3.0f} days\")\n",
    "\n",
    "print(f\"\\nRegional SOS statistics:\")\n",
    "print(f\"- Mean SOS: Day {sos_times.mean().values:.1f}\")\n",
    "print(f\"- Std deviation: {sos_times.std().values:.1f} days\")\n",
    "print(f\"- Earliest SOS: Day {sos_times.min().values:.0f}\")\n",
    "print(f\"- Latest SOS: Day {sos_times.max().values:.0f}\")\n",
    "print(f\"- SOS range: {(sos_times.max() - sos_times.min()).values:.0f} days\")\n",
    "\n",
    "# Plot phenological maps\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Per-Pixel Phenological Information from MOGPR Fusion', fontsize=16)\n",
    "\n",
    "# Start of Season timing\n",
    "im1 = axes[0, 0].imshow(sos_times, cmap='viridis', vmin=60, vmax=150)\n",
    "axes[0, 0].set_title('Start of Season (Day of Year)\\nüìÖ Every Pixel Has SOS Information')\n",
    "axes[0, 0].scatter(sample_x, sample_y, c='red', s=100, marker='x', linewidth=3, label='Sample pixel')\n",
    "plt.colorbar(im1, ax=axes[0, 0], label='Day of Year')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# End of Season timing\n",
    "im2 = axes[0, 1].imshow(eos_times, cmap='plasma', vmin=250, vmax=320)\n",
    "axes[0, 1].set_title('End of Season (Day of Year)\\nüçÇ Complete Spatial Coverage')\n",
    "axes[0, 1].scatter(sample_x, sample_y, c='red', s=100, marker='x', linewidth=3)\n",
    "plt.colorbar(im2, ax=axes[0, 1], label='Day of Year')\n",
    "\n",
    "# Start of Season NDVI values\n",
    "im3 = axes[1, 0].imshow(sos_values, cmap='RdYlGn', vmin=0.2, vmax=0.5)\n",
    "axes[1, 0].set_title('NDVI at Start of Season\\nüå± Vegetation Greenness at SOS')\n",
    "axes[1, 0].scatter(sample_x, sample_y, c='red', s=100, marker='x', linewidth=3)\n",
    "plt.colorbar(im3, ax=axes[1, 0], label='NDVI')\n",
    "\n",
    "# Growing season length\n",
    "season_length = eos_times - sos_times\n",
    "im4 = axes[1, 1].imshow(season_length, cmap='YlOrRd', vmin=150, vmax=250)\n",
    "axes[1, 1].set_title('Growing Season Length\\nüìè Season Duration per Pixel')\n",
    "axes[1, 1].scatter(sample_x, sample_y, c='red', s=100, marker='x', linewidth=3)\n",
    "plt.colorbar(im4, ax=axes[1, 1], label='Days')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('X coordinate (pixel)')\n",
    "    ax.set_ylabel('Y coordinate (pixel)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Result: You now have complete Start of Season information for all {len(y_coords) * len(x_coords):,} pixels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Multi-Sensor Fusion Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced per-pixel analysis and data access examples\n",
    "print(\"üîç ADVANCED PER-PIXEL ANALYSIS EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Individual pixel analysis\n",
    "def analyze_pixel(y, x, title=\"Pixel Analysis\"):\n",
    "    \"\"\"Analyze a specific pixel's phenological information\"\"\"\n",
    "    print(f\"\\nüìç {title} - Pixel ({y}, {x}):\")\n",
    "    print(f\"   ‚Ä¢ Start of Season: Day {sos_times[y, x].values:.0f}\")\n",
    "    print(f\"   ‚Ä¢ End of Season: Day {eos_times[y, x].values:.0f}\")\n",
    "    print(f\"   ‚Ä¢ NDVI at SOS: {sos_values[y, x].values:.3f}\")\n",
    "    print(f\"   ‚Ä¢ NDVI at EOS: {eos_values[y, x].values:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Growing season length: {(eos_times[y, x] - sos_times[y, x]).values:.0f} days\")\n",
    "    \n",
    "    return {\n",
    "        'sos_day': sos_times[y, x].values,\n",
    "        'eos_day': eos_times[y, x].values,\n",
    "        'sos_ndvi': sos_values[y, x].values,\n",
    "        'eos_ndvi': eos_values[y, x].values,\n",
    "        'season_length': (eos_times[y, x] - sos_times[y, x]).values\n",
    "    }\n",
    "\n",
    "# Analyze several representative pixels\n",
    "sample_pixels = [\n",
    "    (10, 10, \"Early SOS Pixel\"),\n",
    "    (25, 25, \"Center Pixel\"),\n",
    "    (40, 40, \"Late SOS Pixel\"),\n",
    "    (5, 45, \"Edge Pixel\")\n",
    "]\n",
    "\n",
    "pixel_data = []\n",
    "for y, x, label in sample_pixels:\n",
    "    data = analyze_pixel(y, x, label)\n",
    "    data['y'] = y\n",
    "    data['x'] = x\n",
    "    data['label'] = label\n",
    "    pixel_data.append(data)\n",
    "\n",
    "# 2. Spatial statistics and patterns\n",
    "print(f\"\\nüìä SPATIAL STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Total pixels analyzed: {sos_times.size:,}\")\n",
    "print(f\"   ‚Ä¢ Mean SOS: Day {sos_times.mean().values:.1f} ¬± {sos_times.std().values:.1f}\")\n",
    "print(f\"   ‚Ä¢ Mean EOS: Day {eos_times.mean().values:.1f} ¬± {eos_times.std().values:.1f}\")\n",
    "print(f\"   ‚Ä¢ Mean season length: {(eos_times - sos_times).mean().values:.1f} days\")\n",
    "\n",
    "# Calculate percentiles\n",
    "sos_percentiles = np.percentile(sos_times.values, [10, 25, 50, 75, 90])\n",
    "print(f\"   ‚Ä¢ SOS percentiles (10th, 25th, 50th, 75th, 90th): {sos_percentiles}\")\n",
    "\n",
    "# 3. Time series visualization with phenological markers\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot NDVI time series for sample pixel\n",
    "sample_y, sample_x = 25, 25\n",
    "ndvi_ts = fused_result['S2ndvi'][:, sample_y, sample_x]\n",
    "ax.plot(time_idx, ndvi_ts, 'go-', alpha=0.8, label='Fused NDVI', linewidth=2, markersize=4)\n",
    "\n",
    "# Add phenological markers\n",
    "sos_doy = sos_times[sample_y, sample_x].values\n",
    "eos_doy = eos_times[sample_y, sample_x].values\n",
    "sos_val = sos_values[sample_y, sample_x].values  \n",
    "eos_val = eos_values[sample_y, sample_x].values\n",
    "\n",
    "# Convert day of year to actual dates\n",
    "year = time_idx[0].year\n",
    "sos_date = datetime(year, 1, 1) + timedelta(days=int(sos_doy) - 1)\n",
    "eos_date = datetime(year, 1, 1) + timedelta(days=int(eos_doy) - 1)\n",
    "\n",
    "# Vertical lines for SOS and EOS\n",
    "ax.axvline(sos_date, color='blue', linestyle='--', alpha=0.8, linewidth=2, \n",
    "           label=f'Start of Season (Day {sos_doy:.0f})')\n",
    "ax.axvline(eos_date, color='red', linestyle='--', alpha=0.8, linewidth=2, \n",
    "           label=f'End of Season (Day {eos_doy:.0f})')\n",
    "\n",
    "# Markers for SOS and EOS points\n",
    "ax.scatter([sos_date], [sos_val], color='blue', s=150, zorder=5, \n",
    "           label=f'SOS NDVI: {sos_val:.3f}', marker='o', edgecolor='darkblue', linewidth=2)\n",
    "ax.scatter([eos_date], [eos_val], color='red', s=150, zorder=5, \n",
    "           label=f'EOS NDVI: {eos_val:.3f}', marker='o', edgecolor='darkred', linewidth=2)\n",
    "\n",
    "# Highlight growing season\n",
    "ax.axvspan(sos_date, eos_date, alpha=0.2, color='green', \n",
    "           label=f'Growing Season ({(eos_doy - sos_doy):.0f} days)')\n",
    "\n",
    "ax.set_title(f'Complete Phenological Profile - Pixel ({sample_y}, {sample_x})', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('NDVI', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Export options for GIS and further analysis\n",
    "print(f\"\\nüíæ EXPORT OPTIONS FOR PER-PIXEL DATA:\")\n",
    "\n",
    "# Option 1: Save as GeoTIFF (preserves spatial reference)\n",
    "try:\n",
    "    import rioxarray\n",
    "    print(\"   ‚úÖ GeoTIFF export available (requires rioxarray)\")\n",
    "    print(\"      Usage: sos_times.rio.to_raster('start_of_season.tif')\")\n",
    "    print(\"      Result: Georeferenced raster for QGIS/ArcGIS\")\n",
    "except ImportError:\n",
    "    print(\"   ‚ö†Ô∏è  GeoTIFF export requires: pip install rioxarray\")\n",
    "\n",
    "# Option 2: Save as NetCDF (preserves all metadata)\n",
    "print(\"   ‚úÖ NetCDF export (comprehensive format)\")\n",
    "print(\"      Usage: phenology_metrics.to_netcdf('phenology_data.nc')\")\n",
    "print(\"      Result: All phenological metrics with full metadata\")\n",
    "\n",
    "# Option 3: CSV export for specific pixels/regions\n",
    "print(\"   ‚úÖ CSV export for statistical analysis\")\n",
    "print(\"      Usage: Extract values and save as CSV for R/Python analysis\")\n",
    "\n",
    "# 5. Summary of per-pixel capabilities\n",
    "print(f\"\\nüéØ SUMMARY - WHAT YOU GET FOR EACH PIXEL:\")\n",
    "print(f\"   ‚Ä¢ Exact day of year when vegetation starts growing (SOS)\")\n",
    "print(f\"   ‚Ä¢ Exact day of year when vegetation senescence begins (EOS)\")\n",
    "print(f\"   ‚Ä¢ NDVI values at these critical phenological stages\")\n",
    "print(f\"   ‚Ä¢ Growing season length in days\")\n",
    "print(f\"   ‚Ä¢ Complete spatial coverage at your input resolution\")\n",
    "print(f\"   ‚Ä¢ Ready for spatial analysis, mapping, and export\")\n",
    "\n",
    "print(f\"\\nüåç SPATIAL COVERAGE:\")\n",
    "print(f\"   ‚Ä¢ Total area coverage: {len(y_coords)} √ó {len(x_coords)} pixels\")\n",
    "print(f\"   ‚Ä¢ Resolution: Matches your input data (e.g., 10m for S2)\")\n",
    "print(f\"   ‚Ä¢ Missing data: Minimized through MOGPR sensor fusion\")\n",
    "print(f\"   ‚Ä¢ Quality: Enhanced through S1+S2 integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results and Per-Pixel Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways and Real-World Applications\n",
    "\n",
    "### ‚úÖ What You Accomplished:\n",
    "\n",
    "1. **Per-Pixel Phenological Analysis**: Extracted Start/End of Season information for **every single pixel** in your study area\n",
    "2. **Multi-Sensor Data Fusion**: Combined S1 (weather-independent) and S2 (vegetation-sensitive) data using MOGPR\n",
    "3. **Gap Filling Enhancement**: Used SAR data to fill optical data gaps, improving temporal completeness\n",
    "4. **Spatial Coverage**: Achieved complete spatial coverage at your input resolution (e.g., 10m pixels)\n",
    "5. **Multiple Export Formats**: Generated data suitable for GIS, statistical analysis, and agricultural applications\n",
    "\n",
    "### üå± Per-Pixel Information Available:\n",
    "\n",
    "For **every pixel** in your study area, you now have:\n",
    "- **Start of Season (SOS) timing**: Exact day of year when vegetation growth begins\n",
    "- **End of Season (EOS) timing**: Exact day of year when senescence starts  \n",
    "- **NDVI values at SOS/EOS**: Vegetation greenness levels at critical phenological stages\n",
    "- **Growing season length**: Duration of the growing season in days\n",
    "- **Spatial patterns**: Complete mapping of phenological variations across the landscape\n",
    "\n",
    "### üéØ Real-World Applications:\n",
    "\n",
    "#### üåæ **Agricultural Applications**\n",
    "- **Crop monitoring**: Track planting and harvest timing across different fields\n",
    "- **Yield prediction**: Use SOS timing as input to crop growth models\n",
    "- **Irrigation management**: Optimize water application based on crop phenological stage\n",
    "- **Insurance claims**: Verify crop development stages for agricultural insurance\n",
    "\n",
    "#### üó∫Ô∏è **Spatial Analysis & Mapping**\n",
    "- **Land cover classification**: Use phenological patterns to distinguish crop types\n",
    "- **Climate change studies**: Analyze shifts in growing season timing over multiple years\n",
    "- **Ecosystem monitoring**: Track vegetation response to environmental changes\n",
    "- **Conservation planning**: Identify areas with unique phenological characteristics\n",
    "\n",
    "#### üìä **Research & Monitoring**\n",
    "- **Validation studies**: Compare satellite-derived SOS with ground observations\n",
    "- **Model calibration**: Use per-pixel data to calibrate ecosystem and crop models\n",
    "- **Trend analysis**: Analyze spatial patterns of phenological changes\n",
    "- **Multi-scale studies**: Aggregate pixel-level data to field, regional, or global scales\n",
    "\n",
    "### üöÄ Scaling to Larger Areas:\n",
    "\n",
    "For **operational large-scale applications**:\n",
    "\n",
    "1. **Google Earth Engine Workflow**: Use the `GEE_Data_Preparation_for_FuseTS.ipynb` notebook to:\n",
    "   - Extract data for entire countries or continents\n",
    "   - Process multiple years of data efficiently\n",
    "   - Handle cloud computing for massive datasets\n",
    "\n",
    "2. **OpenEO Integration**: Scale processing using cloud infrastructure:\n",
    "   - Process continental-scale datasets\n",
    "   - Automate annual phenology monitoring\n",
    "   - Integrate with existing operational systems\n",
    "\n",
    "3. **Temporal Analysis**: Extend to multi-year analysis:\n",
    "   - Track phenological trends over decades\n",
    "   - Analyze climate change impacts on growing seasons\n",
    "   - Generate long-term agricultural statistics\n",
    "\n",
    "### üí° Key Benefits of MOGPR Fusion:\n",
    "\n",
    "- **Weather Independence**: SAR data fills gaps during cloudy periods\n",
    "- **Enhanced Accuracy**: Cross-sensor correlations improve phenological detection\n",
    "- **Temporal Consistency**: More complete time series for robust analysis\n",
    "- **Uncertainty Quantification**: MOGPR provides confidence estimates for results\n",
    "\n",
    "### üîÑ Workflow Integration:\n",
    "\n",
    "This analysis integrates seamlessly with:\n",
    "- **GIS software** (QGIS, ArcGIS) for spatial analysis and mapping\n",
    "- **Statistical software** (R, Python, MATLAB) for advanced analytics\n",
    "- **Agricultural management systems** for operational crop monitoring\n",
    "- **Climate monitoring networks** for environmental assessments\n",
    "\n",
    "**The result**: You now have comprehensive, per-pixel Start of Season information ready for any agricultural, environmental, or research application!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Multi-Season Analysis for Tropical Agriculture (Indonesia Case)\n",
    "\n",
    "### üåæ Indonesian Agricultural Calendar:\n",
    "- **First planting season**: November - January (following year)\n",
    "- **Second planting season**: April - May  \n",
    "- **Potential third season**: August - September (some areas)\n",
    "\n",
    "This section demonstrates how to detect **multiple planting seasons per pixel** and classify areas by cropping intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_flexible_seasons_indonesia(ndvi_timeseries, time_coords, \n",
    "                                      season_duration_range=(70, 120),\n",
    "                                      min_peak_prominence=0.08, \n",
    "                                      min_peak_distance=40):\n",
    "    \"\"\"\n",
    "    Flexible multi-season detection for Indonesian agriculture with regional variations\n",
    "    \n",
    "    Handles:\n",
    "    - Season 1: Nov-Mar (flexible 3-4 month cycles)\n",
    "    - Season 2: Apr-May start (flexible timing)\n",
    "    - Season 3: Jul-Aug start (optional)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ndvi_timeseries : xarray.DataArray\n",
    "        NDVI time series data with dimensions (t, y, x)\n",
    "    time_coords : pandas.DatetimeIndex\n",
    "        Time coordinates\n",
    "    season_duration_range : tuple\n",
    "        Min and max days for a growing season\n",
    "    min_peak_prominence : float\n",
    "        Minimum NDVI prominence for peak detection\n",
    "    min_peak_distance : int\n",
    "        Minimum days between peaks\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.signal import find_peaks\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    \n",
    "    print(\"üåæ Flexible multi-season detection for Indonesian agriculture...\")\n",
    "    print(\"   Adapting to regional variations in planting timing\")\n",
    "    \n",
    "    ny, nx = ndvi_timeseries.shape[1], ndvi_timeseries.shape[2]\n",
    "    \n",
    "    # Initialize result arrays\n",
    "    season_count = np.zeros((ny, nx))\n",
    "    all_seasons = np.full((ny, nx, 6), np.nan)  # Max 3 seasons √ó 2 (SOS, EOS)\n",
    "    season_types = np.zeros((ny, nx, 3))  # Which seasons are detected\n",
    "    cropping_intensity = np.zeros((ny, nx))  # Seasons per year\n",
    "    \n",
    "    # Define flexible windows for Indonesian seasons (day of year)\n",
    "    # Season 1: November to March (305-90, handling year crossing)\n",
    "    # Season 2: April to June (90-180)  \n",
    "    # Season 3: July to September (180-270)\n",
    "    \n",
    "    day_of_year = np.array([d.dayofyear for d in time_coords])\n",
    "    \n",
    "    print(f\"Processing {ny} x {nx} = {ny*nx:,} pixels...\")\n",
    "    \n",
    "    processed_pixels = 0\n",
    "    \n",
    "    for y in range(ny):\n",
    "        if y % 10 == 0:\n",
    "            print(f\"  Processing row {y+1}/{ny}\")\n",
    "            \n",
    "        for x in range(nx):\n",
    "            pixel_ndvi = ndvi_timeseries[:, y, x].values\n",
    "            \n",
    "            # Skip if too much missing data\n",
    "            if np.isnan(pixel_ndvi).sum() > len(pixel_ndvi) * 0.5:\n",
    "                continue\n",
    "                \n",
    "            # Interpolate missing values\n",
    "            valid_mask = ~np.isnan(pixel_ndvi)\n",
    "            if valid_mask.sum() < 15:  # Need minimum data points\n",
    "                continue\n",
    "                \n",
    "            # Linear interpolation for gaps\n",
    "            pixel_ndvi_interp = np.interp(np.arange(len(pixel_ndvi)), \n",
    "                                        np.where(valid_mask)[0], \n",
    "                                        pixel_ndvi[valid_mask])\n",
    "            \n",
    "            # Light smoothing to reduce noise while preserving peaks\n",
    "            pixel_ndvi_smooth = gaussian_filter1d(pixel_ndvi_interp, sigma=1.2)\n",
    "            \n",
    "            # Find all potential peaks\n",
    "            peaks, properties = find_peaks(pixel_ndvi_smooth, \n",
    "                                         prominence=min_peak_prominence,\n",
    "                                         distance=min_peak_distance,\n",
    "                                         height=np.nanmean(pixel_ndvi_smooth) + np.nanstd(pixel_ndvi_smooth) * 0.3)\\n            \\n            if len(peaks) == 0:\\n                continue\\n                \\n            processed_pixels += 1\\n            \\n            # Get peak information\\n            peak_days = day_of_year[peaks]\\n            peak_values = pixel_ndvi_smooth[peaks]\\n            peak_positions = peaks\\n            \\n            # Group peaks by likely agricultural seasons\\n            detected_seasons = []\\n            \\n            for i, (peak_day, peak_val, peak_pos) in enumerate(zip(peak_days, peak_values, peak_positions)):\\n                \\n                # Determine which season this peak likely belongs to\\n                season_type = classify_peak_season(peak_day)\\n                \\n                if season_type > 0:\\n                    # Find season boundaries with flexible duration\\n                    sos_pos, eos_pos, season_length = find_flexible_season_boundaries(\\n                        pixel_ndvi_smooth, peak_pos, season_duration_range)\\n                    \\n                    if sos_pos is not None and eos_pos is not None:\\n                        sos_day = day_of_year[sos_pos]\\n                        eos_day = day_of_year[eos_pos]\\n                        \\n                        # Check if this season doesn't overlap too much with existing ones\\n                        is_new_season = True\\n                        for existing in detected_seasons:\\n                            if existing['type'] == season_type:\\n                                # Only keep the stronger peak for same season type\\n                                if peak_val > existing['peak_value']:\\n                                    detected_seasons.remove(existing)\\n                                else:\\n                                    is_new_season = False\\n                                break\\n                        \\n                        if is_new_season:\\n                            detected_seasons.append({\\n                                'type': season_type,\\n                                'sos_day': sos_day,\\n                                'eos_day': eos_day,\\n                                'peak_day': peak_day,\\n                                'peak_value': peak_val,\\n                                'season_length': season_length\\n                            })\\n            \\n            # Sort seasons by type (chronological order)\\n            detected_seasons.sort(key=lambda x: x['type'])\\n            \\n            # Store results\\n            num_seasons = len(detected_seasons)\\n            season_count[y, x] = num_seasons\\n            cropping_intensity[y, x] = num_seasons\\n            \\n            # Store season details\\n            for i, season in enumerate(detected_seasons):\\n                if i < 3:  # Maximum 3 seasons\\n                    all_seasons[y, x, i*2] = season['sos_day']      # SOS\\n                    all_seasons[y, x, i*2+1] = season['eos_day']    # EOS\\n                    season_types[y, x, season['type']-1] = 1        # Mark season type as detected\\n    \\n    print(f\\\"\\\\nProcessed {processed_pixels:,} pixels with valid agricultural data\\\")\\n    \\n    return {\\n        'season_count': season_count,\\n        'cropping_intensity': cropping_intensity,\\n        'season_types': season_types,\\n        'all_seasons': all_seasons,\\n        'processed_pixels': processed_pixels\\n    }\\n\\ndef classify_peak_season(day_of_year):\\n    \\\"\\\"\\\"\\n    Classify which Indonesian agricultural season a peak belongs to\\n    Returns: 1 (Nov-Mar), 2 (Apr-Jun), 3 (Jul-Sep), 0 (unclassified)\\n    \\\"\\\"\\\"\\n    \\n    # Season 1: November to March (handle year boundary)\\n    # Nov-Dec: days 305-365, Jan-Mar: days 1-90\\n    if day_of_year >= 305 or day_of_year <= 90:\\n        return 1\\n    \\n    # Season 2: April to June (days 90-180)\\n    elif 90 < day_of_year <= 180:\\n        return 2\\n        \\n    # Season 3: July to September (days 180-270) \\n    elif 180 < day_of_year <= 270:\\n        return 3\\n        \\n    # October: transition period, usually not main planting\\n    else:\\n        return 0\\n\\ndef find_flexible_season_boundaries(ndvi_smooth, peak_pos, duration_range):\\n    \\\"\\\"\\\"\\n    Find flexible season boundaries allowing for variable crop duration\\n    \\\"\\\"\\\"\\n    min_duration, max_duration = duration_range\\n    \\n    # Search for SOS: look backwards from peak\\n    sos_search_window = min(peak_pos, max_duration // 2)\\n    sos_start = max(0, peak_pos - sos_search_window)\\n    \\n    # Find the valley (minimum) before the peak\\n    pre_peak_values = ndvi_smooth[sos_start:peak_pos]\\n    if len(pre_peak_values) > 5:\\n        sos_rel_pos = np.argmin(pre_peak_values)\\n        sos_pos = sos_start + sos_rel_pos\\n    else:\\n        sos_pos = max(0, peak_pos - min_duration // 2)\\n    \\n    # Search for EOS: look forwards from peak\\n    eos_search_window = min(len(ndvi_smooth) - peak_pos, max_duration // 2)\\n    eos_end = min(len(ndvi_smooth), peak_pos + eos_search_window)\\n    \\n    # Find the valley (minimum) after the peak\\n    post_peak_values = ndvi_smooth[peak_pos:eos_end]\\n    if len(post_peak_values) > 5:\\n        eos_rel_pos = np.argmin(post_peak_values)\\n        eos_pos = peak_pos + eos_rel_pos\\n    else:\\n        eos_pos = min(len(ndvi_smooth) - 1, peak_pos + min_duration // 2)\\n    \\n    # Calculate season length\\n    season_length = eos_pos - sos_pos\\n    \\n    # Validate season length\\n    if min_duration <= season_length <= max_duration:\\n        return sos_pos, eos_pos, season_length\\n    else:\\n        return None, None, 0\\n\\n# Apply flexible multi-season detection\\nprint(\\\"üáÆüá© FLEXIBLE MULTI-SEASON DETECTION FOR INDONESIA\\\")\\nprint(\\\"=\\\" * 60)\\nprint(\\\"Adapting to regional variations:\\\")\\nprint(\\\"‚Ä¢ Season 1: Nov-Mar (flexible 3-4 month duration)\\\")\\nprint(\\\"‚Ä¢ Season 2: Apr-Jun (flexible timing)\\\")\\nprint(\\\"‚Ä¢ Season 3: Jul-Sep (optional, region-dependent)\\\")\\nprint()\\n\\nflexible_results = detect_flexible_seasons_indonesia(\\n    fused_result['S2ndvi'], \\n    time_idx,\\n    season_duration_range=(70, 130),  # 2.5-4.5 month seasons\\n    min_peak_prominence=0.06,         # Lower threshold for subtle changes\\n    min_peak_distance=35              # Allow closer peaks for intensive systems\\n)\\n\\n# Analyze results\\nprint(\\\"\\\\nüìä INDONESIAN AGRICULTURAL PATTERNS DETECTED:\\\")\\n\\nseason_counts = flexible_results['season_count']\\ncropping_intensity = flexible_results['cropping_intensity']\\nseason_types = flexible_results['season_types']\\n\\ntotal_pixels = season_counts.size\\nvalid_pixels = flexible_results['processed_pixels']\\n\\nprint(f\\\"\\\\nüåç Spatial Coverage:\\\")\\nprint(f\\\"Total pixels: {total_pixels:,}\\\")\\nprint(f\\\"Agricultural pixels: {valid_pixels:,} ({valid_pixels/total_pixels*100:.1f}%)\\\")\\n\\n# Cropping intensity analysis\\nprint(f\\\"\\\\nüåæ Cropping Intensity (Seasons per Year):\\\")\\nfor intensity in [1, 2, 3]:\\n    count = (season_counts == intensity).sum()\\n    pct = count / valid_pixels * 100 if valid_pixels > 0 else 0\\n    print(f\\\"  {intensity} season(s): {count:,} pixels ({pct:.1f}%)\\\")\\n\\n# Seasonal pattern analysis\\nprint(f\\\"\\\\nüìÖ Seasonal Patterns:\\\")\\nseason_names = ['Nov-Mar (Season 1)', 'Apr-Jun (Season 2)', 'Jul-Sep (Season 3)']\\n\\nfor i, season_name in enumerate(season_names):\\n    season_pixels = (season_types[:, :, i] == 1).sum()\\n    pct = season_pixels / valid_pixels * 100 if valid_pixels > 0 else 0\\n    print(f\\\"  {season_name}: {season_pixels:,} pixels ({pct:.1f}%)\\\")\\n\\n# Regional cropping patterns\\nprint(f\\\"\\\\nüó∫Ô∏è  Regional Cropping Patterns:\\\")\\n\\n# Single season areas (likely rain-fed)\\nsingle_season = (season_counts == 1).sum()\\nprint(f\\\"  Rain-fed areas (1 season): {single_season:,} pixels\\\")\\n\\n# Double season areas (common irrigated rice)\\ndouble_season = (season_counts == 2).sum() \\nprint(f\\\"  Irrigated areas (2 seasons): {double_season:,} pixels\\\")\\n\\n# Triple season areas (intensive agriculture)\\ntriple_season = (season_counts == 3).sum()\\nprint(f\\\"  Intensive areas (3 seasons): {triple_season:,} pixels\\\")\\n\\nprint(f\\\"\\\\n‚úÖ Flexible multi-season detection completed!\\\")\\nprint(f\\\"   Each pixel classified by cropping intensity and seasonal patterns\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Quick Asset Check - Run This First!\n",
    "\n",
    "Before running the main loading code, let's verify your assets exist and are accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: List assets in your FuseTS folder\n",
    "print(\"=\"*70)\n",
    "print(\"üîç CHECKING GEE ASSETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Initialize GEE\n",
    "    ee.Initialize(project='ee-geodeticengineeringundip')\n",
    "    print(\"‚úÖ GEE initialized\\n\")\n",
    "    \n",
    "    # List assets in the folder\n",
    "    asset_folder = 'projects/ee-geodeticengineeringundip/assets/FuseTS'\n",
    "    \n",
    "    print(f\"üìÇ Listing assets in: {asset_folder}\\n\")\n",
    "    \n",
    "    # Get list of assets\n",
    "    asset_list = ee.data.listAssets({'parent': asset_folder})\n",
    "    \n",
    "    if 'assets' in asset_list and len(asset_list['assets']) > 0:\n",
    "        assets = asset_list['assets']\n",
    "        print(f\"‚úÖ Found {len(assets)} assets:\\n\")\n",
    "        \n",
    "        # Group by period\n",
    "        period_assets = [a for a in assets if 'Period_' in a['name']]\n",
    "        print(f\"   üìä Period files: {len(period_assets)}\")\n",
    "        \n",
    "        # Show first few\n",
    "        for asset in period_assets[:5]:\n",
    "            asset_name = asset['name'].split('/')[-1]\n",
    "            print(f\"      ‚Ä¢ {asset_name}\")\n",
    "        \n",
    "        if len(period_assets) > 5:\n",
    "            print(f\"      ... and {len(period_assets) - 5} more\")\n",
    "        \n",
    "        # Check for expected pattern\n",
    "        expected_prefix = \"S1_S2_Nov2024_Oct2025_Period_\"\n",
    "        matching = [a for a in period_assets if expected_prefix in a['name']]\n",
    "        \n",
    "        print(f\"\\n   ‚úÖ Assets matching '{expected_prefix}*': {len(matching)}\")\n",
    "        \n",
    "        if len(matching) >= 31:\n",
    "            print(f\"   ‚úÖ All 31 periods found! Ready to load.\")\n",
    "        elif len(matching) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  Only {len(matching)}/31 periods found. Some exports may still be running.\")\n",
    "            print(f\"   üìç Check: https://code.earthengine.google.com/tasks\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå No assets matching expected pattern found!\")\n",
    "            print(f\"   üí° Your assets might have a different naming pattern.\")\n",
    "            print(f\"\\n   Available asset names:\")\n",
    "            for asset in assets[:10]:\n",
    "                print(f\"      ‚Ä¢ {asset['name'].split('/')[-1]}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"‚ùå No assets found in folder: {asset_folder}\")\n",
    "        print(f\"\\nüí° Possible reasons:\")\n",
    "        print(f\"   1. Assets haven't been exported yet\")\n",
    "        print(f\"   2. Exports are still running (check https://code.earthengine.google.com/tasks)\")\n",
    "        print(f\"   3. Wrong folder path\")\n",
    "        print(f\"   4. No read permissions\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking assets: {e}\")\n",
    "    print(f\"\\nüí° Make sure you've:\")\n",
    "    print(f\"   1. Authenticated GEE (ran cell 1 above)\")\n",
    "    print(f\"   2. Exported data using GEE_Data_Preparation notebook\")\n",
    "    print(f\"   3. Waited for exports to complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize Indonesian Multi-Season Agriculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Indonesian agricultural patterns\\nprint(\\\"üó∫Ô∏è  VISUALIZING INDONESIAN AGRICULTURAL PATTERNS\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# Create comprehensive visualization\\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\\nfig.suptitle('Indonesian Agricultural Patterns - Multi-Season Analysis', fontsize=16, fontweight='bold')\\n\\n# 1. Cropping Intensity Map\\nim1 = axes[0, 0].imshow(cropping_intensity, cmap='RdYlGn', vmin=0, vmax=3)\\naxes[0, 0].set_title('üåæ Cropping Intensity\\\\n(Seasons per Year)', fontweight='bold')\\ncbar1 = plt.colorbar(im1, ax=axes[0, 0])\\ncbar1.set_label('Number of Seasons')\\ncbar1.set_ticks([0, 1, 2, 3])\\ncbar1.set_ticklabels(['None', '1 Season\\\\n(Rain-fed)', '2 Seasons\\\\n(Irrigated)', '3 Seasons\\\\n(Intensive)'])\\n\\n# 2. Season 1 (Nov-Mar) Distribution\\nseason1_map = season_types[:, :, 0]  # Season 1 presence\\nim2 = axes[0, 1].imshow(season1_map, cmap='Blues', vmin=0, vmax=1)\\naxes[0, 1].set_title('üåæ Season 1: Nov-Mar\\\\n(Main Season)', fontweight='bold')\\ncbar2 = plt.colorbar(im2, ax=axes[0, 1])\\ncbar2.set_label('Season Present')\\ncbar2.set_ticks([0, 1])\\ncbar2.set_ticklabels(['No', 'Yes'])\\n\\n# 3. Season 2 (Apr-Jun) Distribution  \\nseason2_map = season_types[:, :, 1]  # Season 2 presence\\nim3 = axes[0, 2].imshow(season2_map, cmap='Greens', vmin=0, vmax=1)\\naxes[0, 2].set_title('üåæ Season 2: Apr-Jun\\\\n(Dry Season)', fontweight='bold')\\ncbar3 = plt.colorbar(im3, ax=axes[0, 2])\\ncbar3.set_label('Season Present')\\ncbar3.set_ticks([0, 1])\\ncbar3.set_ticklabels(['No', 'Yes'])\\n\\n# 4. Season 3 (Jul-Sep) Distribution\\nseason3_map = season_types[:, :, 2]  # Season 3 presence  \\nim4 = axes[1, 0].imshow(season3_map, cmap='Oranges', vmin=0, vmax=1)\\naxes[1, 0].set_title('üåæ Season 3: Jul-Sep\\\\n(Optional)', fontweight='bold')\\ncbar4 = plt.colorbar(im4, ax=axes[1, 0])\\ncbar4.set_label('Season Present')\\ncbar4.set_ticks([0, 1])\\ncbar4.set_ticklabels(['No', 'Yes'])\\n\\n# 5. Agricultural vs Non-Agricultural Areas\\nagri_mask = (season_counts > 0).astype(int)\\nim5 = axes[1, 1].imshow(agri_mask, cmap='RdYlBu_r', vmin=0, vmax=1)\\naxes[1, 1].set_title('üó∫Ô∏è  Agricultural Areas\\\\n(Any Season Detected)', fontweight='bold')\\ncbar5 = plt.colorbar(im5, ax=axes[1, 1])\\ncbar5.set_label('Land Use')\\ncbar5.set_ticks([0, 1])\\ncbar5.set_ticklabels(['Non-Agricultural', 'Agricultural'])\\n\\n# 6. Season Start Timing for Season 1 (handling year boundary)\\nseason1_sos = flexible_results['all_seasons'][:, :, 0]  # First season SOS\\n# Mask out non-season1 pixels\\nseason1_sos_masked = np.where(season_types[:, :, 0] == 1, season1_sos, np.nan)\\n\\n# Handle year boundary for Season 1 (Nov-Mar)\\n# Convert to continuous scale: Nov=1, Dec=2, Jan=13, Feb=14, Mar=15\\nseason1_sos_continuous = season1_sos_masked.copy()\\nfor y in range(season1_sos_continuous.shape[0]):\\n    for x in range(season1_sos_continuous.shape[1]):\\n        if not np.isnan(season1_sos_continuous[y, x]):\\n            day = season1_sos_continuous[y, x]\\n            if day >= 305:  # Nov-Dec\\n                season1_sos_continuous[y, x] = (day - 305) + 1  # Nov=1, Dec=32\\n            elif day <= 90:  # Jan-Mar\\n                season1_sos_continuous[y, x] = day + 61  # Jan=62, Mar=151\\n\\nim6 = axes[1, 2].imshow(season1_sos_continuous, cmap='viridis', vmin=1, vmax=151)\\naxes[1, 2].set_title('üìÖ Season 1 Start Timing\\\\n(Nov-Mar)', fontweight='bold')\\ncbar6 = plt.colorbar(im6, ax=axes[1, 2])\\ncbar6.set_label('Planting Time')\\n# Custom labels for year-boundary season\\ncbar6.set_ticks([1, 32, 62, 92, 121, 151])\\ncbar6.set_ticklabels(['Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Mar-end'])\\n\\n# Add pixel coordinates\\nfor ax in axes.flat:\\n    ax.set_xlabel('X coordinate (pixel)')\\n    ax.set_ylabel('Y coordinate (pixel)')\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Additional analysis: Detailed statistics\\nprint(\\\"\\\\nüìà DETAILED AGRICULTURAL STATISTICS:\\\")\\n\\n# Season timing analysis\\nprint(\\\"\\\\n‚è∞ Season Timing Analysis:\\\")\\n\\nfor season_idx, season_name in enumerate(['Season 1 (Nov-Mar)', 'Season 2 (Apr-Jun)', 'Season 3 (Jul-Sep)']):\\n    season_sos = flexible_results['all_seasons'][:, :, season_idx*2]\\n    season_present = season_types[:, :, season_idx] == 1\\n    \\n    if season_present.sum() > 0:\\n        valid_sos = season_sos[season_present]\\n        valid_sos_clean = valid_sos[~np.isnan(valid_sos)]\\n        \\n        if len(valid_sos_clean) > 0:\\n            print(f\\\"\\\\n  {season_name}:\\\")\\n            print(f\\\"    Pixels with this season: {season_present.sum():,}\\\")\\n            print(f\\\"    Average start: Day {np.mean(valid_sos_clean):.0f}\\\")\\n            print(f\\\"    Start range: Day {np.min(valid_sos_clean):.0f} - {np.max(valid_sos_clean):.0f}\\\")\\n            print(f\\\"    Standard deviation: {np.std(valid_sos_clean):.1f} days\\\")\\n\\n# Regional agricultural patterns\\nprint(\\\"\\\\nüåç Regional Patterns Summary:\\\")\\ntotal_agri_pixels = (season_counts > 0).sum()\\n\\nif total_agri_pixels > 0:\\n    # Calculate percentages of different farming systems\\n    rain_fed_pct = (season_counts == 1).sum() / total_agri_pixels * 100\\n    irrigated_pct = (season_counts == 2).sum() / total_agri_pixels * 100  \\n    intensive_pct = (season_counts == 3).sum() / total_agri_pixels * 100\\n    \\n    print(f\\\"  Rain-fed agriculture (1 season): {rain_fed_pct:.1f}% of agricultural areas\\\")\\n    print(f\\\"  Irrigated agriculture (2 seasons): {irrigated_pct:.1f}% of agricultural areas\\\")\\n    print(f\\\"  Intensive agriculture (3 seasons): {intensive_pct:.1f}% of agricultural areas\\\")\\n    \\n    # Season popularity\\n    print(f\\\"\\\\nüìä Season Popularity:\\\")\\n    for i, season_name in enumerate(['Season 1 (Nov-Mar)', 'Season 2 (Apr-Jun)', 'Season 3 (Jul-Sep)']):\\n        season_pixels = (season_types[:, :, i] == 1).sum()\\n        season_pct = season_pixels / total_agri_pixels * 100\\n        print(f\\\"  {season_name}: {season_pct:.1f}% of agricultural pixels\\\")\\n\\nprint(f\\\"\\\\n‚úÖ Indonesian multi-season analysis completed!\\\")\\nprint(f\\\"   üéØ Result: Complete classification of agricultural intensity and seasonal patterns\\\")\\nprint(f\\\"   üó∫Ô∏è  Output: Per-pixel cropping intensity and seasonal timing information\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Export Indonesian Multi-Season Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Indonesian multi-season agricultural analysis results\\nprint(\\\"üíæ EXPORTING INDONESIAN MULTI-SEASON AGRICULTURAL DATA\\\")\\nprint(\\\"=\\\" * 60)\\n\\n# Create comprehensive dataset with all Indonesian agricultural information\\nindonesian_agricultural_data = xr.Dataset({\\n    # Cropping intensity (number of seasons per year)\\n    'cropping_intensity': xr.DataArray(\\n        cropping_intensity,\\n        dims=['y', 'x'],\\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Agricultural Cropping Intensity',\\n            'description': 'Number of planting seasons per year (0=non-agricultural, 1=rain-fed, 2=irrigated, 3=intensive)',\\n            'units': 'seasons per year',\\n            'valid_range': [0, 3]\\n        }\\n    ),\\n    \\n    # Season presence maps\\n    'season1_present': xr.DataArray(\\n        season_types[:, :, 0],\\n        dims=['y', 'x'],\\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Season 1 Presence (Nov-Mar)',\\n            'description': 'Whether first planting season (Nov-Mar) is detected',\\n            'units': 'boolean (0=absent, 1=present)'\\n        }\\n    ),\\n    \\n    'season2_present': xr.DataArray(\\n        season_types[:, :, 1],\\n        dims=['y', 'x'], \\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Season 2 Presence (Apr-Jun)',\\n            'description': 'Whether second planting season (Apr-Jun) is detected',\\n            'units': 'boolean (0=absent, 1=present)'\\n        }\\n    ),\\n    \\n    'season3_present': xr.DataArray(\\n        season_types[:, :, 2],\\n        dims=['y', 'x'],\\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Season 3 Presence (Jul-Sep)',\\n            'description': 'Whether third planting season (Jul-Sep) is detected',\\n            'units': 'boolean (0=absent, 1=present)'\\n        }\\n    ),\\n    \\n    # Season timing information\\n    'season1_start': xr.DataArray(\\n        flexible_results['all_seasons'][:, :, 0],\\n        dims=['y', 'x'],\\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Season 1 Start Day (Nov-Mar)',\\n            'description': 'Day of year when first planting season starts (handles Nov-Mar year boundary)',\\n            'units': 'day of year',\\n            'note': 'Nov-Dec: days 305-365, Jan-Mar: days 1-90'\\n        }\\n    ),\\n    \\n    'season1_end': xr.DataArray(\\n        flexible_results['all_seasons'][:, :, 1],\\n        dims=['y', 'x'],\\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Season 1 End Day (Nov-Mar)',\\n            'description': 'Day of year when first planting season ends',\\n            'units': 'day of year'\\n        }\\n    ),\\n    \\n    'season2_start': xr.DataArray(\\n        flexible_results['all_seasons'][:, :, 2],\\n        dims=['y', 'x'],\\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Season 2 Start Day (Apr-Jun)',\\n            'description': 'Day of year when second planting season starts',\\n            'units': 'day of year'\\n        }\\n    ),\\n    \\n    'season2_end': xr.DataArray(\\n        flexible_results['all_seasons'][:, :, 3],\\n        dims=['y', 'x'],\\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Season 2 End Day (Apr-Jun)',\\n            'description': 'Day of year when second planting season ends',\\n            'units': 'day of year'\\n        }\\n    ),\\n    \\n    'season3_start': xr.DataArray(\\n        flexible_results['all_seasons'][:, :, 4],\\n        dims=['y', 'x'],\\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Season 3 Start Day (Jul-Sep)',\\n            'description': 'Day of year when third planting season starts (optional)',\\n            'units': 'day of year'\\n        }\\n    ),\\n    \\n    'season3_end': xr.DataArray(\\n        flexible_results['all_seasons'][:, :, 5],\\n        dims=['y', 'x'],\\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Season 3 End Day (Jul-Sep)',\\n            'description': 'Day of year when third planting season ends (optional)',\\n            'units': 'day of year'\\n        }\\n    ),\\n    \\n    # Agricultural mask\\n    'agricultural_areas': xr.DataArray(\\n        (season_counts > 0).astype(int),\\n        dims=['y', 'x'],\\n        coords={'y': y_coords, 'x': x_coords},\\n        attrs={\\n            'long_name': 'Agricultural Land Classification',\\n            'description': 'Binary mask identifying agricultural vs non-agricultural areas',\\n            'units': 'boolean (0=non-agricultural, 1=agricultural)'\\n        }\\n    )\\n})\\n\\n# Add global attributes\\nindonesian_agricultural_data.attrs.update({\\n    'title': 'Indonesian Multi-Season Agricultural Analysis from MOGPR S1+S2 Fusion',\\n    'description': 'Per-pixel classification of agricultural intensity and seasonal timing for Indonesian agriculture',\\n    'methodology': 'Flexible multi-season detection adapted for Indonesian agricultural calendar',\\n    'seasons': {\\n        'season_1': 'November-March (main season, handles year boundary)',\\n        'season_2': 'April-June (dry season)',\\n        'season_3': 'July-September (optional intensive season)'\\n    },\\n    'cropping_systems': {\\n        '1_season': 'Rain-fed agriculture',\\n        '2_seasons': 'Irrigated agriculture (typical rice systems)',\\n        '3_seasons': 'Intensive agriculture with optimal irrigation'\\n    },\\n    'spatial_coverage': f'{len(y_coords)} x {len(x_coords)} pixels',\\n    'total_pixels': len(y_coords) * len(x_coords),\\n    'agricultural_pixels': int((season_counts > 0).sum()),\\n    'processing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\\n    'software': 'FuseTS with MOGPR algorithm + Indonesian agricultural calendar adaptation',\\n    'data_source': 'Sentinel-1 VV/VH + Sentinel-2 NDVI fusion',\\n    'country': 'Indonesia',\\n    'contact': 'Adapted for Indonesian agricultural patterns'\\n})\\n\\n# Save the comprehensive dataset\\nindonesian_output = \\\"indonesian_multi_season_agriculture.nc\\\"\\nindonesian_agricultural_data.to_netcdf(indonesian_output)\\nprint(f\\\"‚úÖ Indonesian agricultural data saved to: {indonesian_output}\\\")\\nprint(f\\\"   üìä Contains complete multi-season information for {(season_counts > 0).sum():,} agricultural pixels\\\")\\n\\n# Create summary statistics for Indonesian agriculture\\nindonesian_stats = {\\n    'total_analysis': {\\n        'total_pixels': int(season_counts.size),\\n        'agricultural_pixels': int((season_counts > 0).sum()),\\n        'agricultural_percentage': float((season_counts > 0).sum() / season_counts.size * 100)\\n    },\\n    'cropping_intensity': {\\n        'single_season_pixels': int((season_counts == 1).sum()),\\n        'double_season_pixels': int((season_counts == 2).sum()),\\n        'triple_season_pixels': int((season_counts == 3).sum()),\\n        'single_season_percentage': float((season_counts == 1).sum() / (season_counts > 0).sum() * 100) if (season_counts > 0).sum() > 0 else 0,\\n        'double_season_percentage': float((season_counts == 2).sum() / (season_counts > 0).sum() * 100) if (season_counts > 0).sum() > 0 else 0,\\n        'triple_season_percentage': float((season_counts == 3).sum() / (season_counts > 0).sum() * 100) if (season_counts > 0).sum() > 0 else 0\\n    },\\n    'seasonal_patterns': {\\n        'season1_nov_mar_pixels': int((season_types[:, :, 0] == 1).sum()),\\n        'season2_apr_jun_pixels': int((season_types[:, :, 1] == 1).sum()),\\n        'season3_jul_sep_pixels': int((season_types[:, :, 2] == 1).sum()),\\n        'season1_percentage': float((season_types[:, :, 0] == 1).sum() / (season_counts > 0).sum() * 100) if (season_counts > 0).sum() > 0 else 0,\\n        'season2_percentage': float((season_types[:, :, 1] == 1).sum() / (season_counts > 0).sum() * 100) if (season_counts > 0).sum() > 0 else 0,\\n        'season3_percentage': float((season_types[:, :, 2] == 1).sum() / (season_counts > 0).sum() * 100) if (season_counts > 0).sum() > 0 else 0\\n    }\\n}\\n\\n# Save statistics\\nindonesian_stats_file = \\\"indonesian_agriculture_statistics.json\\\"\\nwith open(indonesian_stats_file, 'w') as f:\\n    json.dump(indonesian_stats, f, indent=2)\\nprint(f\\\"‚úÖ Indonesian agricultural statistics saved to: {indonesian_stats_file}\\\")\\n\\n# Create a simple CSV for immediate analysis\\nprint(f\\\"\\\\nüìä Creating sample CSV for agricultural analysis...\\\")\\n\\n# Extract sample data for CSV (every 3rd pixel to reduce file size)\\nsample_data_indonesia = []\\nfor i in range(0, len(y_coords), 3):\\n    for j in range(0, len(x_coords), 3):\\n        if season_counts[i, j] > 0:  # Only agricultural pixels\\n            sample_data_indonesia.append({\\n                'pixel_y': i,\\n                'pixel_x': j,\\n                'cropping_intensity': int(cropping_intensity[i, j]),\\n                'season1_present': int(season_types[i, j, 0]),\\n                'season2_present': int(season_types[i, j, 1]),\\n                'season3_present': int(season_types[i, j, 2]),\\n                'season1_start_day': flexible_results['all_seasons'][i, j, 0] if not np.isnan(flexible_results['all_seasons'][i, j, 0]) else None,\\n                'season1_end_day': flexible_results['all_seasons'][i, j, 1] if not np.isnan(flexible_results['all_seasons'][i, j, 1]) else None,\\n                'season2_start_day': flexible_results['all_seasons'][i, j, 2] if not np.isnan(flexible_results['all_seasons'][i, j, 2]) else None,\\n                'season2_end_day': flexible_results['all_seasons'][i, j, 3] if not np.isnan(flexible_results['all_seasons'][i, j, 3]) else None,\\n                'season3_start_day': flexible_results['all_seasons'][i, j, 4] if not np.isnan(flexible_results['all_seasons'][i, j, 4]) else None,\\n                'season3_end_day': flexible_results['all_seasons'][i, j, 5] if not np.isnan(flexible_results['all_seasons'][i, j, 5]) else None,\\n                'farming_system': 'rain-fed' if cropping_intensity[i, j] == 1 else 'irrigated' if cropping_intensity[i, j] == 2 else 'intensive'\\n            })\\n\\nsample_df_indonesia = pd.DataFrame(sample_data_indonesia)\\ncsv_file_indonesia = \\\"sample_indonesian_agriculture.csv\\\"\\nsample_df_indonesia.to_csv(csv_file_indonesia, index=False)\\nprint(f\\\"‚úÖ Sample Indonesian agricultural data saved to: {csv_file_indonesia}\\\")\\nprint(f\\\"   üìä Contains {len(sample_df_indonesia)} sample agricultural pixels\\\")\\n\\n# Print summary of exports\\nprint(f\\\"\\\\nüìÅ INDONESIAN AGRICULTURAL ANALYSIS - EXPORTED FILES:\\\")\\nprint(f\\\"\\\\nüåæ Main Dataset:\\\")\\nprint(f\\\"   ‚Ä¢ {indonesian_output} - Complete multi-season agricultural data\\\")\\nprint(f\\\"   ‚Ä¢ Contains: Cropping intensity, seasonal timing, agricultural classification\\\")\\nprint(f\\\"   ‚Ä¢ Format: NetCDF (GIS-compatible, preserves metadata)\\\")\\n\\nprint(f\\\"\\\\nüìä Statistics & Analysis:\\\")\\nprint(f\\\"   ‚Ä¢ {indonesian_stats_file} - Comprehensive agricultural statistics\\\")\\nprint(f\\\"   ‚Ä¢ {csv_file_indonesia} - Sample data for immediate analysis\\\")\\n\\nprint(f\\\"\\\\nüéØ KEY RESULTS FOR INDONESIAN AGRICULTURE:\\\")\\nprint(f\\\"   ‚úÖ Per-pixel cropping intensity classification (1-3 seasons)\\\")\\nprint(f\\\"   ‚úÖ Seasonal timing for each planting season (Nov-Mar, Apr-Jun, Jul-Sep)\\\")\\nprint(f\\\"   ‚úÖ Agricultural vs non-agricultural area identification\\\")\\nprint(f\\\"   ‚úÖ Farming system classification (rain-fed, irrigated, intensive)\\\")\\nprint(f\\\"   ‚úÖ Flexible adaptation to regional planting variations\\\")\\n\\nprint(f\\\"\\\\nüöÄ APPLICATIONS:\\\")\\nprint(f\\\"   ‚Ä¢ Agricultural monitoring and planning\\\")\\nprint(f\\\"   ‚Ä¢ Irrigation system optimization\\\")\\nprint(f\\\"   ‚Ä¢ Crop insurance and yield estimation\\\")\\nprint(f\\\"   ‚Ä¢ Food security assessments\\\")\\nprint(f\\\"   ‚Ä¢ Climate change impact studies\\\")\\n\\nprint(f\\\"\\\\nüåç READY FOR:\\\")\\nprint(f\\\"   ‚Ä¢ Ministry of Agriculture reporting\\\")\\nprint(f\\\"   ‚Ä¢ Regional agricultural planning\\\")\\nprint(f\\\"   ‚Ä¢ Research and academic studies\\\")\\nprint(f\\\"   ‚Ä¢ International agricultural monitoring\\\")\\n\\nprint(f\\\"\\\\n‚úÖ Indonesian multi-season agricultural analysis complete!\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Calculate Planting Indices and Agricultural Patterns\n",
    "\n",
    "### üåæ Advanced Agricultural Analytics\n",
    "\n",
    "This section derives comprehensive planting indices from the multi-season analysis, providing insights for:\n",
    "- **Agricultural Planning**: Understanding planting patterns and timing\n",
    "- **Irrigation Management**: Identifying irrigation-dependent areas\n",
    "- **Food Security**: Assessing agricultural intensification\n",
    "- **Policy Making**: Supporting subsidy and insurance programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåæ CALCULATING PLANTING INDICES AND AGRICULTURAL PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PLANTING PATTERN CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def classify_planting_pattern(season_types):\n",
    "    \"\"\"\n",
    "    Classify agricultural planting patterns based on seasonal presence\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Pattern codes:\n",
    "        0: Non-agricultural\n",
    "        1: Single season (Nov-Mar only) - Traditional rain-fed\n",
    "        2: Double season (Nov-Mar + Apr-Jun) - Standard irrigated\n",
    "        3: Triple season (All three) - Intensive agriculture\n",
    "        4: Dry season focus (Apr-Jun + Jul-Sep) - Alternative pattern\n",
    "        5: Mid-year cropping (Apr-Jun only)\n",
    "        6: Late season only (Jul-Sep only)\n",
    "        7: Other combinations\n",
    "    \"\"\"\n",
    "    patterns = np.zeros(season_types.shape[:2], dtype=int)\n",
    "    \n",
    "    s1 = season_types[:, :, 0]  # Nov-Mar\n",
    "    s2 = season_types[:, :, 1]  # Apr-Jun\n",
    "    s3 = season_types[:, :, 2]  # Jul-Sep\n",
    "    \n",
    "    # Pattern 1: Single season (Nov-Mar) - Traditional rain-fed\n",
    "    patterns[(s1 == 1) & (s2 == 0) & (s3 == 0)] = 1\n",
    "    \n",
    "    # Pattern 2: Double season (Nov-Mar + Apr-Jun) - Standard irrigated\n",
    "    patterns[(s1 == 1) & (s2 == 1) & (s3 == 0)] = 2\n",
    "    \n",
    "    # Pattern 3: Triple season - Intensive agriculture\n",
    "    patterns[(s1 == 1) & (s2 == 1) & (s3 == 1)] = 3\n",
    "    \n",
    "    # Pattern 4: Dry season focus (skip main season)\n",
    "    patterns[(s1 == 0) & (s2 == 1) & (s3 == 1)] = 4\n",
    "    \n",
    "    # Pattern 5: Mid-year only\n",
    "    patterns[(s1 == 0) & (s2 == 1) & (s3 == 0)] = 5\n",
    "    \n",
    "    # Pattern 6: Late season only\n",
    "    patterns[(s1 == 0) & (s2 == 0) & (s3 == 1)] = 6\n",
    "    \n",
    "    # Pattern 7: Other combinations\n",
    "    patterns[(s1 + s2 + s3 > 0) & (patterns == 0)] = 7\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "planting_patterns = classify_planting_pattern(season_types)\n",
    "\n",
    "print(\"‚úÖ Planting Pattern Classification Complete\")\n",
    "print(f\"\\nPattern Distribution:\")\n",
    "pattern_names = {\n",
    "    0: 'Non-agricultural',\n",
    "    1: 'Single season (rain-fed)',\n",
    "    2: 'Double season (irrigated)',\n",
    "    3: 'Triple season (intensive)',\n",
    "    4: 'Dry season focus',\n",
    "    5: 'Mid-year only',\n",
    "    6: 'Late season only',\n",
    "    7: 'Other patterns'\n",
    "}\n",
    "\n",
    "for pattern_code, pattern_name in pattern_names.items():\n",
    "    pixel_count = (planting_patterns == pattern_code).sum()\n",
    "    if pixel_count > 0:\n",
    "        percentage = pixel_count / planting_patterns.size * 100\n",
    "        print(f\"  Pattern {pattern_code} ({pattern_name}): {pixel_count:,} pixels ({percentage:.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. IRRIGATION DEPENDENCY INDEX\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_irrigation_index(cropping_intensity, season_types):\n",
    "    \"\"\"\n",
    "    Calculate irrigation dependency index (0-1)\n",
    "    \n",
    "    Based on:\n",
    "    - Number of cropping seasons (more seasons = more irrigation)\n",
    "    - Presence of dry season crops (Apr-Jun, Jul-Sep)\n",
    "    - Triple cropping capability (requires consistent irrigation)\n",
    "    \"\"\"\n",
    "    irrigation_index = np.zeros(cropping_intensity.shape, dtype=float)\n",
    "    \n",
    "    # Rain-fed areas (single season, only Nov-Mar)\n",
    "    irrigation_index[cropping_intensity == 1] = 0.0\n",
    "    \n",
    "    # Partial irrigation (double cropping)\n",
    "    irrigation_index[cropping_intensity == 2] = 0.5\n",
    "    \n",
    "    # Full irrigation (triple cropping)\n",
    "    irrigation_index[cropping_intensity == 3] = 1.0\n",
    "    \n",
    "    # Adjust based on dry season presence\n",
    "    # If crops in dry season (Apr-Jun or Jul-Sep), increase irrigation dependency\n",
    "    dry_season_crops = (season_types[:, :, 1] == 1) | (season_types[:, :, 2] == 1)\n",
    "    irrigation_index[dry_season_crops & (irrigation_index < 0.5)] = 0.5\n",
    "    \n",
    "    return irrigation_index\n",
    "\n",
    "irrigation_dependency = calculate_irrigation_index(cropping_intensity, season_types)\n",
    "\n",
    "print(\"\\n‚úÖ Irrigation Dependency Index Calculated\")\n",
    "print(f\"\\nIrrigation Dependency Distribution:\")\n",
    "print(f\"  No irrigation (0.0): {(irrigation_dependency == 0.0).sum():,} pixels\")\n",
    "print(f\"  Partial irrigation (0.5): {(irrigation_dependency == 0.5).sum():,} pixels\")\n",
    "print(f\"  Full irrigation (1.0): {(irrigation_dependency == 1.0).sum():,} pixels\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PLANTING DATE STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_planting_dates(season_start_data, season_mask, season_name):\n",
    "    \"\"\"\n",
    "    Analyze planting date distribution for a specific season\n",
    "    \"\"\"\n",
    "    valid_dates = season_start_data[season_mask == 1]\n",
    "    valid_dates_clean = valid_dates[~np.isnan(valid_dates)]\n",
    "    \n",
    "    if len(valid_dates_clean) == 0:\n",
    "        return None\n",
    "    \n",
    "    stats = {\n",
    "        'season': season_name,\n",
    "        'pixel_count': len(valid_dates_clean),\n",
    "        'earliest_planting': int(np.min(valid_dates_clean)),\n",
    "        'latest_planting': int(np.max(valid_dates_clean)),\n",
    "        'median_planting': int(np.median(valid_dates_clean)),\n",
    "        'mean_planting': float(np.mean(valid_dates_clean)),\n",
    "        'std_planting': float(np.std(valid_dates_clean)),\n",
    "        'planting_window_days': int(np.max(valid_dates_clean) - np.min(valid_dates_clean))\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"\\n‚úÖ Planting Date Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "planting_stats = []\n",
    "\n",
    "# Season 1 (Nov-Mar)\n",
    "season1_stats = analyze_planting_dates(\n",
    "    flexible_results['all_seasons'][:, :, 0],\n",
    "    season_types[:, :, 0],\n",
    "    \"Season 1 (Nov-Mar)\"\n",
    ")\n",
    "if season1_stats:\n",
    "    planting_stats.append(season1_stats)\n",
    "    print(f\"\\n{season1_stats['season']}:\")\n",
    "    print(f\"  Pixels with planting: {season1_stats['pixel_count']:,}\")\n",
    "    print(f\"  Earliest planting: Day {season1_stats['earliest_planting']}\")\n",
    "    print(f\"  Latest planting: Day {season1_stats['latest_planting']}\")\n",
    "    print(f\"  Median planting: Day {season1_stats['median_planting']}\")\n",
    "    print(f\"  Planting window: {season1_stats['planting_window_days']} days\")\n",
    "    print(f\"  Variability (std): {season1_stats['std_planting']:.1f} days\")\n",
    "\n",
    "# Season 2 (Apr-Jun)\n",
    "season2_stats = analyze_planting_dates(\n",
    "    flexible_results['all_seasons'][:, :, 2],\n",
    "    season_types[:, :, 1],\n",
    "    \"Season 2 (Apr-Jun)\"\n",
    ")\n",
    "if season2_stats:\n",
    "    planting_stats.append(season2_stats)\n",
    "    print(f\"\\n{season2_stats['season']}:\")\n",
    "    print(f\"  Pixels with planting: {season2_stats['pixel_count']:,}\")\n",
    "    print(f\"  Earliest planting: Day {season2_stats['earliest_planting']}\")\n",
    "    print(f\"  Latest planting: Day {season2_stats['latest_planting']}\")\n",
    "    print(f\"  Median planting: Day {season2_stats['median_planting']}\")\n",
    "    print(f\"  Planting window: {season2_stats['planting_window_days']} days\")\n",
    "    print(f\"  Variability (std): {season2_stats['std_planting']:.1f} days\")\n",
    "\n",
    "# Season 3 (Jul-Sep)\n",
    "season3_stats = analyze_planting_dates(\n",
    "    flexible_results['all_seasons'][:, :, 4],\n",
    "    season_types[:, :, 2],\n",
    "    \"Season 3 (Jul-Sep)\"\n",
    ")\n",
    "if season3_stats:\n",
    "    planting_stats.append(season3_stats)\n",
    "    print(f\"\\n{season3_stats['season']}:\")\n",
    "    print(f\"  Pixels with planting: {season3_stats['pixel_count']:,}\")\n",
    "    print(f\"  Earliest planting: Day {season3_stats['earliest_planting']}\")\n",
    "    print(f\"  Latest planting: Day {season3_stats['latest_planting']}\")\n",
    "    print(f\"  Median planting: Day {season3_stats['median_planting']}\")\n",
    "    print(f\"  Planting window: {season3_stats['planting_window_days']} days\")\n",
    "    print(f\"  Variability (std): {season3_stats['std_planting']:.1f} days\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. AGRICULTURAL INTENSIFICATION INDEX\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_intensification_index(cropping_intensity, season_durations, peak_ndvi):\n",
    "    \"\"\"\n",
    "    Calculate agricultural intensification index (0-1)\n",
    "    \n",
    "    Combines:\n",
    "    - Cropping intensity (40% weight)\n",
    "    - Season duration efficiency (30% weight)\n",
    "    - Productivity (peak NDVI, 30% weight)\n",
    "    \"\"\"\n",
    "    # Normalize cropping intensity (max 3 seasons)\n",
    "    intensity_normalized = cropping_intensity / 3.0\n",
    "    \n",
    "    # Calculate average season duration (longer = more intensive)\n",
    "    avg_duration = np.nanmean(season_durations, axis=2)\n",
    "    duration_normalized = np.clip(avg_duration / 120.0, 0, 1)  # 120 days reference\n",
    "    \n",
    "    # Normalize peak NDVI (higher = more productive)\n",
    "    ndvi_normalized = np.clip(peak_ndvi / 0.9, 0, 1)  # 0.9 NDVI reference\n",
    "    \n",
    "    # Weighted combination\n",
    "    intensification = (\n",
    "        intensity_normalized * 0.4 +\n",
    "        duration_normalized * 0.3 +\n",
    "        ndvi_normalized * 0.3\n",
    "    )\n",
    "    \n",
    "    return intensification\n",
    "\n",
    "# Calculate season durations\n",
    "season_durations = np.zeros((*flexible_results['all_seasons'].shape[:2], 3))\n",
    "for i in range(3):\n",
    "    sos = flexible_results['all_seasons'][:, :, i*2]\n",
    "    eos = flexible_results['all_seasons'][:, :, i*2 + 1]\n",
    "    season_durations[:, :, i] = eos - sos\n",
    "\n",
    "# Get peak NDVI from fused data\n",
    "peak_ndvi = np.nanmax(fused_result['S2ndvi'].values, axis=0)\n",
    "\n",
    "intensification_index = calculate_intensification_index(\n",
    "    cropping_intensity,\n",
    "    season_durations,\n",
    "    peak_ndvi\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Agricultural Intensification Index Calculated\")\n",
    "print(f\"\\nIntensification Statistics:\")\n",
    "print(f\"  Mean intensification: {np.nanmean(intensification_index[cropping_intensity > 0]):.3f}\")\n",
    "print(f\"  High intensification (>0.7): {(intensification_index > 0.7).sum():,} pixels\")\n",
    "print(f\"  Medium intensification (0.4-0.7): {((intensification_index >= 0.4) & (intensification_index <= 0.7)).sum():,} pixels\")\n",
    "print(f\"  Low intensification (<0.4): {(intensification_index < 0.4).sum():,} pixels\")\n",
    "\n",
    "print(\"\\n‚úÖ All planting indices calculated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Visualize Planting Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è  VISUALIZING PLANTING INDICES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comprehensive planting indices visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Agricultural Planting Indices - Indonesia', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Planting Pattern Classification\n",
    "im1 = axes[0, 0].imshow(planting_patterns, cmap='tab10', vmin=0, vmax=7)\n",
    "axes[0, 0].set_title('üåæ Planting Pattern Classification', fontweight='bold')\n",
    "cbar1 = plt.colorbar(im1, ax=axes[0, 0])\n",
    "cbar1.set_label('Pattern Type')\n",
    "cbar1.set_ticks([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "cbar1.set_ticklabels(['Non-agri', 'Single\\n(Rain)', 'Double\\n(Irrig)', 'Triple\\n(Intens)', \n",
    "                      'Dry\\nFocus', 'Mid-yr', 'Late', 'Other'], fontsize=8)\n",
    "\n",
    "# 2. Irrigation Dependency Index\n",
    "im2 = axes[0, 1].imshow(irrigation_dependency, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0, 1].set_title('üíß Irrigation Dependency Index', fontweight='bold')\n",
    "cbar2 = plt.colorbar(im2, ax=axes[0, 1])\n",
    "cbar2.set_label('Irrigation Need (0=rain-fed, 1=full)')\n",
    "cbar2.set_ticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "\n",
    "# 3. Agricultural Intensification Index\n",
    "# Mask non-agricultural areas\n",
    "intensification_masked = np.where(cropping_intensity > 0, intensification_index, np.nan)\n",
    "im3 = axes[0, 2].imshow(intensification_masked, cmap='YlGnBu', vmin=0, vmax=1)\n",
    "axes[0, 2].set_title('üìà Agricultural Intensification Index', fontweight='bold')\n",
    "cbar3 = plt.colorbar(im3, ax=axes[0, 2])\n",
    "cbar3.set_label('Intensification (0=low, 1=high)')\n",
    "cbar3.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "# 4. Season 1 Planting Date Distribution (Nov-Mar)\n",
    "season1_sos = flexible_results['all_seasons'][:, :, 0]\n",
    "season1_sos_masked = np.where(season_types[:, :, 0] == 1, season1_sos, np.nan)\n",
    "\n",
    "# Handle year boundary for visualization\n",
    "season1_sos_viz = season1_sos_masked.copy()\n",
    "for y in range(season1_sos_viz.shape[0]):\n",
    "    for x in range(season1_sos_viz.shape[1]):\n",
    "        if not np.isnan(season1_sos_viz[y, x]):\n",
    "            day = season1_sos_viz[y, x]\n",
    "            if day >= 305:  # Nov-Dec\n",
    "                season1_sos_viz[y, x] = day - 305  # Nov 1 = 0\n",
    "            elif day <= 90:  # Jan-Mar\n",
    "                season1_sos_viz[y, x] = day + 60  # After Dec 31\n",
    "\n",
    "im4 = axes[1, 0].imshow(season1_sos_viz, cmap='RdYlGn', vmin=0, vmax=150)\n",
    "axes[1, 0].set_title('üìÖ Season 1 Planting Dates (Nov-Mar)', fontweight='bold')\n",
    "cbar4 = plt.colorbar(im4, ax=axes[1, 0])\n",
    "cbar4.set_label('Planting Time')\n",
    "cbar4.set_ticks([0, 30, 60, 90, 120, 150])\n",
    "cbar4.set_ticklabels(['Early Nov', 'Early Dec', 'Early Jan', 'Early Feb', 'Early Mar', 'Late Mar'], fontsize=8)\n",
    "\n",
    "# 5. Season 2 Planting Date Distribution (Apr-Jun)\n",
    "season2_sos = flexible_results['all_seasons'][:, :, 2]\n",
    "season2_sos_masked = np.where(season_types[:, :, 1] == 1, season2_sos, np.nan)\n",
    "\n",
    "im5 = axes[1, 1].imshow(season2_sos_masked, cmap='viridis', vmin=90, vmax=180)\n",
    "axes[1, 1].set_title('üìÖ Season 2 Planting Dates (Apr-Jun)', fontweight='bold')\n",
    "cbar5 = plt.colorbar(im5, ax=axes[1, 1])\n",
    "cbar5.set_label('Day of Year')\n",
    "cbar5.set_ticks([90, 105, 120, 135, 150, 165, 180])\n",
    "cbar5.set_ticklabels(['Apr 1', 'Apr 15', 'May 1', 'May 15', 'Jun 1', 'Jun 15', 'Jun 30'], fontsize=8)\n",
    "\n",
    "# 6. Cropping Intensity with Irrigation Overlay\n",
    "im6 = axes[1, 2].imshow(cropping_intensity, cmap='RdYlGn', vmin=0, vmax=3, alpha=0.7)\n",
    "axes[1, 2].set_title('üåæ Cropping Intensity + Irrigation', fontweight='bold')\n",
    "\n",
    "# Overlay irrigation areas with contours\n",
    "high_irrigation = irrigation_dependency > 0.5\n",
    "axes[1, 2].contour(high_irrigation, levels=[0.5], colors='blue', linewidths=2, alpha=0.5)\n",
    "\n",
    "cbar6 = plt.colorbar(im6, ax=axes[1, 2])\n",
    "cbar6.set_label('Seasons per Year')\n",
    "cbar6.set_ticks([0, 1, 2, 3])\n",
    "cbar6.set_ticklabels(['None', '1 Season', '2 Seasons', '3 Seasons'])\n",
    "\n",
    "# Add coordinates to all subplots\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('X coordinate (pixel)')\n",
    "    ax.set_ylabel('Y coordinate (pixel)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('planting_indices_comprehensive.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Planting indices visualization saved to: planting_indices_comprehensive.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create planting date histograms\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Planting Date Distributions by Season', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Season 1 histogram\n",
    "if season1_stats:\n",
    "    season1_dates = flexible_results['all_seasons'][:, :, 0][season_types[:, :, 0] == 1]\n",
    "    season1_dates_clean = season1_dates[~np.isnan(season1_dates)]\n",
    "    \n",
    "    axes[0].hist(season1_dates_clean, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(season1_stats['median_planting'], color='red', linestyle='--', \n",
    "                    linewidth=2, label=f\"Median: Day {season1_stats['median_planting']}\")\n",
    "    axes[0].set_xlabel('Day of Year')\n",
    "    axes[0].set_ylabel('Number of Pixels')\n",
    "    axes[0].set_title('Season 1: Nov-Mar Planting Dates')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Season 2 histogram\n",
    "if season2_stats:\n",
    "    season2_dates = flexible_results['all_seasons'][:, :, 2][season_types[:, :, 1] == 1]\n",
    "    season2_dates_clean = season2_dates[~np.isnan(season2_dates)]\n",
    "    \n",
    "    axes[1].hist(season2_dates_clean, bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "    axes[1].axvline(season2_stats['median_planting'], color='red', linestyle='--',\n",
    "                    linewidth=2, label=f\"Median: Day {season2_stats['median_planting']}\")\n",
    "    axes[1].set_xlabel('Day of Year')\n",
    "    axes[1].set_ylabel('Number of Pixels')\n",
    "    axes[1].set_title('Season 2: Apr-Jun Planting Dates')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Season 3 histogram\n",
    "if season3_stats:\n",
    "    season3_dates = flexible_results['all_seasons'][:, :, 4][season_types[:, :, 2] == 1]\n",
    "    season3_dates_clean = season3_dates[~np.isnan(season3_dates)]\n",
    "    \n",
    "    axes[2].hist(season3_dates_clean, bins=30, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[2].axvline(season3_stats['median_planting'], color='red', linestyle='--',\n",
    "                    linewidth=2, label=f\"Median: Day {season3_stats['median_planting']}\")\n",
    "    axes[2].set_xlabel('Day of Year')\n",
    "    axes[2].set_ylabel('Number of Pixels')\n",
    "    axes[2].set_title('Season 3: Jul-Sep Planting Dates')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('planting_date_histograms.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Planting date histograms saved to: planting_date_histograms.png\")\n",
    "\n",
    "print(\"\\n‚úÖ All planting indices visualized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Export Planting Indices and Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ EXPORTING PLANTING INDICES AND GENERATING REPORTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CREATE COMPREHENSIVE PLANTING INDICES DATASET\n",
    "# ============================================================================\n",
    "\n",
    "planting_indices_dataset = xr.Dataset({\n",
    "    # Planting pattern classification\n",
    "    'planting_pattern': xr.DataArray(\n",
    "        planting_patterns,\n",
    "        dims=['y', 'x'],\n",
    "        coords={'y': y_coords, 'x': x_coords},\n",
    "        attrs={\n",
    "            'long_name': 'Agricultural Planting Pattern Classification',\n",
    "            'description': 'Classification of planting patterns based on seasonal presence',\n",
    "            'units': 'pattern code',\n",
    "            'pattern_codes': {\n",
    "                0: 'Non-agricultural',\n",
    "                1: 'Single season (rain-fed)',\n",
    "                2: 'Double season (irrigated)',\n",
    "                3: 'Triple season (intensive)',\n",
    "                4: 'Dry season focus',\n",
    "                5: 'Mid-year only',\n",
    "                6: 'Late season only',\n",
    "                7: 'Other patterns'\n",
    "            }\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # Irrigation dependency index\n",
    "    'irrigation_dependency': xr.DataArray(\n",
    "        irrigation_dependency,\n",
    "        dims=['y', 'x'],\n",
    "        coords={'y': y_coords, 'x': x_coords},\n",
    "        attrs={\n",
    "            'long_name': 'Irrigation Dependency Index',\n",
    "            'description': 'Level of irrigation dependency (0=rain-fed, 0.5=partial, 1.0=full)',\n",
    "            'units': 'index (0-1)',\n",
    "            'interpretation': '0=rain-fed, 0.5=partial irrigation, 1.0=full irrigation'\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # Agricultural intensification index\n",
    "    'intensification_index': xr.DataArray(\n",
    "        intensification_index,\n",
    "        dims=['y', 'x'],\n",
    "        coords={'y': y_coords, 'x': x_coords},\n",
    "        attrs={\n",
    "            'long_name': 'Agricultural Intensification Index',\n",
    "            'description': 'Composite index combining cropping intensity, season duration, and productivity',\n",
    "            'units': 'index (0-1)',\n",
    "            'components': 'Cropping intensity (40%), Season duration (30%), Peak NDVI (30%)'\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # Cropping intensity (reference)\n",
    "    'cropping_intensity': xr.DataArray(\n",
    "        cropping_intensity,\n",
    "        dims=['y', 'x'],\n",
    "        coords={'y': y_coords, 'x': x_coords},\n",
    "        attrs={\n",
    "            'long_name': 'Cropping Intensity',\n",
    "            'description': 'Number of cropping seasons per year',\n",
    "            'units': 'seasons per year',\n",
    "            'valid_range': [0, 3]\n",
    "        }\n",
    "    )\n",
    "})\n",
    "\n",
    "# Add global attributes\n",
    "planting_indices_dataset.attrs.update({\n",
    "    'title': 'Agricultural Planting Indices for Indonesia',\n",
    "    'description': 'Comprehensive planting indices derived from MOGPR S1+S2 fusion and multi-season analysis',\n",
    "    'methodology': 'Multi-sensor satellite data fusion with flexible season detection',\n",
    "    'spatial_coverage': f'{len(y_coords)} x {len(x_coords)} pixels',\n",
    "    'processing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'software': 'FuseTS with MOGPR algorithm',\n",
    "    'data_source': 'Sentinel-1 VV/VH + Sentinel-2 NDVI',\n",
    "    'country': 'Indonesia',\n",
    "    'agricultural_calendar': 'Season 1 (Nov-Mar), Season 2 (Apr-Jun), Season 3 (Jul-Sep)'\n",
    "})\n",
    "\n",
    "# Save dataset\n",
    "planting_indices_file = \"planting_indices_indonesia.nc\"\n",
    "planting_indices_dataset.to_netcdf(planting_indices_file)\n",
    "print(f\"‚úÖ Planting indices dataset saved to: {planting_indices_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CREATE DETAILED STATISTICS REPORT\n",
    "# ============================================================================\n",
    "\n",
    "planting_indices_stats = {\n",
    "    'summary': {\n",
    "        'total_pixels': int(planting_patterns.size),\n",
    "        'agricultural_pixels': int((planting_patterns > 0).sum()),\n",
    "        'agricultural_percentage': float((planting_patterns > 0).sum() / planting_patterns.size * 100)\n",
    "    },\n",
    "    \n",
    "    'planting_patterns': {},\n",
    "    \n",
    "    'irrigation_dependency': {\n",
    "        'rain_fed_pixels': int((irrigation_dependency == 0.0).sum()),\n",
    "        'partial_irrigation_pixels': int((irrigation_dependency == 0.5).sum()),\n",
    "        'full_irrigation_pixels': int((irrigation_dependency == 1.0).sum()),\n",
    "        'rain_fed_percentage': float((irrigation_dependency == 0.0).sum() / (irrigation_dependency >= 0).sum() * 100),\n",
    "        'irrigated_percentage': float((irrigation_dependency > 0).sum() / (irrigation_dependency >= 0).sum() * 100)\n",
    "    },\n",
    "    \n",
    "    'intensification': {\n",
    "        'mean_index': float(np.nanmean(intensification_index[cropping_intensity > 0])),\n",
    "        'high_intensification_pixels': int((intensification_index > 0.7).sum()),\n",
    "        'medium_intensification_pixels': int(((intensification_index >= 0.4) & (intensification_index <= 0.7)).sum()),\n",
    "        'low_intensification_pixels': int((intensification_index < 0.4).sum())\n",
    "    },\n",
    "    \n",
    "    'planting_dates': {}\n",
    "}\n",
    "\n",
    "# Add pattern statistics\n",
    "for pattern_code, pattern_name in pattern_names.items():\n",
    "    pixel_count = int((planting_patterns == pattern_code).sum())\n",
    "    if pixel_count > 0:\n",
    "        planting_indices_stats['planting_patterns'][pattern_name] = {\n",
    "            'pixel_count': pixel_count,\n",
    "            'percentage': float(pixel_count / planting_patterns.size * 100)\n",
    "        }\n",
    "\n",
    "# Add planting date statistics\n",
    "for stats in planting_stats:\n",
    "    planting_indices_stats['planting_dates'][stats['season']] = {\n",
    "        'pixel_count': stats['pixel_count'],\n",
    "        'earliest_planting_doy': stats['earliest_planting'],\n",
    "        'latest_planting_doy': stats['latest_planting'],\n",
    "        'median_planting_doy': stats['median_planting'],\n",
    "        'mean_planting_doy': stats['mean_planting'],\n",
    "        'planting_variability_days': stats['std_planting'],\n",
    "        'planting_window_days': stats['planting_window_days']\n",
    "    }\n",
    "\n",
    "# Save statistics\n",
    "stats_file = \"planting_indices_statistics.json\"\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(planting_indices_stats, f, indent=2)\n",
    "print(f\"‚úÖ Planting indices statistics saved to: {stats_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CREATE CSV EXPORT FOR GIS/ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìä Creating detailed CSV for GIS integration...\")\n",
    "\n",
    "# Extract per-pixel data (sample every 3rd pixel for manageable file size)\n",
    "planting_indices_csv_data = []\n",
    "\n",
    "for i in range(0, len(y_coords), 3):\n",
    "    for j in range(0, len(x_coords), 3):\n",
    "        if planting_patterns[i, j] > 0:  # Only agricultural pixels\n",
    "            row_data = {\n",
    "                'pixel_y': i,\n",
    "                'pixel_x': j,\n",
    "                'latitude': float(y_coords[i]),\n",
    "                'longitude': float(x_coords[j]),\n",
    "                'planting_pattern': int(planting_patterns[i, j]),\n",
    "                'pattern_name': pattern_names[int(planting_patterns[i, j])],\n",
    "                'cropping_intensity': int(cropping_intensity[i, j]),\n",
    "                'irrigation_dependency': float(irrigation_dependency[i, j]),\n",
    "                'intensification_index': float(intensification_index[i, j]),\n",
    "                'season1_present': int(season_types[i, j, 0]),\n",
    "                'season2_present': int(season_types[i, j, 1]),\n",
    "                'season3_present': int(season_types[i, j, 2])\n",
    "            }\n",
    "            \n",
    "            # Add planting dates if available\n",
    "            if season_types[i, j, 0] == 1:\n",
    "                row_data['season1_planting_doy'] = flexible_results['all_seasons'][i, j, 0]\n",
    "            if season_types[i, j, 1] == 1:\n",
    "                row_data['season2_planting_doy'] = flexible_results['all_seasons'][i, j, 2]\n",
    "            if season_types[i, j, 2] == 1:\n",
    "                row_data['season3_planting_doy'] = flexible_results['all_seasons'][i, j, 4]\n",
    "            \n",
    "            planting_indices_csv_data.append(row_data)\n",
    "\n",
    "planting_indices_df = pd.DataFrame(planting_indices_csv_data)\n",
    "csv_file = \"planting_indices_detailed.csv\"\n",
    "planting_indices_df.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Detailed planting indices CSV saved to: {csv_file}\")\n",
    "print(f\"   üìä Contains {len(planting_indices_df)} sample agricultural pixels\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. GENERATE SUMMARY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã PLANTING INDICES SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüåæ PLANTING PATTERN DISTRIBUTION:\")\n",
    "for pattern_name, stats in planting_indices_stats['planting_patterns'].items():\n",
    "    print(f\"  {pattern_name}: {stats['pixel_count']:,} pixels ({stats['percentage']:.2f}%)\")\n",
    "\n",
    "print(\"\\nüíß IRRIGATION DEPENDENCY:\")\n",
    "print(f\"  Rain-fed agriculture: {planting_indices_stats['irrigation_dependency']['rain_fed_percentage']:.1f}%\")\n",
    "print(f\"  Irrigated agriculture: {planting_indices_stats['irrigation_dependency']['irrigated_percentage']:.1f}%\")\n",
    "\n",
    "print(\"\\nüìà AGRICULTURAL INTENSIFICATION:\")\n",
    "print(f\"  Mean intensification index: {planting_indices_stats['intensification']['mean_index']:.3f}\")\n",
    "print(f\"  High intensification areas: {planting_indices_stats['intensification']['high_intensification_pixels']:,} pixels\")\n",
    "\n",
    "print(\"\\nüìÖ PLANTING DATE ANALYSIS:\")\n",
    "for season_name, date_stats in planting_indices_stats['planting_dates'].items():\n",
    "    print(f\"\\n  {season_name}:\")\n",
    "    print(f\"    Planted pixels: {date_stats['pixel_count']:,}\")\n",
    "    print(f\"    Median planting: Day {date_stats['median_planting_doy']}\")\n",
    "    print(f\"    Planting window: {date_stats['planting_window_days']} days\")\n",
    "    print(f\"    Variability: ¬±{date_stats['planting_variability_days']:.1f} days\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÅ EXPORTED FILES:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  1. {planting_indices_file} - NetCDF dataset (all indices)\")\n",
    "print(f\"  2. {stats_file} - JSON statistics\")\n",
    "print(f\"  3. {csv_file} - Detailed CSV for GIS\")\n",
    "print(f\"  4. planting_indices_comprehensive.png - Index maps\")\n",
    "print(f\"  5. planting_date_histograms.png - Temporal distributions\")\n",
    "\n",
    "print(\"\\nüöÄ APPLICATIONS:\")\n",
    "print(\"  ‚Ä¢ Agricultural extension services - Target interventions\")\n",
    "print(\"  ‚Ä¢ Irrigation planning - Identify high-need areas\")\n",
    "print(\"  ‚Ä¢ Crop insurance - Risk assessment and premium calculation\")\n",
    "print(\"  ‚Ä¢ Food security monitoring - Track intensification trends\")\n",
    "print(\"  ‚Ä¢ Policy making - Evidence-based subsidy allocation\")\n",
    "print(\"  ‚Ä¢ Climate adaptation - Understand planting shifts\")\n",
    "\n",
    "print(\"\\n‚úÖ Planting indices analysis complete and ready for use!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ GPU-Accelerated MOGPR (Optional - 10-100x faster!)\n",
    "\n",
    "If you have a GPU (NVIDIA CUDA or Apple Silicon), you can use the GPU-accelerated version for massive speedup!\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "# Install PyTorch with GPU support\n",
    "# For NVIDIA GPU:\n",
    "pip install torch gpytorch\n",
    "\n",
    "# For Apple Silicon (M1/M2/M3):\n",
    "pip install torch gpytorch\n",
    "\n",
    "# Verify GPU:\n",
    "python -c \"import torch; print(f'GPU: {torch.cuda.is_available() or torch.backends.mps.is_available()}')\"\n",
    "```\n",
    "\n",
    "**Expected Performance:**\n",
    "- **CPU (current)**: 50√ó50 pixels = 11 minutes ‚Üí Full dataset = 21 hours\n",
    "- **GPU**: 50√ó50 pixels = ~1 minute ‚Üí Full dataset = ~2 hours\n",
    "- **Speedup**: 10-100x depending on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Use GPU-accelerated MOGPR (requires torch + gpytorch)\n",
    "# This can provide 10-100x speedup!\n",
    "\n",
    "USE_GPU = False  # Set to True if you have GPU and installed torch + gpytorch\n",
    "\n",
    "if USE_GPU:\n",
    "    try:\n",
    "        import torch\n",
    "        import gpytorch\n",
    "        from fusets.mogpr_gpu import MOGPRTransformerGPU, mogpr_gpu\n",
    "        \n",
    "        # Check GPU availability\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üöÄ NVIDIA GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            print(\"üöÄ Apple Silicon GPU (MPS) detected\")\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No GPU detected, falling back to CPU\")\n",
    "            device = torch.device(\"cpu\")\n",
    "        \n",
    "        print(f\"‚úÖ GPU-accelerated MOGPR ready on {device}\")\n",
    "        print(\"Expected speedup: 10-100x faster than CPU version!\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå GPU libraries not installed: {e}\")\n",
    "        print(\"Install with: pip install torch gpytorch\")\n",
    "        USE_GPU = False\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Using CPU version (slower but works everywhere)\")\n",
    "    print(\"To enable GPU: Set USE_GPU = True and install torch + gpytorch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
