{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-1 and Sentinel-2 Data Fusion using Deep Learning\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load Sentinel-1 (SAR) and Sentinel-2 (optical) time series data\n",
    "2. Apply MOGPR (Multi-Output Gaussian Process Regression) fusion\n",
    "3. Extract phenological metrics from fused data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FuseTS imports\n",
    "from fusets.mogpr import MOGPRTransformer\n",
    "from fusets.analytics import phenology\n",
    "from fusets import whittaker\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition and Loading from MPC\n",
    "\n",
    "For this tutorial, we'll create synthetic S1 and S2 time series data. In practice, you would load your actual GeoTIFF stacks or data from other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Steps\n",
    "\n",
    "‚ö†Ô∏è **STOP! MPC data doesn't exist yet?** Follow these steps first:\n",
    "\n",
    "### üìã Prerequisites: Download Data from MPC First\n",
    "\n",
    "**Before running this notebook, you must:**\n",
    "\n",
    "1. **Open** `MPC_Data_Prep_Fixed.ipynb` \n",
    "2. **Run all cells** to download S1/S2 data from Microsoft Planetary Computer\n",
    "3. **Wait for download to complete**\n",
    "4. **Verify NetCDF files exist** in: `./mpc_data/` directory\n",
    "\n",
    "### üîÑ Data Download Status Check\n",
    "\n",
    "Check that you have NetCDF files matching the pattern:\n",
    "- `mpc_data/S1_S2_Demak_*.nc`\n",
    "\n",
    "If files are missing, return to `MPC_Data_Prep_Fixed.ipynb` and complete the download process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LOAD MPC ASSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MPC data from mpc_data/*.nc...\n",
      "Found 1 NetCDF file(s)\n",
      "Loaded: klambu_glapan_2024-11-01_2025-11-07_final.nc\n",
      "\n",
      "Dataset structure:\n",
      "  Dimensions: {'t': 28, 'y': 4420, 'x': 5800}\n",
      "  Variables: ['VV', 'VH', 'S2ndvi', 'spatial_ref']\n",
      "  Time range: 2024-11-07T00:00:00.000000000 to 2025-10-21T00:00:00.000000000\n",
      "  Spatial shape: 4420 x 5800\n",
      "\n",
      "======================================================================\n",
      "MPC Dataset loaded successfully!\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in notebooks */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(\n",
       "    --jp-content-font-color0,\n",
       "    var(--pst-color-text-base rgba(0, 0, 0, 1))\n",
       "  );\n",
       "  --xr-font-color2: var(\n",
       "    --jp-content-font-color2,\n",
       "    var(--pst-color-text-base, rgba(0, 0, 0, 0.54))\n",
       "  );\n",
       "  --xr-font-color3: var(\n",
       "    --jp-content-font-color3,\n",
       "    var(--pst-color-text-base, rgba(0, 0, 0, 0.38))\n",
       "  );\n",
       "  --xr-border-color: var(\n",
       "    --jp-border-color2,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 10))\n",
       "  );\n",
       "  --xr-disabled-color: var(\n",
       "    --jp-layout-color3,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 40))\n",
       "  );\n",
       "  --xr-background-color: var(\n",
       "    --jp-layout-color0,\n",
       "    var(--pst-color-on-background, white)\n",
       "  );\n",
       "  --xr-background-color-row-even: var(\n",
       "    --jp-layout-color1,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 5))\n",
       "  );\n",
       "  --xr-background-color-row-odd: var(\n",
       "    --jp-layout-color2,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 15))\n",
       "  );\n",
       "}\n",
       "\n",
       "html[theme=\"dark\"],\n",
       "html[data-theme=\"dark\"],\n",
       "body[data-theme=\"dark\"],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: var(\n",
       "    --jp-content-font-color0,\n",
       "    var(--pst-color-text-base, rgba(255, 255, 255, 1))\n",
       "  );\n",
       "  --xr-font-color2: var(\n",
       "    --jp-content-font-color2,\n",
       "    var(--pst-color-text-base, rgba(255, 255, 255, 0.54))\n",
       "  );\n",
       "  --xr-font-color3: var(\n",
       "    --jp-content-font-color3,\n",
       "    var(--pst-color-text-base, rgba(255, 255, 255, 0.38))\n",
       "  );\n",
       "  --xr-border-color: var(\n",
       "    --jp-border-color2,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 10))\n",
       "  );\n",
       "  --xr-disabled-color: var(\n",
       "    --jp-layout-color3,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 40))\n",
       "  );\n",
       "  --xr-background-color: var(\n",
       "    --jp-layout-color0,\n",
       "    var(--pst-color-on-background, #111111)\n",
       "  );\n",
       "  --xr-background-color-row-even: var(\n",
       "    --jp-layout-color1,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 5))\n",
       "  );\n",
       "  --xr-background-color-row-odd: var(\n",
       "    --jp-layout-color2,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 15))\n",
       "  );\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "  line-height: 1.6;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-obj-name,\n",
       ".xr-group-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-group-name::before {\n",
       "  content: \"üìÅ\";\n",
       "  padding-right: 0.3em;\n",
       "}\n",
       "\n",
       ".xr-group-name,\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
       "  margin-block-start: 0;\n",
       "  margin-block-end: 0;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: inline-block;\n",
       "  opacity: 0;\n",
       "  height: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "  border: 2px solid transparent !important;\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:focus + label {\n",
       "  border: 2px solid var(--xr-font-color0) !important;\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: \"‚ñ∫\";\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: \"‚ñº\";\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-top: 4px;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-group-box {\n",
       "  display: inline-grid;\n",
       "  grid-template-columns: 0px 20px auto;\n",
       "  width: 100%;\n",
       "}\n",
       "\n",
       ".xr-group-box-vline {\n",
       "  grid-column-start: 1;\n",
       "  border-right: 0.2em solid;\n",
       "  border-color: var(--xr-border-color);\n",
       "  width: 0px;\n",
       "}\n",
       "\n",
       ".xr-group-box-hline {\n",
       "  grid-column-start: 2;\n",
       "  grid-row-start: 1;\n",
       "  height: 1em;\n",
       "  width: 20px;\n",
       "  border-bottom: 0.2em solid;\n",
       "  border-color: var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-group-box-contents {\n",
       "  grid-column-start: 3;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: \"(\";\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: \")\";\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: \",\";\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  border-color: var(--xr-background-color-row-odd);\n",
       "  margin-bottom: 0;\n",
       "  padding-top: 2px;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "  border-color: var(--xr-background-color-row-even);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  border-top: 2px dotted var(--xr-background-color);\n",
       "  padding-bottom: 20px !important;\n",
       "  padding-top: 10px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in + label,\n",
       ".xr-var-data-in + label,\n",
       ".xr-index-data-in + label {\n",
       "  padding: 0 1px;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-data > pre,\n",
       ".xr-index-data > pre,\n",
       ".xr-var-data > table > tbody > tr {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked + label > .xr-icon-file-text2,\n",
       ".xr-var-data-in:checked + label > .xr-icon-database,\n",
       ".xr-index-data-in:checked + label > .xr-icon-database {\n",
       "  color: var(--xr-font-color0);\n",
       "  filter: drop-shadow(1px 1px 5px var(--xr-font-color2));\n",
       "  stroke-width: 0.8px;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 9GB\n",
       "Dimensions:      (t: 28, y: 4420, x: 5800)\n",
       "Coordinates:\n",
       "  * t            (t) datetime64[ns] 224B 2024-11-07 2024-11-19 ... 2025-10-21\n",
       "  * y            (y) float64 35kB 9.214e+06 9.214e+06 ... 9.258e+06 9.258e+06\n",
       "  * x            (x) float64 46kB 4.462e+05 4.462e+05 ... 5.042e+05 5.042e+05\n",
       "    band         int64 8B ...\n",
       "Data variables:\n",
       "    VV           (t, y, x) float32 3GB ...\n",
       "    VH           (t, y, x) float32 3GB ...\n",
       "    S2ndvi       (t, y, x) float32 3GB ...\n",
       "    spatial_ref  int64 8B ...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-44d057d2-c369-4688-963b-3752e9d0cd2c' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-44d057d2-c369-4688-963b-3752e9d0cd2c' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>t</span>: 28</li><li><span class='xr-has-index'>y</span>: 4420</li><li><span class='xr-has-index'>x</span>: 5800</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-86a5f674-89c2-447f-9b54-39a2e5a407f1' class='xr-section-summary-in' type='checkbox'  checked><label for='section-86a5f674-89c2-447f-9b54-39a2e5a407f1' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>t</span></div><div class='xr-var-dims'>(t)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2024-11-07 ... 2025-10-21</div><input id='attrs-ab1f82bc-cd58-4e7f-98a5-0fd4bc0fc01a' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-ab1f82bc-cd58-4e7f-98a5-0fd4bc0fc01a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ab7f9f1f-c0f2-4c6c-a3ed-c8982d845469' class='xr-var-data-in' type='checkbox'><label for='data-ab7f9f1f-c0f2-4c6c-a3ed-c8982d845469' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;2024-11-07T00:00:00.000000000&#x27;, &#x27;2024-11-19T00:00:00.000000000&#x27;,\n",
       "       &#x27;2024-12-13T00:00:00.000000000&#x27;, &#x27;2024-12-25T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-01-06T00:00:00.000000000&#x27;, &#x27;2025-01-18T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-01-30T00:00:00.000000000&#x27;, &#x27;2025-02-11T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-02-23T00:00:00.000000000&#x27;, &#x27;2025-03-07T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-03-19T00:00:00.000000000&#x27;, &#x27;2025-03-31T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-04-12T00:00:00.000000000&#x27;, &#x27;2025-04-24T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-05-06T00:00:00.000000000&#x27;, &#x27;2025-05-18T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-06-11T00:00:00.000000000&#x27;, &#x27;2025-06-23T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-07-05T00:00:00.000000000&#x27;, &#x27;2025-07-17T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-07-29T00:00:00.000000000&#x27;, &#x27;2025-08-10T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-08-22T00:00:00.000000000&#x27;, &#x27;2025-09-03T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-09-15T00:00:00.000000000&#x27;, &#x27;2025-09-27T00:00:00.000000000&#x27;,\n",
       "       &#x27;2025-10-09T00:00:00.000000000&#x27;, &#x27;2025-10-21T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>9.214e+06 9.214e+06 ... 9.258e+06</div><input id='attrs-2271db0e-e645-4137-a8a7-6e3e6fcf984c' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-2271db0e-e645-4137-a8a7-6e3e6fcf984c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ff67611d-df09-4627-9da1-b630e6d90977' class='xr-var-data-in' type='checkbox'><label for='data-ff67611d-df09-4627-9da1-b630e6d90977' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>axis :</span></dt><dd>Y</dd><dt><span>long_name :</span></dt><dd>y coordinate of projection</dd><dt><span>standard_name :</span></dt><dd>projection_y_coordinate</dd><dt><span>units :</span></dt><dd>metre</dd></dl></div><div class='xr-var-data'><pre>array([9213755., 9213765., 9213775., ..., 9257925., 9257935., 9257945.],\n",
       "      shape=(4420,))</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>4.462e+05 4.462e+05 ... 5.042e+05</div><input id='attrs-ff2750d0-e077-4259-a07a-0a948cf00447' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-ff2750d0-e077-4259-a07a-0a948cf00447' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-169e836d-4a03-45dd-8e2e-86c754b8f6fc' class='xr-var-data-in' type='checkbox'><label for='data-169e836d-4a03-45dd-8e2e-86c754b8f6fc' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>axis :</span></dt><dd>X</dd><dt><span>long_name :</span></dt><dd>x coordinate of projection</dd><dt><span>standard_name :</span></dt><dd>projection_x_coordinate</dd><dt><span>units :</span></dt><dd>metre</dd></dl></div><div class='xr-var-data'><pre>array([446225., 446235., 446245., ..., 504195., 504205., 504215.],\n",
       "      shape=(5800,))</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>band</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-9d2174e5-e7e8-46d4-b18e-4f7704f1dac5' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-9d2174e5-e7e8-46d4-b18e-4f7704f1dac5' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-eed7e6b3-3de2-4de7-a0af-0309be0b7bcf' class='xr-var-data-in' type='checkbox'><label for='data-eed7e6b3-3de2-4de7-a0af-0309be0b7bcf' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=int64]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-501c7816-8686-432d-bd49-281d345d88b6' class='xr-section-summary-in' type='checkbox'  checked><label for='section-501c7816-8686-432d-bd49-281d345d88b6' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>VV</span></div><div class='xr-var-dims'>(t, y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-6869f9b4-0a10-43b5-9695-49f9bf163a05' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-6869f9b4-0a10-43b5-9695-49f9bf163a05' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ed1d38ff-4edd-41fa-b64f-cbf0aeeb3118' class='xr-var-data-in' type='checkbox'><label for='data-ed1d38ff-4edd-41fa-b64f-cbf0aeeb3118' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>grid_mapping :</span></dt><dd>spatial_ref</dd></dl></div><div class='xr-var-data'><pre>[717808000 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>VH</span></div><div class='xr-var-dims'>(t, y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-3e44daad-853e-46a9-82f3-7aa636d5d9e7' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-3e44daad-853e-46a9-82f3-7aa636d5d9e7' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-db1cb389-5e2d-40e5-8e3b-4c364aab0a95' class='xr-var-data-in' type='checkbox'><label for='data-db1cb389-5e2d-40e5-8e3b-4c364aab0a95' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>grid_mapping :</span></dt><dd>spatial_ref</dd></dl></div><div class='xr-var-data'><pre>[717808000 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>S2ndvi</span></div><div class='xr-var-dims'>(t, y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-53ed05c5-3020-4395-b710-e4e185a56315' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-53ed05c5-3020-4395-b710-e4e185a56315' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-dab2f156-08e4-4cff-b15e-2f2f36df6ce0' class='xr-var-data-in' type='checkbox'><label for='data-dab2f156-08e4-4cff-b15e-2f2f36df6ce0' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>grid_mapping :</span></dt><dd>spatial_ref</dd></dl></div><div class='xr-var-data'><pre>[717808000 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-3054e412-ba95-4259-8f34-9988cf1a615f' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-3054e412-ba95-4259-8f34-9988cf1a615f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-af500157-623e-4dc2-a446-f9af1d62f7cc' class='xr-var-data-in' type='checkbox'><label for='data-af500157-623e-4dc2-a446-f9af1d62f7cc' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>crs_wkt :</span></dt><dd>PROJCS[&quot;WGS 84 / UTM zone 49S&quot;,GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]],PROJECTION[&quot;Transverse_Mercator&quot;],PARAMETER[&quot;latitude_of_origin&quot;,0],PARAMETER[&quot;central_meridian&quot;,111],PARAMETER[&quot;scale_factor&quot;,0.9996],PARAMETER[&quot;false_easting&quot;,500000],PARAMETER[&quot;false_northing&quot;,10000000],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,EAST],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;32749&quot;]]</dd><dt><span>semi_major_axis :</span></dt><dd>6378137.0</dd><dt><span>semi_minor_axis :</span></dt><dd>6356752.314245179</dd><dt><span>inverse_flattening :</span></dt><dd>298.257223563</dd><dt><span>reference_ellipsoid_name :</span></dt><dd>WGS 84</dd><dt><span>longitude_of_prime_meridian :</span></dt><dd>0.0</dd><dt><span>prime_meridian_name :</span></dt><dd>Greenwich</dd><dt><span>geographic_crs_name :</span></dt><dd>WGS 84</dd><dt><span>horizontal_datum_name :</span></dt><dd>World Geodetic System 1984</dd><dt><span>projected_crs_name :</span></dt><dd>WGS 84 / UTM zone 49S</dd><dt><span>grid_mapping_name :</span></dt><dd>transverse_mercator</dd><dt><span>latitude_of_projection_origin :</span></dt><dd>0.0</dd><dt><span>longitude_of_central_meridian :</span></dt><dd>111.0</dd><dt><span>false_easting :</span></dt><dd>500000.0</dd><dt><span>false_northing :</span></dt><dd>10000000.0</dd><dt><span>scale_factor_at_central_meridian :</span></dt><dd>0.9996</dd><dt><span>spatial_ref :</span></dt><dd>PROJCS[&quot;WGS 84 / UTM zone 49S&quot;,GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]],PROJECTION[&quot;Transverse_Mercator&quot;],PARAMETER[&quot;latitude_of_origin&quot;,0],PARAMETER[&quot;central_meridian&quot;,111],PARAMETER[&quot;scale_factor&quot;,0.9996],PARAMETER[&quot;false_easting&quot;,500000],PARAMETER[&quot;false_northing&quot;,10000000],UNIT[&quot;metre&quot;,1,AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]],AXIS[&quot;Easting&quot;,EAST],AXIS[&quot;Northing&quot;,NORTH],AUTHORITY[&quot;EPSG&quot;,&quot;32749&quot;]]</dd><dt><span>GeoTransform :</span></dt><dd>446220.0 10.0 0.0 9213750.0 0.0 10.0</dd></dl></div><div class='xr-var-data'><pre>[1 values with dtype=int64]</pre></div></li></ul></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 9GB\n",
       "Dimensions:      (t: 28, y: 4420, x: 5800)\n",
       "Coordinates:\n",
       "  * t            (t) datetime64[ns] 224B 2024-11-07 2024-11-19 ... 2025-10-21\n",
       "  * y            (y) float64 35kB 9.214e+06 9.214e+06 ... 9.258e+06 9.258e+06\n",
       "  * x            (x) float64 46kB 4.462e+05 4.462e+05 ... 5.042e+05 5.042e+05\n",
       "    band         int64 8B ...\n",
       "Data variables:\n",
       "    VV           (t, y, x) float32 3GB ...\n",
       "    VH           (t, y, x) float32 3GB ...\n",
       "    S2ndvi       (t, y, x) float32 3GB ...\n",
       "    spatial_ref  int64 8B ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data ready for processing!\n",
      "   Dataset variable: 'mpc_dataset'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load MPC Data from Pre-processed NetCDF Files\n",
    "# ============================================================================\n",
    "\n",
    "import xarray as xr\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mpc_data(data_dir='mpc_data', pattern='klambu_glapan_2024*.nc'):\n",
    "    \"\"\"\n",
    "    Load pre-processed MPC data from NetCDF files.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir: Directory containing NetCDF files\n",
    "    - pattern: File pattern to match (default: S1_S2_Demak_*.nc)\n",
    "    \n",
    "    Returns:\n",
    "    - xarray Dataset with dimensions (t, y, x) and variables (VV, VH, S2ndvi)\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    nc_files = sorted(data_path.glob(pattern))\n",
    "    \n",
    "    if not nc_files:\n",
    "        raise FileNotFoundError(f\"No files found matching {data_dir}/{pattern}\")\n",
    "    \n",
    "    print(f\"Found {len(nc_files)} NetCDF file(s)\")\n",
    "    \n",
    "    # Load the NetCDF file(s)\n",
    "    if len(nc_files) == 1:\n",
    "        mpc_dataset = xr.open_dataset(nc_files[0])\n",
    "        print(f\"Loaded: {nc_files[0].name}\")\n",
    "    else:\n",
    "        # If multiple files, concatenate along time dimension\n",
    "        datasets = [xr.open_dataset(f) for f in nc_files]\n",
    "        mpc_dataset = xr.concat(datasets, dim='t')\n",
    "        print(f\"Concatenated {len(nc_files)} files\")\n",
    "    \n",
    "    print(f\"\\nDataset structure:\")\n",
    "    print(f\"  Dimensions: {dict(mpc_dataset.dims)}\")\n",
    "    print(f\"  Variables: {list(mpc_dataset.data_vars)}\")\n",
    "    print(f\"  Time range: {mpc_dataset.t.min().values} to {mpc_dataset.t.max().values}\")\n",
    "    print(f\"  Spatial shape: {mpc_dataset.dims['y']} x {mpc_dataset.dims['x']}\")\n",
    "    \n",
    "    return mpc_dataset\n",
    "\n",
    "# ============================================================================\n",
    "# Load MPC Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading MPC data from mpc_data/*.nc...\")\n",
    "try:\n",
    "    mpc_dataset = load_mpc_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MPC Dataset loaded successfully!\")\n",
    "    print(\"=\"*70)\n",
    "    display(mpc_dataset)\n",
    "    print(\"\\n‚úÖ Data ready for processing!\")\n",
    "    print(\"   Dataset variable: 'mpc_dataset'\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please ensure you have:\")\n",
    "    print(\"   1. Downloaded MPC data using MPC_Data_Prep_Fixed.ipynb\")\n",
    "    print(\"   2. NetCDF files are in './mpc_data/' directory\")\n",
    "    print(\"   3. Files match pattern 'S1_S2_Demak_*.nc'\")\n",
    "    mpc_dataset = None\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "    mpc_dataset = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìò Quick Start Guide: Using MPC Data\n",
    "\n",
    "**Step 1: Download data from MPC** (one-time setup)\n",
    "- Run `MPC_Data_Prep_Fixed.ipynb`\n",
    "- Monitor download progress in the notebook output\n",
    "- NetCDF files will be saved to `./mpc_data/` directory\n",
    "- Wait for download to complete (timing depends on region size)\n",
    "\n",
    "**Step 2: Load data in this notebook**\n",
    "- Execute Cell 6 above to load MPC data\n",
    "- Files are automatically loaded from `./mpc_data/S1_S2_Demak_*.nc`\n",
    "- The `mpc_dataset` variable will contain your time series data\n",
    "- Verify the dataset structure and time range in the output\n",
    "\n",
    "**Step 3: Run MOGPR analysis**\n",
    "- Continue executing cells below\n",
    "- All processing uses the loaded MPC data\n",
    "- Results will reflect your downloaded time period\n",
    "\n",
    "**üí° Tips:**\n",
    "- The MPC data preparation notebook handles all data extraction and formatting\n",
    "- Downloaded NetCDF files are reusable - no need to re-download\n",
    "- Check dataset dimensions to verify spatial and temporal coverage\n",
    "- Time coordinates should be in datetime64 format for proper analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for MOGPR...\n",
      "‚úÖ Using MPC Data dataset\n",
      "\n",
      "Dataset structure:\n",
      "<xarray.Dataset> Size: 9GB\n",
      "Dimensions:      (t: 28, y: 4420, x: 5800)\n",
      "Coordinates:\n",
      "  * t            (t) datetime64[ns] 224B 2024-11-07 2024-11-19 ... 2025-10-21\n",
      "  * y            (y) float64 35kB 9.214e+06 9.214e+06 ... 9.258e+06 9.258e+06\n",
      "  * x            (x) float64 46kB 4.462e+05 4.462e+05 ... 5.042e+05 5.042e+05\n",
      "    band         int64 8B ...\n",
      "Data variables:\n",
      "    VV           (t, y, x) float32 3GB ...\n",
      "    VH           (t, y, x) float32 3GB ...\n",
      "    S2ndvi       (t, y, x) float32 3GB ...\n",
      "    spatial_ref  int64 8B ...\n",
      "\n",
      "Missing data summary:\n",
      "VV: 77.9% missing\n",
      "VH: 77.9% missing\n",
      "S2ndvi: 75.4% missing\n",
      "spatial_ref: 0.0% missing\n"
     ]
    }
   ],
   "source": [
    "def prepare_mogpr_dataset(s1_vv, s1_vh, s2_ndvi, time_coords, y_coords, x_coords):\n",
    "    \"\"\"\n",
    "    Prepare properly formatted xarray Dataset for MOGPR processing\n",
    "    (Only used for synthetic data)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create individual DataArrays with proper naming and coordinates\n",
    "    vv_da = xr.DataArray(\n",
    "        s1_vv,\n",
    "        dims=['t', 'y', 'x'],  # Note: 't' dimension name is required by FuseTS\n",
    "        coords={\n",
    "            't': time_coords,\n",
    "            'y': y_coords,\n",
    "            'x': x_coords\n",
    "        },\n",
    "        name='VV',\n",
    "        attrs={'long_name': 'Sentinel-1 VV backscatter', 'units': 'dB'}\n",
    "    )\n",
    "    \n",
    "    vh_da = xr.DataArray(\n",
    "        s1_vh,\n",
    "        dims=['t', 'y', 'x'],\n",
    "        coords={\n",
    "            't': time_coords,\n",
    "            'y': y_coords,\n",
    "            'x': x_coords\n",
    "        },\n",
    "        name='VH',\n",
    "        attrs={'long_name': 'Sentinel-1 VH backscatter', 'units': 'dB'}\n",
    "    )\n",
    "    \n",
    "    ndvi_da = xr.DataArray(\n",
    "        s2_ndvi,\n",
    "        dims=['t', 'y', 'x'],\n",
    "        coords={\n",
    "            't': time_coords,\n",
    "            'y': y_coords,\n",
    "            'x': x_coords\n",
    "        },\n",
    "        name='S2ndvi',  # Specific naming required by MOGPR\n",
    "        attrs={'long_name': 'Sentinel-2 NDVI', 'units': 'dimensionless'}\n",
    "    )\n",
    "    \n",
    "    # Combine into Dataset\n",
    "    dataset = xr.Dataset({\n",
    "        'VV': vv_da,\n",
    "        'VH': vh_da,\n",
    "        'S2ndvi': ndvi_da\n",
    "    })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Prepare the dataset\n",
    "print(\"Preparing dataset for MOGPR...\")\n",
    "\n",
    "if mpc_dataset is not None:\n",
    "    # Use MPC Data data (already in proper format)\n",
    "    combined_dataset = mpc_dataset\n",
    "    print(\"‚úÖ Using MPC Data dataset\")\n",
    "else:\n",
    "    # Use synthetic data\n",
    "    combined_dataset = prepare_mogpr_dataset(\n",
    "        vv_data, vh_data, ndvi_data,\n",
    "        time_idx, y_coords, x_coords\n",
    "    )\n",
    "    print(\"‚úÖ Using synthetic dataset\")\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "print(combined_dataset)\n",
    "\n",
    "# Check for missing data\n",
    "print(\"\\nMissing data summary:\")\n",
    "for var in combined_dataset.data_vars:\n",
    "    missing_pct = (combined_dataset[var].isnull().sum() / combined_dataset[var].size * 100).values\n",
    "    print(f\"{var}: {missing_pct:.1f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GPU Detection and Configuration\n",
    "\n",
    "Configure whether to use GPU-accelerated MOGPR (if available) or CPU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ GPU detected: NVIDIA H100 80GB HBM3\n",
      "‚úÖ GPU detected: NVIDIA H100 80GB HBM3\n",
      "   VRAM: 84.9 GB\n",
      "   CUDA version: 12.4\n",
      "\n",
      "üöÄ GPU-accelerated MOGPR enabled!\n",
      "   Expected speedup: 10-100x faster than CPU\n",
      "\n",
      "Final configuration: GPU mode\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GPU Detection and Configuration\n",
    "# ============================================================================\n",
    "\n",
    "USE_GPU = True  # Set to True if you have GPU and installed torch + gpytorch\n",
    "\n",
    "if USE_GPU:\n",
    "    try:\n",
    "        import torch\n",
    "        from fusets.mogpr_gpu import MOGPRTransformerGPU\n",
    "        \n",
    "        # Check GPU availability\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            print(f\"‚úÖ GPU detected: {gpu_name}\")\n",
    "            print(f\"   VRAM: {gpu_memory:.1f} GB\")\n",
    "            print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "            print(\"‚úÖ Apple Silicon GPU (MPS) detected\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"‚ö†Ô∏è  No GPU detected, falling back to CPU\")\n",
    "            USE_GPU = False\n",
    "            \n",
    "        if USE_GPU:\n",
    "            print(f\"\\nüöÄ GPU-accelerated MOGPR enabled!\")\n",
    "            print(f\"   Expected speedup: 10-100x faster than CPU\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è  GPU libraries not available: {e}\")\n",
    "        print(\"   Falling back to CPU version\")\n",
    "        USE_GPU = False\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Using CPU version of MOGPR\")\n",
    "    print(\"   To enable GPU: Set USE_GPU = True and install torch + gpytorch\")\n",
    "\n",
    "print(f\"\\nFinal configuration: {'GPU' if USE_GPU else 'CPU'} mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è URGENT: Check if combined_dataset exists and has data\n",
    "\n",
    "**Run this BEFORE creating test_subset!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking if combined_dataset is loaded...\n",
      "\n",
      "‚úÖ combined_dataset exists\n",
      "   Type: <class 'xarray.core.dataset.Dataset'>\n",
      "\n",
      "üìä Dataset structure:\n",
      "<xarray.Dataset> Size: 9GB\n",
      "Dimensions:      (t: 28, y: 4420, x: 5800)\n",
      "Coordinates:\n",
      "  * t            (t) datetime64[ns] 224B 2024-11-07 2024-11-19 ... 2025-10-21\n",
      "  * y            (y) float64 35kB 9.214e+06 9.214e+06 ... 9.258e+06 9.258e+06\n",
      "  * x            (x) float64 46kB 4.462e+05 4.462e+05 ... 5.042e+05 5.042e+05\n",
      "    band         int64 8B ...\n",
      "Data variables:\n",
      "    VV           (t, y, x) float32 3GB nan nan nan nan nan ... nan nan nan nan\n",
      "    VH           (t, y, x) float32 3GB nan nan nan nan nan ... nan nan nan nan\n",
      "    S2ndvi       (t, y, x) float32 3GB nan nan nan nan nan ... nan nan nan nan\n",
      "    spatial_ref  int64 8B 0\n",
      "\n",
      "üìà Data variables:\n",
      "\n",
      "VV:\n",
      "  Shape: (28, 4420, 5800)\n",
      "  Valid: 158,538,349 / 717,808,000 (22.1%)\n",
      "  Range: [0.0014, 8671.5850]\n",
      "\n",
      "VH:\n",
      "  Shape: (28, 4420, 5800)\n",
      "  Valid: 158,538,349 / 717,808,000 (22.1%)\n",
      "  Range: [0.0004, 620.4758]\n",
      "\n",
      "S2ndvi:\n",
      "  Shape: (28, 4420, 5800)\n",
      "  Valid: 176,303,532 / 717,808,000 (24.6%)\n",
      "  Range: [-0.5947, 0.8793]\n",
      "\n",
      "============================================================\n",
      "‚úÖ Dataset has valid data - Ready to proceed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CRITICAL: Verify combined_dataset exists and has data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç Checking if combined_dataset is loaded...\\n\")\n",
    "\n",
    "# Check if variable exists\n",
    "try:\n",
    "    print(f\"‚úÖ combined_dataset exists\")\n",
    "    print(f\"   Type: {type(combined_dataset)}\")\n",
    "    print(f\"\\nüìä Dataset structure:\")\n",
    "    print(combined_dataset)\n",
    "    \n",
    "    # Check data variables\n",
    "    print(f\"\\nüìà Data variables:\")\n",
    "    for var in ['VV', 'VH', 'S2ndvi']:\n",
    "        if var in combined_dataset:\n",
    "            values = combined_dataset[var].values\n",
    "            total = values.size\n",
    "            nans = np.isnan(values).sum()\n",
    "            valid = total - nans\n",
    "            pct_valid = (valid / total) * 100\n",
    "            \n",
    "            print(f\"\\n{var}:\")\n",
    "            print(f\"  Shape: {combined_dataset[var].shape}\")\n",
    "            print(f\"  Valid: {valid:,} / {total:,} ({pct_valid:.1f}%)\")\n",
    "            \n",
    "            if pct_valid == 0:\n",
    "                print(f\"  ‚ùå ALL NaN - NO DATA!\")\n",
    "            elif pct_valid < 10:\n",
    "                print(f\"  ‚ö†Ô∏è  Very sparse data\")\n",
    "            else:\n",
    "                valid_vals = values[~np.isnan(values)]\n",
    "                print(f\"  Range: [{valid_vals.min():.4f}, {valid_vals.max():.4f}]\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå {var}: NOT FOUND in dataset!\")\n",
    "    \n",
    "    # Final verdict\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    all_vars_empty = all(\n",
    "        (combined_dataset[var].values if var in combined_dataset else np.array([np.nan])).size == np.isnan(\n",
    "            combined_dataset[var].values if var in combined_dataset else np.array([np.nan])\n",
    "        ).sum()\n",
    "        for var in ['VV', 'VH', 'S2ndvi']\n",
    "    )\n",
    "    \n",
    "    if all_vars_empty:\n",
    "        print(\"‚ùå CRITICAL ERROR: Dataset is EMPTY (all NaN)!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nüîß YOU NEED TO:\")\n",
    "        print(\"1. Make sure GEOTIFF_DIR is set correctly\")\n",
    "        print(\"2. Re-run the data loading cell:\")\n",
    "        print(\"   combined_dataset = load_geotiff_periods_to_xarray(...)\")\n",
    "        print(\"3. Check that GeoTIFF files exist and have data\")\n",
    "    else:\n",
    "        print(\"‚úÖ Dataset has valid data - Ready to proceed!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "except NameError:\n",
    "    print(\"‚ùå combined_dataset is NOT DEFINED!\")\n",
    "    print(\"\\nüîß YOU NEED TO:\")\n",
    "    print(\"1. Set GEOTIFF_DIR to your GeoTIFF directory\")\n",
    "    print(\"2. Run the cell that calls:\")\n",
    "    print(\"   combined_dataset = load_geotiff_periods_to_xarray(...)\")\n",
    "    print(\"\\nExample:\")\n",
    "    print(\"   GEOTIFF_DIR = '/home/username/data/demak'\")\n",
    "    print(\"   combined_dataset = load_geotiff_periods_to_xarray(\")\n",
    "    print(\"       geotiff_dir=GEOTIFF_DIR,\")\n",
    "    print(\"       num_periods=62,\")\n",
    "    print(\"       file_pattern='period_{:02d}.tif'\")\n",
    "    print(\"   )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Fix: Replace -inf and invalid values with NaN\n",
    "\n",
    "Your data contains `-inf` (negative infinity) values which break MOGPR. This cell will clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Cleaning dataset: Replacing invalid values with NaN...\n",
      "\n",
      "Processing VV...\n",
      "  -inf/+inf values: 0 (0.0%)\n",
      "  Zero values: 0 (0.0%)\n",
      "  ‚úÖ Valid values after cleaning: 158,538,349 (22.1%)\n",
      "\n",
      "Processing VH...\n",
      "  -inf/+inf values: 0 (0.0%)\n",
      "  Zero values: 0 (0.0%)\n",
      "  ‚úÖ Valid values after cleaning: 158,538,349 (22.1%)\n",
      "\n",
      "Processing S2ndvi...\n",
      "  -inf/+inf values: 0 (0.0%)\n",
      "  Zero values: 50,726 (0.0%)\n",
      "  ‚úÖ Valid values after cleaning: 176,303,532 (24.6%)\n",
      "\n",
      "============================================================\n",
      "‚úÖ Dataset cleaned successfully!\n",
      "============================================================\n",
      "\n",
      "üìä Final Data Quality:\n",
      "\n",
      "VV:\n",
      "  Valid: 158,538,349 / 717,808,000 (22.1%)\n",
      "  Range: [0.0014, 8671.5850]\n",
      "  Mean: 0.2021\n",
      "\n",
      "VH:\n",
      "  Valid: 158,538,349 / 717,808,000 (22.1%)\n",
      "  Range: [0.0004, 620.4758]\n",
      "  Mean: 0.0443\n",
      "\n",
      "S2ndvi:\n",
      "  Valid: 176,303,532 / 717,808,000 (24.6%)\n",
      "  Range: [-0.5947, 0.8793]\n",
      "  Mean: 0.2452\n",
      "\n",
      "============================================================\n",
      "‚úÖ Dataset has valid data after cleaning!\n",
      "   You can now proceed with MOGPR fusion.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Clean the dataset: Replace -inf, +inf, and zeros with NaN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîß Cleaning dataset: Replacing invalid values with NaN...\\n\")\n",
    "\n",
    "# Make a copy to avoid modifying original\n",
    "combined_dataset_clean = combined_dataset.copy()\n",
    "\n",
    "for var in ['VV', 'VH', 'S2ndvi']:\n",
    "    print(f\"Processing {var}...\")\n",
    "    \n",
    "    # Get values\n",
    "    values = combined_dataset_clean[var].values\n",
    "    \n",
    "    # Count invalid values\n",
    "    n_inf = np.isinf(values).sum()\n",
    "    n_zero = (values == 0).sum()\n",
    "    total = values.size\n",
    "    \n",
    "    print(f\"  -inf/+inf values: {n_inf:,} ({n_inf/total*100:.1f}%)\")\n",
    "    print(f\"  Zero values: {n_zero:,} ({n_zero/total*100:.1f}%)\")\n",
    "    \n",
    "    # Replace -inf, +inf with NaN\n",
    "    values_clean = np.where(np.isinf(values), np.nan, values)\n",
    "    \n",
    "    # For S2ndvi: Keep zeros (valid NDVI)\n",
    "    # For VV/VH: Zeros might be valid backscatter, but check if they're fill values\n",
    "    if var in ['VV', 'VH']:\n",
    "        # If ALL non-inf values are zero, they're likely fill values\n",
    "        non_inf = values_clean[~np.isnan(values_clean)]\n",
    "        if non_inf.size > 0 and np.all(non_inf == 0):\n",
    "            print(f\"  ‚ö†Ô∏è  All non-inf values are zero - treating as missing data\")\n",
    "            values_clean = np.where(values_clean == 0, np.nan, values_clean)\n",
    "    \n",
    "    # Update dataset\n",
    "    combined_dataset_clean[var].values = values_clean\n",
    "    \n",
    "    # Report after cleaning\n",
    "    valid_after = (~np.isnan(values_clean)).sum()\n",
    "    print(f\"  ‚úÖ Valid values after cleaning: {valid_after:,} ({valid_after/total*100:.1f}%)\\n\")\n",
    "\n",
    "# Replace the original dataset\n",
    "combined_dataset = combined_dataset_clean\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Dataset cleaned successfully!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show final statistics\n",
    "print(\"\\nüìä Final Data Quality:\")\n",
    "for var in ['VV', 'VH', 'S2ndvi']:\n",
    "    values = combined_dataset[var].values\n",
    "    valid = (~np.isnan(values)).sum()\n",
    "    total = values.size\n",
    "    pct_valid = valid / total * 100\n",
    "    \n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"  Valid: {valid:,} / {total:,} ({pct_valid:.1f}%)\")\n",
    "    \n",
    "    if pct_valid > 0:\n",
    "        valid_vals = values[~np.isnan(values)]\n",
    "        print(f\"  Range: [{valid_vals.min():.4f}, {valid_vals.max():.4f}]\")\n",
    "        print(f\"  Mean: {valid_vals.mean():.4f}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Still no valid data!\")\n",
    "\n",
    "# Check if we can proceed\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "any_valid = any(\n",
    "    (~np.isnan(combined_dataset[var].values)).sum() > 0\n",
    "    for var in ['VV', 'VH', 'S2ndvi']\n",
    ")\n",
    "\n",
    "if any_valid:\n",
    "    print(\"‚úÖ Dataset has valid data after cleaning!\")\n",
    "    print(\"   You can now proceed with MOGPR fusion.\")\n",
    "else:\n",
    "    print(\"‚ùå CRITICAL: Still no valid data after cleaning!\")\n",
    "    print(\"\\nüí° This means your MPC download failed or is corrupted.\")\n",
    "    print(\"   You need to:\")\n",
    "    print(\"   1. Check the MPC Data in MPC Code Editor\")\n",
    "    print(\"   2. Re-download/re-export from MPC\")\n",
    "    print(\"   3. Or use the GeoTIFF loading method if you have local files\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking data quality in combined_datase...\n",
      "\n",
      "Dataset structure:\n",
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:  (t: 62, y: 892, x: 1170)\n",
      "Coordinates:\n",
      "  * t        (t) datetime64[ns] 496B 2024-11-07 2024-11-19 ... 2026-11-09\n",
      "  * y        (y) float64 7kB -6.713 -6.713 -6.714 ... -7.112 -7.113 -7.113\n",
      "  * x        (x) float64 9kB 110.5 110.5 110.5 110.5 ... 111.0 111.0 111.0 111.0\n",
      "Data variables:\n",
      "    VV       (t, y, x) float64 518MB nan nan nan nan ... -9.329 -6.132 -8.967\n",
      "    VH       (t, y, x) float64 518MB nan nan nan nan ... -17.68 -18.47 -23.3\n",
      "    S2ndvi   (t, y, x) float64 518MB nan nan nan nan ... 0.2015 0.1685 0.1521\n",
      "Attributes:\n",
      "    title:                Sentinel-1/2 Time Series from GEE Assets\n",
      "    source:               projects/ee-geodeticengineeringundip/assets/FuseTS/...\n",
      "    temporal_resolution:  12-day composites\n",
      "    spatial_resolution:   50m\n",
      "    date_range:           2024-11-07 to 2026-11-09\n",
      "    num_periods:          62\n",
      "    region:               Kabupaten Demak\n",
      "    crs:                  EPSG:4326\n",
      "    fusets_ready:         True\n",
      "\n",
      "üìä Data Statistics:\n",
      "\n",
      "VV:\n",
      "  Shape: (62, 892, 1170)\n",
      "  Total elements: 64,705,680\n",
      "  Valid values: 64,634,310 (99.9%)\n",
      "  NaN values: 71,370 (0.1%)\n",
      "  Range: [-43.5751, 33.6401]\n",
      "  Mean: -9.3731\n",
      "\n",
      "VH:\n",
      "  Shape: (62, 892, 1170)\n",
      "  Total elements: 64,705,680\n",
      "  Valid values: 64,634,310 (99.9%)\n",
      "  NaN values: 71,370 (0.1%)\n",
      "  Range: [-49.4238, 19.2947]\n",
      "  Mean: -16.9913\n",
      "\n",
      "S2ndvi:\n",
      "  Shape: (62, 892, 1170)\n",
      "  Total elements: 64,705,680\n",
      "  Valid values: 55,329,597 (85.5%)\n",
      "  NaN values: 9,376,083 (14.5%)\n",
      "  Range: [-0.2758, 0.6444]\n",
      "  Mean: 0.1790\n",
      "\n",
      "============================================================\n",
      "MOGPR Requirements Check:\n",
      "============================================================\n",
      "‚úÖ VV: 99.9% valid - Good quality\n",
      "‚úÖ VH: 99.9% valid - Good quality\n",
      "‚úÖ S2ndvi: 85.5% valid - Good quality\n",
      "\n",
      "‚úÖ Data quality check passed! Ready for MOGPR.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CRITICAL: Check data before running MOGPR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç Checking data quality in combined_datase...\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(combined_dataset)\n",
    "\n",
    "print(f\"\\nüìä Data Statistics:\")\n",
    "for var in ['VV', 'VH', 'S2ndvi']:\n",
    "    values = combined_dataset[var].values\n",
    "    valid_values = values[~np.isnan(values)]\n",
    "    \n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"  Shape: {values.shape}\")\n",
    "    print(f\"  Total elements: {values.size:,}\")\n",
    "    print(f\"  Valid values: {valid_values.size:,} ({valid_values.size/values.size*100:.1f}%)\")\n",
    "    print(f\"  NaN values: {np.isnan(values).sum():,} ({np.isnan(values).sum()/values.size*100:.1f}%)\")\n",
    "    \n",
    "    if valid_values.size > 0:\n",
    "        print(f\"  Range: [{valid_values.min():.4f}, {valid_values.max():.4f}]\")\n",
    "        print(f\"  Mean: {valid_values.mean():.4f}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå ERROR: No valid values! All NaN!\")\n",
    "\n",
    "# Check if data is usable for MOGPR\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MOGPR Requirements Check:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_nan = {}\n",
    "for var in ['VV', 'VH', 'S2ndvi']:\n",
    "    values = combined_dataset[var].values\n",
    "    pct_valid = (~np.isnan(values)).sum() / values.size * 100\n",
    "    all_nan[var] = pct_valid == 0\n",
    "    \n",
    "    if pct_valid == 0:\n",
    "        print(f\"‚ùå {var}: 100% NaN - CANNOT RUN MOGPR!\")\n",
    "    elif pct_valid < 10:\n",
    "        print(f\"‚ö†Ô∏è  {var}: Only {pct_valid:.1f}% valid - Poor quality\")\n",
    "    elif pct_valid < 50:\n",
    "        print(f\"‚ö†Ô∏è  {var}: {pct_valid:.1f}% valid - Marginal quality\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {var}: {pct_valid:.1f}% valid - Good quality\")\n",
    "\n",
    "if any(all_nan.values()):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ùå CRITICAL ERROR: Some variables are completely empty!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüí° Possible causes:\")\n",
    "    print(\"   1. GeoTIFF files not properly loaded\")\n",
    "    print(\"   2. Wrong directory path in GEOTIFF_DIR\")\n",
    "    print(\"   3. Files exist but have no data\")\n",
    "    print(\"   4. Band indexing is wrong\")\n",
    "    print(\"\\nüîß Solutions:\")\n",
    "    print(\"   1. Check: print(list(GEOTIFF_DIR.glob('*.tif')))\")\n",
    "    print(\"   2. Verify GeoTIFF files have 3 bands (VV, VH, NDVI)\")\n",
    "    print(\"   3. Re-run the data loading cell\")\n",
    "    print(\"   4. Try loading one file manually to inspect\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Data quality check passed! Ready for MOGPR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Training and Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1a. OPTIMIZED Training on FULL Dataset (671√ó893 pixels)\n",
    "\n",
    "**üöÄ Train on ALL pixels for maximum accuracy!**\n",
    "\n",
    "This cell trains the S1‚ÜíNDVI model on the **entire Demak dataset** (599,403 pixels √ó 62 timesteps) instead of just a small subset.\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ More training samples ‚Üí Better model generalization\n",
    "- ‚úÖ Captures full spatial variability across the region\n",
    "- ‚úÖ Learns all crop types and conditions\n",
    "- ‚úÖ Memory-efficient implementation with data sampling\n",
    "\n",
    "**GPU Recommended:** Will run on CPU but MUCH faster on GPU (H100/A100/V100/etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ TRAINING S1‚ÜíNDVI MODEL ON FULL DEMAK DATASET\n",
      "======================================================================\n",
      "\n",
      "üñ•Ô∏è  Device: cuda\n",
      "   GPU: NVIDIA H100 80GB HBM3\n",
      "   VRAM: 84.9 GB\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Preparing training data from combined_dataset\n",
      "======================================================================\n",
      "Dataset dimensions: FrozenMappingWarningOnValuesAccess({'t': 28, 'y': 4420, 'x': 5800})\n",
      "Total pixels: 25,636,000\n",
      "Timesteps: 28\n",
      "\n",
      "Total data points: 717,808,000\n",
      "\n",
      "Cleaning data...\n",
      "Valid training samples: 105,748,366 / 717,808,000 (14.7%)\n",
      "\n",
      "Creating training arrays...\n",
      "Training set size: 105,748,366 samples\n",
      "Features: VV, VH, RVI (3 inputs)\n",
      "Target: NDVI (1 output)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Normalizing inputs and outputs\n",
      "======================================================================\n",
      "\n",
      "Input statistics (before normalization):\n",
      "  VV:   [0.0014, 8671.5850]\n",
      "  VH:   [0.0004, 616.0133]\n",
      "  RVI:  [0.0005, 3.9488]\n",
      "  NDVI: [-0.5947, 0.8793]\n",
      "\n",
      "After normalization:\n",
      "  X: [-0.67, 3092.47]\n",
      "  y: [-5.01, 3.89]\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Creating DataLoader\n",
      "======================================================================\n",
      "Using ALL 105,748,366 training samples\n",
      "Batch size: 2048\n",
      "Number of batches: 51635\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Building neural network\n",
      "======================================================================\n",
      "\n",
      "Model architecture:\n",
      "S1toNDVI_Full(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 2,881\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Training model on H100 GPU\n",
      "======================================================================\n",
      "\n",
      "Starting training for 100 epochs...\n",
      "Early stopping patience: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 204\u001b[39m\n\u001b[32m    201\u001b[39m epoch_loss = \u001b[32m0\u001b[39m\n\u001b[32m    202\u001b[39m epoch_start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/mogpr_h100/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/mogpr_h100/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/mogpr_h100/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/mogpr_h100/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/mogpr_h100/lib/python3.11/site-packages/torch/utils/data/dataset.py:211\u001b[39m, in \u001b[36mTensorDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tensors)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/mogpr_h100/lib/python3.11/site-packages/torch/utils/data/dataset.py:211\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tensors)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN S1‚ÜíNDVI MODEL ON FULL DATASET (671√ó893 = 599,403 pixels)\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ TRAINING S1‚ÜíNDVI MODEL ON FULL DEMAK DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüñ•Ô∏è  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Prepare training data from FULL dataset\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: Preparing training data from combined_dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use combined_dataset (full 671√ó893 pixels) instead of test_subset\n",
    "print(f\"Dataset dimensions: {combined_dataset.dims}\")\n",
    "print(f\"Total pixels: {combined_dataset.dims['y'] * combined_dataset.dims['x']:,}\")\n",
    "print(f\"Timesteps: {combined_dataset.dims['t']}\")\n",
    "\n",
    "# Extract all data\n",
    "VV_full = combined_dataset['VV'].values.flatten()\n",
    "VH_full = combined_dataset['VH'].values.flatten()\n",
    "NDVI_full = combined_dataset['S2ndvi'].values.flatten()\n",
    "\n",
    "print(f\"\\nTotal data points: {len(VV_full):,}\")\n",
    "\n",
    "# Calculate RVI = 4 * VH / (VV + VH)\n",
    "RVI_full = 4 * VH_full / (VV_full + VH_full)\n",
    "\n",
    "# Remove ALL invalid values\n",
    "print(\"\\nCleaning data...\")\n",
    "valid_mask = (\n",
    "    ~np.isnan(VV_full) & ~np.isinf(VV_full) &\n",
    "    ~np.isnan(VH_full) & ~np.isinf(VH_full) &\n",
    "    ~np.isnan(NDVI_full) & ~np.isinf(NDVI_full) &\n",
    "    ~np.isnan(RVI_full) & ~np.isinf(RVI_full)\n",
    ")\n",
    "\n",
    "n_valid = valid_mask.sum()\n",
    "n_total = len(valid_mask)\n",
    "print(f\"Valid training samples: {n_valid:,} / {n_total:,} ({100*n_valid/n_total:.1f}%)\")\n",
    "\n",
    "if n_valid < 1000:\n",
    "    print(\"‚ùå ERROR: Not enough valid training data!\")\n",
    "    print(\"   Please check that combined_dataset is properly loaded.\")\n",
    "else:\n",
    "    # Create training dataset\n",
    "    print(\"\\nCreating training arrays...\")\n",
    "    X_train_full = np.column_stack([\n",
    "        VV_full[valid_mask], \n",
    "        VH_full[valid_mask], \n",
    "        RVI_full[valid_mask]\n",
    "    ])\n",
    "    y_train_full = NDVI_full[valid_mask].reshape(-1, 1)\n",
    "    \n",
    "    print(f\"Training set size: {X_train_full.shape[0]:,} samples\")\n",
    "    print(f\"Features: VV, VH, RVI (3 inputs)\")\n",
    "    print(f\"Target: NDVI (1 output)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Normalize data (CRITICAL for training stability)\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 2: Normalizing inputs and outputs\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    X_mean_full = X_train_full.mean(axis=0)\n",
    "    X_std_full = X_train_full.std(axis=0)\n",
    "    X_train_norm = (X_train_full - X_mean_full) / (X_std_full + 1e-8)\n",
    "    \n",
    "    y_mean_full = y_train_full.mean()\n",
    "    y_std_full = y_train_full.std()\n",
    "    y_train_norm = (y_train_full - y_mean_full) / (y_std_full + 1e-8)\n",
    "    \n",
    "    print(f\"\\nInput statistics (before normalization):\")\n",
    "    print(f\"  VV:   [{X_train_full[:, 0].min():.4f}, {X_train_full[:, 0].max():.4f}]\")\n",
    "    print(f\"  VH:   [{X_train_full[:, 1].min():.4f}, {X_train_full[:, 1].max():.4f}]\")\n",
    "    print(f\"  RVI:  [{X_train_full[:, 2].min():.4f}, {X_train_full[:, 2].max():.4f}]\")\n",
    "    print(f\"  NDVI: [{y_train_full.min():.4f}, {y_train_full.max():.4f}]\")\n",
    "    \n",
    "    print(f\"\\nAfter normalization:\")\n",
    "    print(f\"  X: [{X_train_norm.min():.2f}, {X_train_norm.max():.2f}]\")\n",
    "    print(f\"  y: [{y_train_norm.min():.2f}, {y_train_norm.max():.2f}]\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Create efficient DataLoader (handle large dataset)\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 3: Creating DataLoader\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # For very large datasets, we can subsample or use all data\n",
    "    # Option 1: Use ALL data (slower but most accurate)\n",
    "    USE_ALL_DATA = True\n",
    "    \n",
    "    if USE_ALL_DATA:\n",
    "        X_tensor = torch.FloatTensor(X_train_norm).to(device)\n",
    "        y_tensor = torch.FloatTensor(y_train_norm).to(device)\n",
    "        print(f\"Using ALL {len(X_tensor):,} training samples\")\n",
    "    else:\n",
    "        # Option 2: Subsample for faster training\n",
    "        sample_size = min(1_000_000, len(X_train_norm))\n",
    "        sample_idx = np.random.choice(len(X_train_norm), sample_size, replace=False)\n",
    "        X_tensor = torch.FloatTensor(X_train_norm[sample_idx]).to(device)\n",
    "        y_tensor = torch.FloatTensor(y_train_norm[sample_idx]).to(device)\n",
    "        print(f\"Using {len(X_tensor):,} / {len(X_train_norm):,} samples (random subsample)\")\n",
    "    \n",
    "    # Create DataLoader with optimized batch size\n",
    "    batch_size = 2048 if torch.cuda.is_available() else 512\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Number of batches: {len(train_loader)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: Define neural network model\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 4: Building neural network\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    class S1toNDVI_Full(nn.Module):\n",
    "        \"\"\"\n",
    "        Deep neural network for S1 (VV, VH, RVI) ‚Üí NDVI prediction\n",
    "        \n",
    "        Architecture:\n",
    "        - Input: 3 features (VV, VH, RVI)\n",
    "        - Hidden layers: 64 ‚Üí 32 ‚Üí 16 neurons\n",
    "        - Output: 1 (NDVI)\n",
    "        - Activation: ReLU\n",
    "        - Regularization: Dropout(0.1)\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            super(S1toNDVI_Full, self).__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(3, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    \n",
    "    model_full = S1toNDVI_Full().to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    n_params = sum(p.numel() for p in model_full.parameters())\n",
    "    print(f\"\\nModel architecture:\")\n",
    "    print(model_full)\n",
    "    print(f\"\\nTotal parameters: {n_params:,}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: Train the model\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 5: Training model on H100 GPU\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_full.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    # Learning rate scheduler (reduce on plateau)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 100\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    early_stop_patience = 20\n",
    "    \n",
    "    print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "    print(f\"Early stopping patience: {early_stop_patience}\")\n",
    "    \n",
    "    training_start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model_full.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model_full(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model_full.state_dict(), 's1_ndvi_model_full_BEST.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {avg_loss:.6f} | \"\n",
    "                  f\"Best: {best_loss:.6f} | Time: {epoch_time:.1f}s | \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"\\n‚ö†Ô∏è  Early stopping triggered after {epoch+1} epochs\")\n",
    "            print(f\"   No improvement for {early_stop_patience} epochs\")\n",
    "            break\n",
    "    \n",
    "    training_time = time.time() - training_start\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total training time: {training_time:.1f}s ({training_time/60:.1f} minutes)\")\n",
    "    print(f\"Final loss: {avg_loss:.6f}\")\n",
    "    print(f\"Best loss: {best_loss:.6f}\")\n",
    "    print(f\"Model saved to: s1_ndvi_model_full_BEST.pth\")\n",
    "    \n",
    "    # Load best model\n",
    "    model_full.load_state_dict(torch.load('s1_ndvi_model_full_BEST.pth'))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: Evaluate on training data (sanity check)\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 6: Evaluating model performance\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model_full.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample predictions for evaluation\n",
    "        sample_size = min(10000, len(X_tensor))\n",
    "        sample_idx = np.random.choice(len(X_tensor), sample_size, replace=False)\n",
    "        \n",
    "        X_eval = X_tensor[sample_idx]\n",
    "        y_eval = y_tensor[sample_idx]\n",
    "        \n",
    "        predictions_eval = model_full(X_eval).cpu().numpy()\n",
    "        y_eval_np = y_eval.cpu().numpy()\n",
    "        \n",
    "        # Denormalize for interpretability\n",
    "        pred_denorm = predictions_eval * y_std_full + y_mean_full\n",
    "        true_denorm = y_eval_np * y_std_full + y_mean_full\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = np.mean((pred_denorm - true_denorm)**2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(pred_denorm - true_denorm))\n",
    "        r2 = 1 - (np.sum((true_denorm - pred_denorm)**2) / \n",
    "                  np.sum((true_denorm - true_denorm.mean())**2))\n",
    "        \n",
    "        print(f\"\\nPerformance on {sample_size:,} validation samples:\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MAE:  {mae:.4f}\")\n",
    "        print(f\"  R¬≤:   {r2:.4f}\")\n",
    "    \n",
    "    # Quick visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax = axes[0]\n",
    "    ax.scatter(true_denorm, pred_denorm, alpha=0.3, s=1)\n",
    "    ax.plot([true_denorm.min(), true_denorm.max()], \n",
    "            [true_denorm.min(), true_denorm.max()], \n",
    "            'r--', linewidth=2, label='Perfect prediction')\n",
    "    ax.set_xlabel('True NDVI', fontsize=12)\n",
    "    ax.set_ylabel('Predicted NDVI', fontsize=12)\n",
    "    ax.set_title(f'S1‚ÜíNDVI Prediction (R¬≤={r2:.3f})', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals\n",
    "    ax = axes[1]\n",
    "    residuals = pred_denorm - true_denorm\n",
    "    ax.hist(residuals, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Residual (Predicted - True)', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_title(f'Residuals Distribution (MAE={mae:.4f})', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('s1_ndvi_model_full_evaluation.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Model training and evaluation complete!\")\n",
    "    print(\"   Model saved: s1_ndvi_model_full_BEST.pth\")\n",
    "    print(\"   Evaluation plot: s1_ndvi_model_full_evaluation.png\")\n",
    "    print(\"\\nüí° Next: Use this model to predict NDVI for ALL pixels\")\n",
    "    print(\"   Variables saved: model_full, X_mean_full, X_std_full, y_mean_full, y_std_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1b. Apply Trained Model to Fill NDVI Gaps\n",
    "\n",
    "Use the trained model to predict NDVI for pixels with missing S2 data but available S1 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# APPLY TRAINED MODEL TO FILL NDVI GAPS IN FULL DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîÆ APPLYING S1‚ÜíNDVI MODEL TO FILL GAPS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare ALL data for prediction\n",
    "print(\"\\nPreparing full dataset for gap filling...\")\n",
    "VV_all = combined_dataset['VV'].values.flatten()\n",
    "VH_all = combined_dataset['VH'].values.flatten()\n",
    "RVI_all = 4 * VH_all / (VV_all + VH_all)\n",
    "NDVI_all = combined_dataset['S2ndvi'].values.flatten()\n",
    "\n",
    "# Find pixels with valid S1 but missing S2\n",
    "s1_valid = (\n",
    "    ~np.isnan(VV_all) & ~np.isinf(VV_all) &\n",
    "    ~np.isnan(VH_all) & ~np.isinf(VH_all) &\n",
    "    ~np.isnan(RVI_all) & ~np.isinf(RVI_all)\n",
    ")\n",
    "\n",
    "s2_missing = np.isnan(NDVI_all) | np.isinf(NDVI_all)\n",
    "fillable = s1_valid & s2_missing\n",
    "\n",
    "print(f\"\\nData coverage:\")\n",
    "print(f\"  Total pixels:        {len(NDVI_all):,}\")\n",
    "print(f\"  Original S2 valid:   {(~s2_missing).sum():,} ({100*(~s2_missing).sum()/len(NDVI_all):.1f}%)\")\n",
    "print(f\"  S2 missing:          {s2_missing.sum():,} ({100*s2_missing.sum()/len(NDVI_all):.1f}%)\")\n",
    "print(f\"  Fillable with S1:    {fillable.sum():,} ({100*fillable.sum()/len(NDVI_all):.1f}%)\")\n",
    "\n",
    "if fillable.sum() == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No gaps to fill! All S2 data is already complete.\")\n",
    "else:\n",
    "    # Predict NDVI for fillable pixels\n",
    "    print(f\"\\nüìä Predicting NDVI for {fillable.sum():,} pixels...\")\n",
    "    \n",
    "    # Prepare input data\n",
    "    X_predict = np.column_stack([\n",
    "        VV_all[fillable], \n",
    "        VH_all[fillable], \n",
    "        RVI_all[fillable]\n",
    "    ])\n",
    "    \n",
    "    # Normalize with training statistics\n",
    "    X_predict_norm = (X_predict - X_mean_full) / (X_std_full + 1e-8)\n",
    "    \n",
    "    # Batch prediction for memory efficiency\n",
    "    batch_size = 100000\n",
    "    predictions_list = []\n",
    "    \n",
    "    model_full.eval()\n",
    "    prediction_start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_predict_norm), batch_size):\n",
    "            batch = torch.FloatTensor(X_predict_norm[i:i+batch_size]).to(device)\n",
    "            pred_batch = model_full(batch).cpu().numpy().flatten()\n",
    "            predictions_list.append(pred_batch)\n",
    "            \n",
    "            # Progress\n",
    "            processed = min(i + batch_size, len(X_predict_norm))\n",
    "            if i % (5 * batch_size) == 0 or processed >= len(X_predict_norm):\n",
    "                elapsed = time.time() - prediction_start\n",
    "                rate = processed / elapsed if elapsed > 0 else 0\n",
    "                remaining = (len(X_predict_norm) - processed) / rate if rate > 0 and processed < len(X_predict_norm) else 0\n",
    "                print(f\"  {processed:,} / {len(X_predict_norm):,} ({100*processed/len(X_predict_norm):.1f}%) \"\n",
    "                      f\"- {rate:.0f} px/s - ETA: {remaining:.0f}s\")\n",
    "    \n",
    "    predictions = np.concatenate(predictions_list)\n",
    "    \n",
    "    # Denormalize predictions\n",
    "    predictions_denorm = predictions * y_std_full + y_mean_full\n",
    "    \n",
    "    # Clip to valid NDVI range [-1, 1] with conservative bounds\n",
    "    train_min = np.percentile(y_train_full, 0.1)\n",
    "    train_max = np.percentile(y_train_full, 99.9)\n",
    "    predictions_clipped = np.clip(predictions_denorm, train_min, train_max)\n",
    "    \n",
    "    prediction_time = time.time() - prediction_start\n",
    "    print(f\"\\n‚úÖ Prediction complete in {prediction_time:.1f}s ({len(predictions)/prediction_time:.0f} px/s)\")\n",
    "    \n",
    "    # Fill gaps\n",
    "    print(\"\\nFilling NDVI gaps...\")\n",
    "    NDVI_filled = NDVI_all.copy()\n",
    "    \n",
    "    # Fill missing values with predictions\n",
    "    fill_idx = 0\n",
    "    for i in range(len(NDVI_filled)):\n",
    "        if fillable[i]:\n",
    "            NDVI_filled[i] = predictions_clipped[fill_idx]\n",
    "            fill_idx += 1\n",
    "    \n",
    "    # Reshape to (t, y, x)\n",
    "    nt, ny, nx = combined_dataset.dims['t'], combined_dataset.dims['y'], combined_dataset.dims['x']\n",
    "    NDVI_filled_3d = NDVI_filled.reshape(nt, ny, nx)\n",
    "    \n",
    "    # Create output dataset\n",
    "    print(\"\\nCreating gap-filled dataset...\")\n",
    "    fused_dataset = xr.Dataset(\n",
    "        data_vars={\n",
    "            'VV': combined_dataset['VV'],\n",
    "            'VH': combined_dataset['VH'],\n",
    "            'S2ndvi': combined_dataset['S2ndvi'],\n",
    "            'S2ndvi_DL': (('t', 'y', 'x'), NDVI_filled_3d)\n",
    "        },\n",
    "        coords=combined_dataset.coords,\n",
    "        attrs={\n",
    "            'title': 'S1-S2 Fused NDVI using Deep Learning',\n",
    "            'description': 'Gap-filled NDVI predicted from Sentinel-1 using neural network',\n",
    "            'model': 'S1toNDVI_Full (3-layer NN)',\n",
    "            'training_samples': f'{len(X_train_full):,}',\n",
    "            'creation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Statistics\n",
    "    orig_coverage = (~s2_missing).sum()\n",
    "    filled_coverage = (~np.isnan(NDVI_filled) & ~np.isinf(NDVI_filled)).sum()\n",
    "    improvement = filled_coverage - orig_coverage\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä GAP-FILLING RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Original S2 coverage:  {orig_coverage:,} / {len(NDVI_all):,} ({100*orig_coverage/len(NDVI_all):.1f}%)\")\n",
    "    print(f\"DL-filled coverage:    {filled_coverage:,} / {len(NDVI_all):,} ({100*filled_coverage/len(NDVI_all):.1f}%)\")\n",
    "    print(f\"Improvement:           +{improvement:,} pixels (+{100*improvement/len(NDVI_all):.1f}%)\")\n",
    "    print(f\"\\nPredicted NDVI range:  [{predictions_clipped.min():.4f}, {predictions_clipped.max():.4f}]\")\n",
    "    print(f\"Mean predicted NDVI:   {predictions_clipped.mean():.4f}\")\n",
    "    \n",
    "    # Save\n",
    "    output_file = 'demak_full_s1_ndvi_dl_gapfilled.nc'\n",
    "    print(f\"\\nSaving gap-filled dataset...\")\n",
    "    fused_dataset.to_netcdf(output_file, \n",
    "                           encoding={var: {'zlib': True, 'complevel': 4} \n",
    "                                    for var in fused_dataset.data_vars})\n",
    "    print(f\"‚úÖ Saved to: {output_file}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    print(\"\\nCreating visualization...\")\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Select a representative timestep\n",
    "    t_idx = 15\n",
    "    \n",
    "    # Row 1: Original vs Filled\n",
    "    ax = axes[0, 0]\n",
    "    im = ax.imshow(fused_dataset['S2ndvi'][t_idx].values, cmap='RdYlGn', vmin=-0.2, vmax=0.8)\n",
    "    ax.set_title(f'Original S2 NDVI (t={t_idx})', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    im = ax.imshow(fused_dataset['S2ndvi_DL'][t_idx].values, cmap='RdYlGn', vmin=-0.2, vmax=0.8)\n",
    "    ax.set_title(f'DL Gap-Filled NDVI (t={t_idx})', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    # Difference map\n",
    "    ax = axes[0, 2]\n",
    "    diff = fused_dataset['S2ndvi_DL'][t_idx].values - fused_dataset['S2ndvi'][t_idx].values\n",
    "    im = ax.imshow(diff, cmap='RdBu_r', vmin=-0.3, vmax=0.3)\n",
    "    ax.set_title(f'Difference (DL - Original)', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    # Row 2: Coverage maps and histograms\n",
    "    ax = axes[1, 0]\n",
    "    coverage_orig = ~np.isnan(fused_dataset['S2ndvi'][t_idx].values)\n",
    "    ax.imshow(coverage_orig, cmap='gray')\n",
    "    ax.set_title(f'Original Coverage\\n({coverage_orig.sum():,} pixels)', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    coverage_filled = ~np.isnan(fused_dataset['S2ndvi_DL'][t_idx].values)\n",
    "    ax.imshow(coverage_filled, cmap='gray')\n",
    "    ax.set_title(f'DL-Filled Coverage\\n({coverage_filled.sum():,} pixels)', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Histogram comparison\n",
    "    ax = axes[1, 2]\n",
    "    orig_valid = fused_dataset['S2ndvi'].values.flatten()\n",
    "    orig_valid = orig_valid[~np.isnan(orig_valid)]\n",
    "    filled_valid = fused_dataset['S2ndvi_DL'].values.flatten()\n",
    "    filled_valid = filled_valid[~np.isnan(filled_valid)]\n",
    "    \n",
    "    ax.hist(orig_valid, bins=50, alpha=0.6, label='Original S2', color='orange', density=True, range=(-0.5, 1))\n",
    "    ax.hist(filled_valid, bins=50, alpha=0.6, label='DL Gap-Filled', color='green', density=True, range=(-0.5, 1))\n",
    "    ax.set_xlabel('NDVI', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title('NDVI Distribution Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('demak_full_dl_gapfilling_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ GAP-FILLING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Dataset saved: {output_file}\")\n",
    "    print(f\"Visualization: demak_full_dl_gapfilling_results.png\")\n",
    "    print(f\"\\nüí° Next steps:\")\n",
    "    print(f\"   1. Use 'fused_dataset' for phenology extraction\")\n",
    "    print(f\"   2. Visualize time series for specific pixels\")\n",
    "    print(f\"   3. Export to GeoTIFF for GIS analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED VERSION - Clean data and normalize inputs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Prepare training data with CAREFUL cleaning\n",
    "print(\"Preparing training data...\")\n",
    "VV = test_subset_clean['VV'].values.flatten()\n",
    "VH = test_subset_clean['VH'].values.flatten()\n",
    "NDVI = test_subset_clean['S2ndvi'].values.flatten()\n",
    "\n",
    "# Calculate RVI = 4 * VH / (VV + VH)\n",
    "RVI = 4 * VH / (VV + VH)\n",
    "\n",
    "# CRITICAL: Remove ALL invalid values (NaN, inf)\n",
    "valid_mask = (\n",
    "    ~np.isnan(VV) & ~np.isinf(VV) &\n",
    "    ~np.isnan(VH) & ~np.isinf(VH) &\n",
    "    ~np.isnan(NDVI) & ~np.isinf(NDVI) &\n",
    "    ~np.isnan(RVI) & ~np.isinf(RVI)\n",
    ")\n",
    "\n",
    "print(f\"Valid training samples: {valid_mask.sum():,} / {len(valid_mask):,} ({100*valid_mask.sum()/len(valid_mask):.1f}%)\")\n",
    "\n",
    "if valid_mask.sum() < 100:\n",
    "    print(\"‚ùå Not enough valid training data!\")\n",
    "else:\n",
    "    # Create training dataset\n",
    "    X_train = np.column_stack([VV[valid_mask], VH[valid_mask], RVI[valid_mask]])\n",
    "    y_train = NDVI[valid_mask].reshape(-1, 1)\n",
    "    \n",
    "    # NORMALIZE inputs (critical for training stability)\n",
    "    X_mean = X_train.mean(axis=0)\n",
    "    X_std = X_train.std(axis=0)\n",
    "    X_train_norm = (X_train - X_mean) / (X_std + 1e-8)\n",
    "    \n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std()\n",
    "    y_train_norm = (y_train - y_mean) / (y_std + 1e-8)\n",
    "    \n",
    "    print(f\"Input ranges after normalization:\")\n",
    "    print(f\"  X: [{X_train_norm.min():.2f}, {X_train_norm.max():.2f}]\")\n",
    "    print(f\"  y: [{y_train_norm.min():.2f}, {y_train_norm.max():.2f}]\")\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.FloatTensor(X_train_norm).cuda()\n",
    "    y_tensor = torch.FloatTensor(y_train_norm).cuda()\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    train_loader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "    \n",
    "    # Simple model\n",
    "    class S1toNDVI(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(S1toNDVI, self).__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(3, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = S1toNDVI().cuda()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nTraining S1‚ÜíNDVI model on H100...\")\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    print(\"‚úÖ Training complete!\")\n",
    "    \n",
    "    # Predict for ALL pixels\n",
    "    print(\"\\nPredicting NDVI from S1...\")\n",
    "    model.eval()\n",
    "    \n",
    "    VV_all = test_subset_clean['VV'].values.flatten()\n",
    "    VH_all = test_subset_clean['VH'].values.flatten()\n",
    "    RVI_all = 4 * VH_all / (VV_all + VH_all)\n",
    "    NDVI_all = test_subset_clean['S2ndvi'].values.flatten()\n",
    "    \n",
    "    # Valid S1 data\n",
    "    s1_valid = (\n",
    "        ~np.isnan(VV_all) & ~np.isinf(VV_all) &\n",
    "        ~np.isnan(VH_all) & ~np.isinf(VH_all) &\n",
    "        ~np.isnan(RVI_all) & ~np.isinf(RVI_all)\n",
    "    )\n",
    "    \n",
    "    # Prepare and normalize prediction data\n",
    "    X_pred = np.column_stack([VV_all[s1_valid], VH_all[s1_valid], RVI_all[s1_valid]])\n",
    "    X_pred_norm = (X_pred - X_mean) / (X_std + 1e-8)\n",
    "    X_pred_tensor = torch.FloatTensor(X_pred_norm).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions_norm = model(X_pred_tensor).cpu().numpy().flatten()\n",
    "        # Denormalize predictions\n",
    "        predictions = predictions_norm * y_std + y_mean\n",
    "    \n",
    "    # Fill gaps\n",
    "    NDVI_filled = NDVI_all.copy()\n",
    "    pred_idx = 0\n",
    "    for i in range(len(NDVI_filled)):\n",
    "        if s1_valid[i]:\n",
    "            if np.isnan(NDVI_filled[i]) or np.isinf(NDVI_filled[i]):\n",
    "                NDVI_filled[i] = predictions[pred_idx]\n",
    "            pred_idx += 1\n",
    "    \n",
    "    # Reshape\n",
    "    ny, nx, nt = test_subset_clean.dims['y'], test_subset_clean.dims['x'], test_subset_clean.dims['t']\n",
    "    # FIX: Recreate S2ndvi_DL with correct dimensions\n",
    "print(\"Fixing dimension order...\")\n",
    "\n",
    "# The filled data is currently (y, x, t) but should be (t, y, x)\n",
    "NDVI_filled_3d_fixed = NDVI_filled_3d.transpose(2, 0, 1)  # (y, x, t) ‚Üí (t, y, x)\n",
    "\n",
    "# Recreate dataset with correct dimensions\n",
    "fused_dl = xr.Dataset(\n",
    "    data_vars={\n",
    "        'VV': test_subset_clean['VV'],\n",
    "        'VH': test_subset_clean['VH'],\n",
    "        'S2ndvi': test_subset_clean['S2ndvi'],\n",
    "        'S2ndvi_DL': (('t', 'y', 'x'), NDVI_filled_3d_fixed)  # Match original order\n",
    "    },\n",
    "    coords=test_subset_clean.coords\n",
    ")\n",
    "\n",
    "print(\"Fixed! New shapes:\")\n",
    "print(f\"S2ndvi: {fused_dl['S2ndvi'].shape}\")\n",
    "print(f\"S2ndvi_DL: {fused_dl['S2ndvi_DL'].shape}\")\n",
    "\n",
    "# Now save\n",
    "fused_dl.to_netcdf('s1_to_ndvi_dl_50x50.nc')\n",
    "print(\"‚úÖ Saved with correct dimensions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Quick Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization - NOW with correct dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for idx, (iy, ix) in enumerate([(10, 10), (25, 25), (40, 40)]):\n",
    "    ax = axes[0, idx]\n",
    "    # Now both are (t, y, x), so index as [:, iy, ix]\n",
    "    original = fused_dl['S2ndvi'][:, iy, ix].values\n",
    "    filled = fused_dl['S2ndvi_DL'][:, iy, ix].values\n",
    "    \n",
    "    ax.plot(range(62), original, 'o', label='Original S2', markersize=8, alpha=0.6, color='orange')\n",
    "    ax.plot(range(62), filled, 's-', label='DL Filled', markersize=5, alpha=0.8, color='green')\n",
    "    ax.set_title(f'Pixel [y={iy}, x={ix}]', fontsize=12)\n",
    "    ax.set_xlabel('Time Period')\n",
    "    ax.set_ylabel('NDVI')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show S1 data\n",
    "    ax2 = axes[1, idx]\n",
    "    vv = fused_dl['VV'][:, iy, ix].values\n",
    "    vh = fused_dl['VH'][:, iy, ix].values\n",
    "    ax2.plot(range(62), vv, 'o-', label='VV', markersize=5)\n",
    "    ax2.plot(range(62), vh, 's-', label='VH', markersize=5)\n",
    "    ax2.set_xlabel('Time Period')\n",
    "    ax2.set_ylabel('Backscatter (dB)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dl_results_3pixels.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"‚úÖ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Apply Model to FULL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DL model to FULL dataset (671√ó893 pixels)\n",
    "print(\"=\" * 70)\n",
    "print(\"SCALING TO FULL DEMAK DATASET (671√ó893 = 599,403 pixels)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if combined_dataset is loaded\n",
    "if 'combined_dataset' not in locals():\n",
    "    print(\"Loading full dataset from GeoTIFF files...\")\n",
    "    # Use your loading function here\n",
    "    # combined_dataset = load_geotiff_periods_to_xarray(...)\n",
    "\n",
    "print(f\"\\nFull dataset dimensions: {combined_dataset.dims}\")\n",
    "\n",
    "# Clean full dataset\n",
    "print(\"Cleaning full dataset...\")\n",
    "full_clean = combined_dataset.copy(deep=True)\n",
    "for var in full_clean.data_vars:\n",
    "    full_clean[var] = full_clean[var].where(~np.isinf(full_clean[var]), np.nan)\n",
    "\n",
    "# Prepare data for prediction\n",
    "print(\"Preparing data for prediction...\")\n",
    "VV_full = full_clean['VV'].values.flatten()\n",
    "VH_full = full_clean['VH'].values.flatten()\n",
    "RVI_full = 4 * VH_full / (VV_full + VH_full)\n",
    "NDVI_full = full_clean['S2ndvi'].values.flatten()\n",
    "\n",
    "# Find valid S1 pixels\n",
    "s1_valid_full = (\n",
    "    ~np.isnan(VV_full) & ~np.isinf(VV_full) &\n",
    "    ~np.isnan(VH_full) & ~np.isinf(VH_full) &\n",
    "    ~np.isnan(RVI_full) & ~np.isinf(RVI_full)\n",
    ")\n",
    "\n",
    "print(f\"Valid S1 pixels: {s1_valid_full.sum():,} / {len(s1_valid_full):,} ({100*s1_valid_full.sum()/len(s1_valid_full):.1f}%)\")\n",
    "\n",
    "# Predict in batches\n",
    "print(\"\\nPredicting NDVI for all valid pixels...\")\n",
    "X_pred_full = np.column_stack([\n",
    "    VV_full[s1_valid_full], \n",
    "    VH_full[s1_valid_full], \n",
    "    RVI_full[s1_valid_full]\n",
    "])\n",
    "\n",
    "# Normalize with training stats\n",
    "X_pred_norm = (X_pred_full - X_mean) / (X_std + 1e-8)\n",
    "\n",
    "# Batch prediction for memory efficiency\n",
    "batch_size = 100000\n",
    "predictions_full = []\n",
    "model.eval()\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_pred_norm), batch_size):\n",
    "        batch = torch.FloatTensor(X_pred_norm[i:i+batch_size]).cuda()\n",
    "        pred_batch = model(batch).cpu().numpy().flatten()\n",
    "        predictions_full.append(pred_batch)\n",
    "        \n",
    "        if len(predictions_full) % 5 == 0 or i + batch_size >= len(X_pred_norm):\n",
    "            processed = min(i + batch_size, len(X_pred_norm))\n",
    "            elapsed = time.time() - start\n",
    "            rate = processed / elapsed\n",
    "            remaining = (len(X_pred_norm) - processed) / rate if processed < len(X_pred_norm) else 0\n",
    "            print(f\"  Processed: {processed:,} / {len(X_pred_norm):,} ({100*processed/len(X_pred_norm):.1f}%) - {rate:.0f} pixels/sec - ETA: {remaining:.0f}s\")\n",
    "\n",
    "predictions_full = np.concatenate(predictions_full)\n",
    "# Denormalize\n",
    "predictions_full = predictions_full * y_std + y_mean\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"‚úÖ Prediction complete in {elapsed:.1f}s ({len(X_pred_norm)/elapsed:.0f} pixels/sec)\")\n",
    "\n",
    "# Fill gaps\n",
    "print(\"\\nFilling NDVI gaps...\")\n",
    "NDVI_filled_full = NDVI_full.copy()\n",
    "pred_idx = 0\n",
    "for i in range(len(NDVI_filled_full)):\n",
    "    if s1_valid_full[i]:\n",
    "        if np.isnan(NDVI_filled_full[i]) or np.isinf(NDVI_filled_full[i]):\n",
    "            NDVI_filled_full[i] = predictions_full[pred_idx]\n",
    "        pred_idx += 1\n",
    "\n",
    "# Reshape to (t, y, x)\n",
    "nt, ny, nx = full_clean.dims['t'], full_clean.dims['y'], full_clean.dims['x']\n",
    "NDVI_filled_full_3d = NDVI_filled_full.reshape(nt, ny, nx)\n",
    "\n",
    "# Create output dataset\n",
    "print(\"Creating output dataset...\")\n",
    "fused_full = xr.Dataset(\n",
    "    data_vars={\n",
    "        'VV': full_clean['VV'],\n",
    "        'VH': full_clean['VH'],\n",
    "        'S2ndvi': full_clean['S2ndvi'],\n",
    "        'S2ndvi_DL': (('t', 'y', 'x'), NDVI_filled_full_3d)\n",
    "    },\n",
    "    coords=full_clean.coords\n",
    ")\n",
    "\n",
    "# Statistics\n",
    "orig_valid = (~np.isnan(NDVI_full) & ~np.isinf(NDVI_full)).sum()\n",
    "filled_valid = (~np.isnan(NDVI_filled_full) & ~np.isinf(NDVI_filled_full)).sum()\n",
    "print(f\"\\nüìä RESULTS:\")\n",
    "print(f\"   Original S2 coverage: {orig_valid:,} / {len(NDVI_full):,} ({100*orig_valid/len(NDVI_full):.1f}%)\")\n",
    "print(f\"   DL-filled coverage:   {filled_valid:,} / {len(NDVI_full):,} ({100*filled_valid/len(NDVI_full):.1f}%)\")\n",
    "print(f\"   Improvement: {filled_valid - orig_valid:,} pixels filled (+{100*(filled_valid-orig_valid)/len(NDVI_full):.1f}%)\")\n",
    "\n",
    "# Save with compression\n",
    "print(\"\\nSaving full dataset (this may take a few minutes)...\")\n",
    "fused_full.to_netcdf('demak_full_671x893_s1_ndvi_dl.nc', \n",
    "                     encoding={var: {'zlib': True, 'complevel': 4} \n",
    "                              for var in fused_full.data_vars})\n",
    "print(\"‚úÖ Saved to demak_full_671x893_s1_ndvi_dl.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX: Reprocess DL predictions with proper NDVI clipping\n",
    "print(\"=\" * 70)\n",
    "print(\"FIXING DL PREDICTIONS - APPLYING NDVI CLIPPING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the filled NDVI (already computed)\n",
    "NDVI_filled_full = fused_full['S2ndvi_DL'].values.flatten()\n",
    "\n",
    "# Check before clipping\n",
    "print(\"\\nBefore clipping:\")\n",
    "print(f\"  Range: [{np.nanmin(NDVI_filled_full):.4f}, {np.nanmax(NDVI_filled_full):.4f}]\")\n",
    "print(f\"  Values < -1: {(NDVI_filled_full < -1).sum():,}\")\n",
    "print(f\"  Values > 1:  {(NDVI_filled_full > 1).sum():,}\")\n",
    "\n",
    "# CLIP to valid NDVI range [-1, 1]\n",
    "NDVI_filled_clipped = np.clip(NDVI_filled_full, -1.0, 1.0)\n",
    "\n",
    "# Also apply a more conservative clip based on training data range\n",
    "# Use the 0.1% and 99.9% percentiles from training to avoid extreme values\n",
    "train_min = np.percentile(y_train, 0.1)\n",
    "train_max = np.percentile(y_train, 99.9)\n",
    "print(f\"\\nTraining NDVI range (0.1% - 99.9%): [{train_min:.4f}, {train_max:.4f}]\")\n",
    "\n",
    "# Apply conservative clipping\n",
    "NDVI_filled_conservative = np.clip(NDVI_filled_full, train_min, train_max)\n",
    "\n",
    "print(\"\\nAfter clipping to [-1, 1]:\")\n",
    "print(f\"  Range: [{np.nanmin(NDVI_filled_clipped):.4f}, {np.nanmax(NDVI_filled_clipped):.4f}]\")\n",
    "print(f\"  Mean: {np.nanmean(NDVI_filled_clipped):.4f}\")\n",
    "\n",
    "print(\"\\nAfter conservative clipping:\")\n",
    "print(f\"  Range: [{np.nanmin(NDVI_filled_conservative):.4f}, {np.nanmax(NDVI_filled_conservative):.4f}]\")\n",
    "print(f\"  Mean: {np.nanmean(NDVI_filled_conservative):.4f}\")\n",
    "\n",
    "# Reshape back to (t, y, x)\n",
    "nt, ny, nx = fused_full.dims['t'], fused_full.dims['y'], fused_full.dims['x']\n",
    "NDVI_clipped_3d = NDVI_filled_conservative.reshape(nt, ny, nx)\n",
    "\n",
    "# Create corrected dataset\n",
    "print(\"\\nCreating corrected dataset...\")\n",
    "fused_full_corrected = xr.Dataset(\n",
    "    data_vars={\n",
    "        'VV': fused_full['VV'],\n",
    "        'VH': fused_full['VH'],\n",
    "        'S2ndvi': fused_full['S2ndvi'],\n",
    "        'S2ndvi_DL': (('t', 'y', 'x'), NDVI_clipped_3d)\n",
    "    },\n",
    "    coords=fused_full.coords\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Corrected dataset created!\")\n",
    "\n",
    "# Save corrected version\n",
    "print(\"\\nSaving corrected dataset...\")\n",
    "fused_full_corrected.to_netcdf('demak_full_671x893_s1_ndvi_dl_CORRECTED.nc', \n",
    "                                encoding={var: {'zlib': True, 'complevel': 4} \n",
    "                                         for var in fused_full_corrected.data_vars})\n",
    "print(\"‚úÖ Saved to demak_full_671x893_s1_ndvi_dl_CORRECTED.nc\")\n",
    "\n",
    "# Compare before/after\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Original S2 NDVI\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(fused_full['S2ndvi'][15, :, :].values, cmap='RdYlGn', vmin=-0.2, vmax=0.6)\n",
    "ax.set_title('Original S2 NDVI (Time=15)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "# DL-filled (uncorrected)\n",
    "ax = axes[0, 1]\n",
    "im = ax.imshow(fused_full['S2ndvi_DL'][15, :, :].values, cmap='RdYlGn', vmin=-0.2, vmax=0.6)\n",
    "ax.set_title('DL-filled NDVI - UNCORRECTED (Time=15)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "# DL-filled (corrected)\n",
    "ax = axes[1, 0]\n",
    "im = ax.imshow(fused_full_corrected['S2ndvi_DL'][15, :, :].values, cmap='RdYlGn', vmin=-0.2, vmax=0.6)\n",
    "ax.set_title('DL-filled NDVI - CORRECTED (Time=15)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "# Histogram comparison\n",
    "ax = axes[1, 1]\n",
    "s2_valid = fused_full['S2ndvi'].values.flatten()\n",
    "s2_valid = s2_valid[~np.isnan(s2_valid)]\n",
    "dl_uncorrected = fused_full['S2ndvi_DL'].values.flatten()\n",
    "dl_uncorrected = dl_uncorrected[~np.isnan(dl_uncorrected)]\n",
    "dl_corrected = fused_full_corrected['S2ndvi_DL'].values.flatten()\n",
    "dl_corrected = dl_corrected[~np.isnan(dl_corrected)]\n",
    "\n",
    "ax.hist(s2_valid, bins=100, alpha=0.5, label='Original S2', range=(-0.5, 1), density=True)\n",
    "ax.hist(dl_corrected, bins=100, alpha=0.5, label='DL Corrected', range=(-0.5, 1), density=True)\n",
    "ax.set_xlabel('NDVI Value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('NDVI Distribution Comparison', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ndvi_correction_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved correction comparison\")\n",
    "\n",
    "# Update the working dataset\n",
    "fused_full = fused_full_corrected\n",
    "print(\"\\n‚úÖ Updated fused_full to use corrected NDVI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Phenological Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-extract phenology with corrected NDVI\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RE-EXTRACTING PHENOLOGY WITH CORRECTED NDVI\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ndvi_for_phenology = fused_full['S2ndvi_DL'].rename({'t': 'time'})\n",
    "\n",
    "print(f\"\\nCorrected NDVI range: [{np.nanmin(ndvi_for_phenology.values):.4f}, {np.nanmax(ndvi_for_phenology.values):.4f}]\")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "phenology_metrics = phenology(ndvi_for_phenology)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Phenology extraction complete in {elapsed:.1f}s\")\n",
    "print(phenology_metrics)\n",
    "\n",
    "# Save\n",
    "phenology_metrics.to_netcdf('demak_phenology_metrics_CORRECTED.nc',\n",
    "                            encoding={var: {'zlib': True, 'complevel': 4} \n",
    "                                     for var in phenology_metrics.data_vars})\n",
    "print(\"‚úÖ Saved corrected phenology metrics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Pixel Start of Season Analysis\n",
    "\n",
    "**Important**: This workflow provides **Start of Season (SOS) information for every pixel** in your study area!\n",
    "\n",
    "### What you get for each pixel:\n",
    "- **SOS Timing**: Day of year when Start of Season occurs (1-365)\n",
    "- **SOS Values**: NDVI value at the Start of Season\n",
    "- **Spatial Coverage**: Complete coverage for your entire study area\n",
    "- **Resolution**: Same spatial resolution as your input data (e.g., 10m pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and Visualize Start of Season (SOS) Information\n",
    "print(\"=\" * 70)\n",
    "print(\"START OF SEASON (SOS) ANALYSIS FOR ALL PIXELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if SOS metrics exist in phenology results\n",
    "print(\"\\nAvailable SOS-related metrics:\")\n",
    "sos_metrics = [var for var in phenology_metrics.data_vars if 'sos' in var.lower()]\n",
    "for metric in sos_metrics:\n",
    "    print(f\"  - {metric}\")\n",
    "\n",
    "# Extract SOS timing (day of year)\n",
    "if 'sos_times' in phenology_metrics.data_vars:\n",
    "    sos_doy = phenology_metrics['sos_times'].values\n",
    "    \n",
    "    print(f\"\\nSOS Statistics (Day of Year):\")\n",
    "    valid_sos = ~np.isnan(sos_doy)\n",
    "    print(f\"  Pixels with SOS detected: {valid_sos.sum():,} / {sos_doy.size:,} ({100*valid_sos.sum()/sos_doy.size:.1f}%)\")\n",
    "    \n",
    "    if valid_sos.sum() > 0:\n",
    "        print(f\"  Range: Day {np.nanmin(sos_doy):.0f} to Day {np.nanmax(sos_doy):.0f}\")\n",
    "        print(f\"  Mean: Day {np.nanmean(sos_doy):.0f}\")\n",
    "        print(f\"  Median: Day {np.nanmedian(sos_doy):.0f}\")\n",
    "        \n",
    "        # Convert to actual dates\n",
    "        base_date = pd.Timestamp('2024-01-01')\n",
    "        earliest_sos = base_date + pd.Timedelta(days=int(np.nanmin(sos_doy)))\n",
    "        latest_sos = base_date + pd.Timedelta(days=int(np.nanmax(sos_doy)))\n",
    "        median_sos = base_date + pd.Timedelta(days=int(np.nanmedian(sos_doy)))\n",
    "        \n",
    "        print(f\"\\n  üìÖ SOS Date Range:\")\n",
    "        print(f\"     Earliest: {earliest_sos.strftime('%Y-%m-%d')} ({earliest_sos.strftime('%B %d')})\")\n",
    "        print(f\"     Median:   {median_sos.strftime('%Y-%m-%d')} ({median_sos.strftime('%B %d')})\")\n",
    "        print(f\"     Latest:   {latest_sos.strftime('%Y-%m-%d')} ({latest_sos.strftime('%B %d')})\")\n",
    "        \n",
    "        # Create SOS map\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "        \n",
    "        # 1. SOS Day of Year Map\n",
    "        ax = axes[0, 0]\n",
    "        im = ax.imshow(sos_doy, cmap='RdYlGn_r', vmin=0, vmax=365, aspect='auto')\n",
    "        ax.set_title('Start of Season - Day of Year', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('X (893 pixels)')\n",
    "        ax.set_ylabel('Y (671 pixels)')\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Day of Year', fontsize=11)\n",
    "        \n",
    "        # 2. SOS Month Classification\n",
    "        ax = axes[0, 1]\n",
    "        sos_month = np.floor((sos_doy % 365) / 30.5).astype(float)\n",
    "        sos_month[~valid_sos] = np.nan\n",
    "        im = ax.imshow(sos_month, cmap='tab20', vmin=0, vmax=12, aspect='auto')\n",
    "        ax.set_title('Start of Season - Month', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('X (893 pixels)')\n",
    "        ax.set_ylabel('Y (671 pixels)')\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04, ticks=range(13))\n",
    "        cbar.set_label('Month (0=Jan, 11=Dec)', fontsize=11)\n",
    "        \n",
    "        # 3. SOS Histogram\n",
    "        ax = axes[1, 0]\n",
    "        sos_valid = sos_doy[valid_sos]\n",
    "        counts, bins, patches = ax.hist(sos_valid, bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "        ax.set_xlabel('Day of Year', fontsize=12)\n",
    "        ax.set_ylabel('Number of Pixels', fontsize=12)\n",
    "        ax.set_title('Distribution of Start of Season', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add month labels\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        month_days = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "        ax2 = ax.twiny()\n",
    "        ax2.set_xlim(ax.get_xlim())\n",
    "        ax2.set_xticks(month_days)\n",
    "        ax2.set_xticklabels(month_names, rotation=45)\n",
    "        \n",
    "        # 4. Pixel count by month\n",
    "        ax = axes[1, 1]\n",
    "        month_counts = []\n",
    "        for m in range(12):\n",
    "            month_mask = (sos_valid >= month_days[m]) & (sos_valid < (month_days[m+1] if m < 11 else 365))\n",
    "            month_counts.append(month_mask.sum())\n",
    "        \n",
    "        bars = ax.bar(range(12), month_counts, color='green', alpha=0.7, edgecolor='black')\n",
    "        ax.set_xlabel('Month', fontsize=12)\n",
    "        ax.set_ylabel('Number of Pixels', fontsize=12)\n",
    "        ax.set_title('SOS Distribution by Month', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(range(12))\n",
    "        ax.set_xticklabels(month_names, rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add percentage labels on bars\n",
    "        for i, (bar, count) in enumerate(zip(bars, month_counts)):\n",
    "            if count > 0:\n",
    "                pct = 100 * count / valid_sos.sum()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(month_counts)*0.01,\n",
    "                       f'{pct:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('demak_SOS_comprehensive_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"\\n‚úÖ Saved SOS analysis to demak_SOS_comprehensive_analysis.png\")\n",
    "        \n",
    "        # Export SOS data to CSV for further analysis\n",
    "        print(\"\\nExporting SOS data to CSV...\")\n",
    "        sos_data = []\n",
    "        y_coords = phenology_metrics.y.values\n",
    "        x_coords = phenology_metrics.x.values\n",
    "        \n",
    "        for iy in range(len(y_coords)):\n",
    "            for ix in range(len(x_coords)):\n",
    "                if not np.isnan(sos_doy[iy, ix]):\n",
    "                    sos_day = int(sos_doy[iy, ix])\n",
    "                    sos_date = base_date + pd.Timedelta(days=sos_day)\n",
    "                    sos_data.append({\n",
    "                        'y_idx': iy,\n",
    "                        'x_idx': ix,\n",
    "                        'lat': y_coords[iy],\n",
    "                        'lon': x_coords[ix],\n",
    "                        'sos_doy': sos_day,\n",
    "                        'sos_date': sos_date.strftime('%Y-%m-%d'),\n",
    "                        'sos_month': sos_date.month,\n",
    "                        'sos_month_name': sos_date.strftime('%B')\n",
    "                    })\n",
    "        \n",
    "        sos_df = pd.DataFrame(sos_data)\n",
    "        sos_df.to_csv('demak_SOS_all_pixels.csv', index=False)\n",
    "        print(f\"‚úÖ Saved SOS data for {len(sos_df):,} pixels to demak_SOS_all_pixels.csv\")\n",
    "        \n",
    "        # Summary statistics by month\n",
    "        print(\"\\nüìä SOS Summary by Month:\")\n",
    "        print(sos_df.groupby('sos_month_name').size().sort_index())\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  SOS timing (sos_times) not found in phenology metrics!\")\n",
    "    print(\"Available metrics:\", list(phenology_metrics.data_vars))\n",
    "\n",
    "# Also check for SOS values (NDVI at start of season)\n",
    "if 'sos_values' in phenology_metrics.data_vars:\n",
    "    sos_values = phenology_metrics['sos_values'].values\n",
    "    valid_values = ~np.isnan(sos_values) & (sos_values != 0)\n",
    "    \n",
    "    print(f\"\\nüìà SOS NDVI Values:\")\n",
    "    print(f\"  Pixels with SOS NDVI: {valid_values.sum():,}\")\n",
    "    if valid_values.sum() > 0:\n",
    "        print(f\"  Range: [{np.nanmin(sos_values[valid_values]):.4f}, {np.nanmax(sos_values[valid_values]):.4f}]\")\n",
    "        print(f\"  Mean: {np.nanmean(sos_values[valid_values]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Per-Pixel Analysis and Export Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Full Phenology Metrics for Individual Pixels\n",
    "print(\"=\" * 70)\n",
    "print(\"FULL PHENOLOGY METRICS PER PIXEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def show_pixel_phenology(iy, ix):\n",
    "    \"\"\"Display all phenology metrics for a specific pixel\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PIXEL [{iy}, {ix}] - Coordinates: (y={phenology_metrics.y.values[iy]:.4f}, x={phenology_metrics.x.values[ix]:.4f})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Extract all metrics for this pixel\n",
    "    metrics = {}\n",
    "    for var in phenology_metrics.data_vars:\n",
    "        value = phenology_metrics[var].values[iy, ix]\n",
    "        metrics[var] = value\n",
    "    \n",
    "    # Organize and display metrics by category\n",
    "    print(\"\\nüìÖ TIMING METRICS (Day of Year):\")\n",
    "    timing_metrics = ['sos_times', 'pos_times', 'eos_times', 'vos_times', 'mos_times']\n",
    "    for metric in timing_metrics:\n",
    "        if metric in metrics:\n",
    "            doy = metrics[metric]\n",
    "            if not np.isnan(doy) and doy > 0:\n",
    "                date = pd.Timestamp('2024-01-01') + pd.Timedelta(days=int(doy))\n",
    "                print(f\"  {metric:12s}: Day {doy:6.1f} = {date.strftime('%Y-%m-%d')} ({date.strftime('%B %d')})\")\n",
    "            else:\n",
    "                print(f\"  {metric:12s}: No data\")\n",
    "    \n",
    "    print(\"\\nüìä VALUE METRICS (NDVI):\")\n",
    "    value_metrics = ['sos_values', 'pos_values', 'eos_values', 'vos_values', 'mos_values', \n",
    "                     'bse_values', 'aos_values']\n",
    "    for metric in value_metrics:\n",
    "        if metric in metrics:\n",
    "            val = metrics[metric]\n",
    "            if not np.isnan(val):\n",
    "                print(f\"  {metric:12s}: {val:8.4f}\")\n",
    "            else:\n",
    "                print(f\"  {metric:12s}: No data\")\n",
    "    \n",
    "    print(\"\\nüìè DURATION METRICS (Days):\")\n",
    "    duration_metrics = ['lios_values', 'sios_values', 'liot_values', 'siot_values']\n",
    "    for metric in duration_metrics:\n",
    "        if metric in metrics:\n",
    "            val = metrics[metric]\n",
    "            if not np.isnan(val) and val > 0:\n",
    "                print(f\"  {metric:12s}: {val:8.1f} days\")\n",
    "            else:\n",
    "                print(f\"  {metric:12s}: No data\")\n",
    "    \n",
    "    print(\"\\nüìà RATE METRICS:\")\n",
    "    rate_metrics = ['roi_values', 'rod_values']\n",
    "    for metric in rate_metrics:\n",
    "        if metric in metrics:\n",
    "            val = metrics[metric]\n",
    "            if not np.isnan(val):\n",
    "                print(f\"  {metric:12s}: {val:8.4f}\")\n",
    "            else:\n",
    "                print(f\"  {metric:12s}: No data\")\n",
    "    \n",
    "    # Plot NDVI time series with phenology markers\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "    \n",
    "    # Get NDVI time series\n",
    "    ndvi_ts = fused_full['S2ndvi_DL'][:, iy, ix].values\n",
    "    dates = pd.to_datetime(fused_full.t.values)\n",
    "    \n",
    "    # Plot NDVI\n",
    "    ax.plot(dates, ndvi_ts, 'o-', linewidth=2, markersize=6, \n",
    "            color='green', label='DL Gap-filled NDVI', alpha=0.8)\n",
    "    \n",
    "    # Add phenology markers\n",
    "    base_date = pd.Timestamp('2024-01-01')\n",
    "    colors = {\n",
    "        'sos_times': ('green', 'Start of Season'),\n",
    "        'pos_times': ('red', 'Peak of Season'),\n",
    "        'eos_times': ('orange', 'End of Season'),\n",
    "        'vos_times': ('blue', 'Valley of Season'),\n",
    "    }\n",
    "    \n",
    "    for metric, (color, label) in colors.items():\n",
    "        if metric in metrics:\n",
    "            doy = metrics[metric]\n",
    "            if not np.isnan(doy) and doy > 0:\n",
    "                date = base_date + pd.Timedelta(days=int(doy))\n",
    "                if date >= dates[0] and date <= dates[-1]:\n",
    "                    # Get NDVI value at this date\n",
    "                    idx = np.argmin(np.abs(dates - date))\n",
    "                    ndvi_val = ndvi_ts[idx]\n",
    "                    ax.axvline(date, color=color, linestyle='--', alpha=0.7, linewidth=2)\n",
    "                    ax.plot(date, ndvi_val, 'o', color=color, markersize=12, \n",
    "                           label=f'{label} ({date.strftime(\"%b %d\")})')\n",
    "    \n",
    "    # Add base line\n",
    "    if 'bse_values' in metrics and not np.isnan(metrics['bse_values']):\n",
    "        ax.axhline(metrics['bse_values'], color='gray', linestyle=':', \n",
    "                  alpha=0.5, label=f\"Base NDVI ({metrics['bse_values']:.3f})\")\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('NDVI', fontsize=12)\n",
    "    ax.set_title(f'NDVI Time Series with Phenology - Pixel [{iy}, {ix}]', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Show metrics for several example pixels\n",
    "sample_pixels = [\n",
    "    (100, 200),\n",
    "    (300, 400), \n",
    "    (500, 700),\n",
    "    (200, 600)\n",
    "]\n",
    "\n",
    "print(\"\\nDisplaying phenology metrics for sample pixels...\")\n",
    "\n",
    "for iy, ix in sample_pixels:\n",
    "    fig = show_pixel_phenology(iy, ix)\n",
    "    plt.savefig(f'pixel_phenology_y{iy}_x{ix}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"‚úÖ Saved plot for pixel [{iy}, {ix}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Pixel Explorer\n",
    "def explore_pixel(iy, ix):\n",
    "    \"\"\"Quick interactive function to explore any pixel\"\"\"\n",
    "    return show_pixel_phenology(iy, ix)\n",
    "\n",
    "# Example usage:\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERACTIVE PIXEL EXPLORER\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTo explore any pixel, use:\")\n",
    "print(\"  explore_pixel(y_index, x_index)\")\n",
    "print(\"\\nExamples:\")\n",
    "print(\"  explore_pixel(335, 446)  # Center of the region\")\n",
    "print(\"  explore_pixel(100, 200)  # Northwest area\")\n",
    "print(\"  explore_pixel(500, 700)  # Southeast area\")\n",
    "print(\"\\nValid ranges:\")\n",
    "print(f\"  y_index: 0 to {phenology_metrics.dims['y']-1}\")\n",
    "print(f\"  x_index: 0 to {phenology_metrics.dims['x']-1}\")\n",
    "\n",
    "# Or create a CSV with ALL pixels' phenology metrics\n",
    "print(\"\\n\\nCreating comprehensive phenology CSV for ALL pixels...\")\n",
    "\n",
    "all_phenology_data = []\n",
    "for iy in range(phenology_metrics.dims['y']):\n",
    "    if iy % 100 == 0:\n",
    "        print(f\"  Processing row {iy}/{phenology_metrics.dims['y']}...\")\n",
    "    \n",
    "    for ix in range(phenology_metrics.dims['x']):\n",
    "        pixel_data = {\n",
    "            'y_idx': iy,\n",
    "            'x_idx': ix,\n",
    "            'lat': phenology_metrics.y.values[iy],\n",
    "            'lon': phenology_metrics.x.values[ix]\n",
    "        }\n",
    "        \n",
    "        # Add all metrics\n",
    "        for var in phenology_metrics.data_vars:\n",
    "            pixel_data[var] = phenology_metrics[var].values[iy, ix]\n",
    "        \n",
    "        # Only include pixels with at least some phenology data\n",
    "        if not np.isnan(pixel_data.get('pos_times', np.nan)):\n",
    "            all_phenology_data.append(pixel_data)\n",
    "\n",
    "phenology_df = pd.DataFrame(all_phenology_data)\n",
    "\n",
    "# Convert day-of-year to actual dates\n",
    "base_date = pd.Timestamp('2024-01-01')\n",
    "for col in ['sos_times', 'pos_times', 'eos_times', 'vos_times', 'mos_times']:\n",
    "    if col in phenology_df.columns:\n",
    "        phenology_df[f'{col}_date'] = phenology_df[col].apply(\n",
    "            lambda x: (base_date + pd.Timedelta(days=int(x))).strftime('%Y-%m-%d') \n",
    "            if not np.isnan(x) else None\n",
    "        )\n",
    "\n",
    "phenology_df.to_csv('demak_ALL_pixels_phenology_complete.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved complete phenology data for {len(phenology_df):,} pixels\")\n",
    "print(f\"   File: demak_ALL_pixels_phenology_complete.csv\")\n",
    "print(f\"\\nColumns: {list(phenology_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_pixel(100,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Extract and Visualize POS and EOS for All Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and Visualize POS and EOS for All Pixels\n",
    "print(\"=\" * 70)\n",
    "print(\"PEAK OF SEASON (POS) AND END OF SEASON (EOS) ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract POS and EOS data\n",
    "pos_doy = phenology_metrics['pos_times'].values if 'pos_times' in phenology_metrics else None\n",
    "eos_doy = phenology_metrics['eos_times'].values if 'eos_times' in phenology_metrics else None\n",
    "pos_values = phenology_metrics['pos_values'].values if 'pos_values' in phenology_metrics else None\n",
    "eos_values = phenology_metrics['eos_values'].values if 'eos_values' in phenology_metrics else None\n",
    "\n",
    "base_date = pd.Timestamp('2024-01-01')\n",
    "\n",
    "# POS Statistics\n",
    "if pos_doy is not None:\n",
    "    print(\"\\nüìä PEAK OF SEASON (POS) STATISTICS:\")\n",
    "    valid_pos = ~np.isnan(pos_doy) & (pos_doy > 0)\n",
    "    print(f\"  Pixels with POS: {valid_pos.sum():,} / {pos_doy.size:,} ({100*valid_pos.sum()/pos_doy.size:.1f}%)\")\n",
    "    \n",
    "    if valid_pos.sum() > 0:\n",
    "        print(f\"  Day Range: {np.nanmin(pos_doy[valid_pos]):.0f} to {np.nanmax(pos_doy[valid_pos]):.0f}\")\n",
    "        earliest_pos = base_date + pd.Timedelta(days=int(np.nanmin(pos_doy[valid_pos])))\n",
    "        latest_pos = base_date + pd.Timedelta(days=int(np.nanmax(pos_doy[valid_pos])))\n",
    "        median_pos = base_date + pd.Timedelta(days=int(np.nanmedian(pos_doy[valid_pos])))\n",
    "        \n",
    "        print(f\"  Date Range:\")\n",
    "        print(f\"    Earliest: {earliest_pos.strftime('%Y-%m-%d')} ({earliest_pos.strftime('%B %d')})\")\n",
    "        print(f\"    Median:   {median_pos.strftime('%Y-%m-%d')} ({median_pos.strftime('%B %d')})\")\n",
    "        print(f\"    Latest:   {latest_pos.strftime('%Y-%m-%d')} ({latest_pos.strftime('%B %d')})\")\n",
    "        \n",
    "        if pos_values is not None:\n",
    "            valid_pos_vals = pos_values[valid_pos]\n",
    "            print(f\"  Peak NDVI Range: [{np.nanmin(valid_pos_vals):.4f}, {np.nanmax(valid_pos_vals):.4f}]\")\n",
    "            print(f\"  Mean Peak NDVI: {np.nanmean(valid_pos_vals):.4f}\")\n",
    "\n",
    "# EOS Statistics\n",
    "if eos_doy is not None:\n",
    "    print(\"\\nüìä END OF SEASON (EOS) STATISTICS:\")\n",
    "    valid_eos = ~np.isnan(eos_doy) & (eos_doy > 0)\n",
    "    print(f\"  Pixels with EOS: {valid_eos.sum():,} / {eos_doy.size:,} ({100*valid_eos.sum()/eos_doy.size:.1f}%)\")\n",
    "    \n",
    "    if valid_eos.sum() > 0:\n",
    "        print(f\"  Day Range: {np.nanmin(eos_doy[valid_eos]):.0f} to {np.nanmax(eos_doy[valid_eos]):.0f}\")\n",
    "        earliest_eos = base_date + pd.Timedelta(days=int(np.nanmin(eos_doy[valid_eos])))\n",
    "        latest_eos = base_date + pd.Timedelta(days=int(np.nanmax(eos_doy[valid_eos])))\n",
    "        median_eos = base_date + pd.Timedelta(days=int(np.nanmedian(eos_doy[valid_eos])))\n",
    "        \n",
    "        print(f\"  Date Range:\")\n",
    "        print(f\"    Earliest: {earliest_eos.strftime('%Y-%m-%d')} ({earliest_eos.strftime('%B %d')})\")\n",
    "        print(f\"    Median:   {median_eos.strftime('%Y-%m-%d')} ({median_eos.strftime('%B %d')})\")\n",
    "        print(f\"    Latest:   {latest_eos.strftime('%Y-%m-%d')} ({latest_eos.strftime('%B %d')})\")\n",
    "        \n",
    "        if eos_values is not None:\n",
    "            valid_eos_vals = eos_values[valid_eos]\n",
    "            print(f\"  EOS NDVI Range: [{np.nanmin(valid_eos_vals):.4f}, {np.nanmax(valid_eos_vals):.4f}]\")\n",
    "            print(f\"  Mean EOS NDVI: {np.nanmean(valid_eos_vals):.4f}\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Row 1: POS Maps\n",
    "if pos_doy is not None:\n",
    "    # POS Day of Year\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "    im = ax.imshow(pos_doy, cmap='RdYlGn_r', vmin=0, vmax=365, aspect='auto')\n",
    "    ax.set_title('Peak of Season - Day of Year', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, label='Day of Year')\n",
    "    \n",
    "    # POS Values (Peak NDVI)\n",
    "    if pos_values is not None:\n",
    "        ax = fig.add_subplot(gs[0, 1])\n",
    "        pos_vals_masked = np.where(pos_values == 0, np.nan, pos_values)\n",
    "        im = ax.imshow(pos_vals_masked, cmap='YlGn', vmin=0, vmax=0.8, aspect='auto')\n",
    "        ax.set_title('Peak NDVI Values', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, label='NDVI')\n",
    "    \n",
    "    # POS Histogram\n",
    "    ax = fig.add_subplot(gs[0, 2])\n",
    "    valid_pos_data = pos_doy[valid_pos]\n",
    "    ax.hist(valid_pos_data, bins=50, color='red', alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Day of Year')\n",
    "    ax.set_ylabel('Number of Pixels')\n",
    "    ax.set_title('POS Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: EOS Maps\n",
    "if eos_doy is not None:\n",
    "    # EOS Day of Year\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    im = ax.imshow(eos_doy, cmap='YlOrRd', vmin=0, vmax=365, aspect='auto')\n",
    "    ax.set_title('End of Season - Day of Year', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, label='Day of Year')\n",
    "    \n",
    "    # EOS Values (NDVI at end)\n",
    "    if eos_values is not None:\n",
    "        ax = fig.add_subplot(gs[1, 1])\n",
    "        eos_vals_masked = np.where(eos_values == 0, np.nan, eos_values)\n",
    "        im = ax.imshow(eos_vals_masked, cmap='YlOrBr', vmin=0, vmax=0.6, aspect='auto')\n",
    "        ax.set_title('EOS NDVI Values', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, label='NDVI')\n",
    "    \n",
    "    # EOS Histogram\n",
    "    ax = fig.add_subplot(gs[1, 2])\n",
    "    valid_eos_data = eos_doy[valid_eos]\n",
    "    ax.hist(valid_eos_data, bins=50, color='orange', alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Day of Year')\n",
    "    ax.set_ylabel('Number of Pixels')\n",
    "    ax.set_title('EOS Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 3: Season Length and Amplitude\n",
    "if 'lios_values' in phenology_metrics:\n",
    "    ax = fig.add_subplot(gs[2, 0])\n",
    "    los = phenology_metrics['lios_values'].values\n",
    "    los_masked = np.where(los == 0, np.nan, los)\n",
    "    im = ax.imshow(los_masked, cmap='viridis', vmin=0, vmax=200, aspect='auto')\n",
    "    ax.set_title('Length of Season (Days)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, label='Days')\n",
    "\n",
    "if 'aos_values' in phenology_metrics:\n",
    "    ax = fig.add_subplot(gs[2, 1])\n",
    "    aos = phenology_metrics['aos_values'].values\n",
    "    aos_masked = np.where(aos == 0, np.nan, aos)\n",
    "    im = ax.imshow(aos_masked, cmap='RdYlGn', vmin=0, vmax=0.5, aspect='auto')\n",
    "    ax.set_title('Amplitude of Season', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, label='NDVI Amplitude')\n",
    "\n",
    "# Monthly distribution comparison\n",
    "ax = fig.add_subplot(gs[2, 2])\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "month_days = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 365]\n",
    "\n",
    "if pos_doy is not None and eos_doy is not None:\n",
    "    pos_months = []\n",
    "    eos_months = []\n",
    "    for m in range(12):\n",
    "        pos_count = ((valid_pos_data >= month_days[m]) & (valid_pos_data < month_days[m+1])).sum()\n",
    "        eos_count = ((valid_eos_data >= month_days[m]) & (valid_eos_data < month_days[m+1])).sum()\n",
    "        pos_months.append(pos_count)\n",
    "        eos_months.append(eos_count)\n",
    "    \n",
    "    x = np.arange(12)\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, pos_months, width, label='POS', color='red', alpha=0.7)\n",
    "    ax.bar(x + width/2, eos_months, width, label='EOS', color='orange', alpha=0.7)\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Number of Pixels')\n",
    "    ax.set_title('POS vs EOS by Month', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(month_names, rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 4: Sample time series\n",
    "sample_pixels = [(100, 200), (300, 400), (500, 700)]\n",
    "for idx, (iy, ix) in enumerate(sample_pixels):\n",
    "    ax = fig.add_subplot(gs[3, idx])\n",
    "    \n",
    "    # Get time series\n",
    "    ndvi_ts = fused_full['S2ndvi_DL'][:, iy, ix].values\n",
    "    dates = pd.to_datetime(fused_full.t.values)\n",
    "    \n",
    "    ax.plot(dates, ndvi_ts, 'o-', linewidth=2, markersize=4, color='green', alpha=0.7)\n",
    "    \n",
    "    # Mark POS\n",
    "    if pos_doy is not None and not np.isnan(pos_doy[iy, ix]) and pos_doy[iy, ix] > 0:\n",
    "        pos_date = base_date + pd.Timedelta(days=int(pos_doy[iy, ix]))\n",
    "        ax.axvline(pos_date, color='red', linestyle='--', linewidth=2, alpha=0.7, label='POS')\n",
    "    \n",
    "    # Mark EOS\n",
    "    if eos_doy is not None and not np.isnan(eos_doy[iy, ix]) and eos_doy[iy, ix] > 0:\n",
    "        eos_date = base_date + pd.Timedelta(days=int(eos_doy[iy, ix]))\n",
    "        ax.axvline(eos_date, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='EOS')\n",
    "    \n",
    "    ax.set_title(f'Pixel [{iy}, {ix}]', fontsize=10)\n",
    "    ax.set_xlabel('Date', fontsize=9)\n",
    "    ax.set_ylabel('NDVI', fontsize=9)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "plt.savefig('demak_POS_EOS_comprehensive.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved comprehensive POS/EOS analysis to demak_POS_EOS_comprehensive.png\")\n",
    "\n",
    "# Export POS and EOS data to CSV\n",
    "print(\"\\nExporting POS and EOS data to CSV...\")\n",
    "phenology_export = []\n",
    "\n",
    "for iy in range(phenology_metrics.dims['y']):\n",
    "    if iy % 100 == 0:\n",
    "        print(f\"  Processing row {iy}/{phenology_metrics.dims['y']}...\")\n",
    "    \n",
    "    for ix in range(phenology_metrics.dims['x']):\n",
    "        has_data = False\n",
    "        pixel_data = {\n",
    "            'y_idx': iy,\n",
    "            'x_idx': ix,\n",
    "            'lat': phenology_metrics.y.values[iy],\n",
    "            'lon': phenology_metrics.x.values[ix]\n",
    "        }\n",
    "        \n",
    "        # POS data\n",
    "        if pos_doy is not None and not np.isnan(pos_doy[iy, ix]) and pos_doy[iy, ix] > 0:\n",
    "            pos_day = int(pos_doy[iy, ix])\n",
    "            pos_date = base_date + pd.Timedelta(days=pos_day)\n",
    "            pixel_data['pos_doy'] = pos_day\n",
    "            pixel_data['pos_date'] = pos_date.strftime('%Y-%m-%d')\n",
    "            pixel_data['pos_month'] = pos_date.month\n",
    "            pixel_data['pos_month_name'] = pos_date.strftime('%B')\n",
    "            if pos_values is not None:\n",
    "                pixel_data['pos_ndvi'] = pos_values[iy, ix]\n",
    "            has_data = True\n",
    "        \n",
    "        # EOS data\n",
    "        if eos_doy is not None and not np.isnan(eos_doy[iy, ix]) and eos_doy[iy, ix] > 0:\n",
    "            eos_day = int(eos_doy[iy, ix])\n",
    "            eos_date = base_date + pd.Timedelta(days=eos_day)\n",
    "            pixel_data['eos_doy'] = eos_day\n",
    "            pixel_data['eos_date'] = eos_date.strftime('%Y-%m-%d')\n",
    "            pixel_data['eos_month'] = eos_date.month\n",
    "            pixel_data['eos_month_name'] = eos_date.strftime('%B')\n",
    "            if eos_values is not None:\n",
    "                pixel_data['eos_ndvi'] = eos_values[iy, ix]\n",
    "            has_data = True\n",
    "        \n",
    "        # Add season length if available\n",
    "        if 'lios_values' in phenology_metrics:\n",
    "            los_val = phenology_metrics['lios_values'].values[iy, ix]\n",
    "            if not np.isnan(los_val) and los_val > 0:\n",
    "                pixel_data['season_length_days'] = los_val\n",
    "        \n",
    "        if has_data:\n",
    "            phenology_export.append(pixel_data)\n",
    "\n",
    "phenology_export_df = pd.DataFrame(phenology_export)\n",
    "phenology_export_df.to_csv('demak_POS_EOS_all_pixels.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved POS/EOS data for {len(phenology_export_df):,} pixels\")\n",
    "print(f\"   File: demak_POS_EOS_all_pixels.csv\")\n",
    "print(f\"\\nüìã Columns: {list(phenology_export_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results and Per-Pixel Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways and Real-World Applications\n",
    "\n",
    "### ‚úÖ What You Accomplished:\n",
    "\n",
    "1. **Per-Pixel Phenological Analysis**: Extracted Start/End of Season information for **every single pixel** in your study area\n",
    "2. **Multi-Sensor Data Fusion**: Combined S1 (weather-independent) and S2 (vegetation-sensitive) data using MOGPR\n",
    "3. **Gap Filling Enhancement**: Used SAR data to fill optical data gaps, improving temporal completeness\n",
    "4. **Spatial Coverage**: Achieved complete spatial coverage at your input resolution (e.g., 10m pixels)\n",
    "5. **Multiple Export Formats**: Generated data suitable for GIS, statistical analysis, and agricultural applications\n",
    "\n",
    "### üå± Per-Pixel Information Available:\n",
    "\n",
    "For **every pixel** in your study area, you now have:\n",
    "- **Start of Season (SOS) timing**: Exact day of year when vegetation growth begins\n",
    "- **End of Season (EOS) timing**: Exact day of year when senescence starts  \n",
    "- **NDVI values at SOS/EOS**: Vegetation greenness levels at critical phenological stages\n",
    "- **Growing season length**: Duration of the growing season in days\n",
    "- **Spatial patterns**: Complete mapping of phenological variations across the landscape\n",
    "\n",
    "### üéØ Real-World Applications:\n",
    "\n",
    "#### üåæ **Agricultural Applications**\n",
    "- **Crop monitoring**: Track planting and harvest timing across different fields\n",
    "- **Yield prediction**: Use SOS timing as input to crop growth models\n",
    "- **Irrigation management**: Optimize water application based on crop phenological stage\n",
    "- **Insurance claims**: Verify crop development stages for agricultural insurance\n",
    "\n",
    "#### üó∫Ô∏è **Spatial Analysis & Mapping**\n",
    "- **Land cover classification**: Use phenological patterns to distinguish crop types\n",
    "- **Climate change studies**: Analyze shifts in growing season timing over multiple years\n",
    "- **Ecosystem monitoring**: Track vegetation response to environmental changes\n",
    "- **Conservation planning**: Identify areas with unique phenological characteristics\n",
    "\n",
    "#### üìä **Research & Monitoring**\n",
    "- **Validation studies**: Compare satellite-derived SOS with ground observations\n",
    "- **Model calibration**: Use per-pixel data to calibrate ecosystem and crop models\n",
    "- **Trend analysis**: Analyze spatial patterns of phenological changes\n",
    "- **Multi-scale studies**: Aggregate pixel-level data to field, regional, or global scales\n",
    "\n",
    "### üöÄ Scaling to Larger Areas:\n",
    "\n",
    "For **operational large-scale applications**:\n",
    "\n",
    "1. **Microsoft Planetary Computer Workflow**: Use the `MPC_Data_Prep_Fixed.ipynb` notebook to:\n",
    "   - Extract data for entire countries or continents\n",
    "   - Process multiple years of data efficiently\n",
    "   - Handle cloud computing for massive datasets\n",
    "\n",
    "2. **OpenEO Integration**: Scale processing using cloud infrastructure:\n",
    "   - Process continental-scale datasets\n",
    "   - Automate annual phenology monitoring\n",
    "   - Integrate with existing operational systems\n",
    "\n",
    "3. **Temporal Analysis**: Extend to multi-year analysis:\n",
    "   - Track phenological trends over decades\n",
    "   - Analyze climate change impacts on growing seasons\n",
    "   - Generate long-term agricultural statistics\n",
    "\n",
    "### üí° Key Benefits of MOGPR Fusion:\n",
    "\n",
    "- **Weather Independence**: SAR data fills gaps during cloudy periods\n",
    "- **Enhanced Accuracy**: Cross-sensor correlations improve phenological detection\n",
    "- **Temporal Consistency**: More complete time series for robust analysis\n",
    "- **Uncertainty Quantification**: MOGPR provides confidence estimates for results\n",
    "\n",
    "### üîÑ Workflow Integration:\n",
    "\n",
    "This analysis integrates seamlessly with:\n",
    "- **GIS software** (QGIS, ArcGIS) for spatial analysis and mapping\n",
    "- **Statistical software** (R, Python, MATLAB) for advanced analytics\n",
    "- **Agricultural management systems** for operational crop monitoring\n",
    "- **Climate monitoring networks** for environmental assessments\n",
    "\n",
    "**The result**: You now have comprehensive, per-pixel Start of Season information ready for any agricultural, environmental, or research application!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Multi-Season Analysis for Tropical Agriculture (Indonesia Case)\n",
    "\n",
    "### üåæ Indonesian Agricultural Calendar:\n",
    "- **First planting season**: November - January (following year)\n",
    "- **Second planting season**: April - May  \n",
    "- **Potential third season**: August - September (some areas)\n",
    "\n",
    "This section demonstrates how to detect **multiple planting seasons per pixel** and classify areas by cropping intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted Flexible Multi-Season Detection for Your Demak Dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"üáÆüá© FLEXIBLE MULTI-SEASON DETECTION FOR DEMAK, INDONESIA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def detect_flexible_seasons_indonesia(ndvi_timeseries, time_coords, \n",
    "                                      season_duration_range=(70, 120),\n",
    "                                      min_peak_prominence=0.08, \n",
    "                                      min_peak_distance=40):\n",
    "    \"\"\"\n",
    "    Flexible multi-season detection for Indonesian agriculture\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüåæ Detecting agricultural seasons...\")\n",
    "    print(f\"   Season duration: {season_duration_range[0]}-{season_duration_range[1]} days\")\n",
    "    print(f\"   Min peak prominence: {min_peak_prominence}\")\n",
    "    print(f\"   Min peak distance: {min_peak_distance} days\")\n",
    "    \n",
    "    nt, ny, nx = ndvi_timeseries.shape\n",
    "    \n",
    "    # Initialize result arrays\n",
    "    season_count = np.zeros((ny, nx))\n",
    "    all_seasons = np.full((ny, nx, 6), np.nan)  # Max 3 seasons √ó 2 (SOS, EOS)\n",
    "    season_types = np.zeros((ny, nx, 3))\n",
    "    cropping_intensity = np.zeros((ny, nx))\n",
    "    \n",
    "    # Convert time to day of year\n",
    "    day_of_year = np.array([d.dayofyear for d in time_coords])\n",
    "    \n",
    "    print(f\"\\nProcessing {ny} x {nx} = {ny*nx:,} pixels...\")\n",
    "    \n",
    "    processed_pixels = 0\n",
    "    \n",
    "    for y in range(ny):\n",
    "        if y % 50 == 0:\n",
    "            print(f\"  Row {y}/{ny} ({100*y/ny:.1f}%)\")\n",
    "            \n",
    "        for x in range(nx):\n",
    "            pixel_ndvi = ndvi_timeseries[:, y, x]\n",
    "            \n",
    "            # Skip if too much missing data\n",
    "            if np.isnan(pixel_ndvi).sum() > len(pixel_ndvi) * 0.5:\n",
    "                continue\n",
    "                \n",
    "            valid_mask = ~np.isnan(pixel_ndvi)\n",
    "            if valid_mask.sum() < 15:\n",
    "                continue\n",
    "                \n",
    "            # Interpolate missing values\n",
    "            pixel_ndvi_interp = np.interp(np.arange(len(pixel_ndvi)), \n",
    "                                        np.where(valid_mask)[0], \n",
    "                                        pixel_ndvi[valid_mask])\n",
    "            \n",
    "            # Light smoothing\n",
    "            pixel_ndvi_smooth = gaussian_filter1d(pixel_ndvi_interp, sigma=1.2)\n",
    "            \n",
    "            # Find peaks\n",
    "            peaks, properties = find_peaks(pixel_ndvi_smooth, \n",
    "                                         prominence=min_peak_prominence,\n",
    "                                         distance=min_peak_distance,\n",
    "                                         height=np.nanmean(pixel_ndvi_smooth) + np.nanstd(pixel_ndvi_smooth) * 0.3)\n",
    "            \n",
    "            if len(peaks) == 0:\n",
    "                continue\n",
    "                \n",
    "            processed_pixels += 1\n",
    "            \n",
    "            # Classify peaks into seasons\n",
    "            peak_days = day_of_year[peaks]\n",
    "            peak_values = pixel_ndvi_smooth[peaks]\n",
    "            \n",
    "            detected_seasons = []\n",
    "            \n",
    "            for peak_pos, peak_day, peak_val in zip(peaks, peak_days, peak_values):\n",
    "                season_type = classify_peak_season(peak_day)\n",
    "                \n",
    "                if season_type > 0:\n",
    "                    sos_pos, eos_pos, season_length = find_flexible_season_boundaries(\n",
    "                        pixel_ndvi_smooth, peak_pos, season_duration_range)\n",
    "                    \n",
    "                    if sos_pos is not None and eos_pos is not None:\n",
    "                        sos_day = day_of_year[sos_pos]\n",
    "                        eos_day = day_of_year[eos_pos]\n",
    "                        \n",
    "                        # Check for duplicates\n",
    "                        is_new_season = True\n",
    "                        for existing in detected_seasons:\n",
    "                            if existing['type'] == season_type:\n",
    "                                if peak_val > existing['peak_value']:\n",
    "                                    detected_seasons.remove(existing)\n",
    "                                else:\n",
    "                                    is_new_season = False\n",
    "                                break\n",
    "                        \n",
    "                        if is_new_season:\n",
    "                            detected_seasons.append({\n",
    "                                'type': season_type,\n",
    "                                'sos_day': sos_day,\n",
    "                                'eos_day': eos_day,\n",
    "                                'peak_day': peak_day,\n",
    "                                'peak_value': peak_val,\n",
    "                                'season_length': season_length\n",
    "                            })\n",
    "            \n",
    "            # Sort and store\n",
    "            detected_seasons.sort(key=lambda x: x['type'])\n",
    "            num_seasons = len(detected_seasons)\n",
    "            season_count[y, x] = num_seasons\n",
    "            cropping_intensity[y, x] = num_seasons\n",
    "            \n",
    "            for i, season in enumerate(detected_seasons[:3]):\n",
    "                all_seasons[y, x, i*2] = season['sos_day']\n",
    "                all_seasons[y, x, i*2+1] = season['eos_day']\n",
    "                season_types[y, x, season['type']-1] = 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processed {processed_pixels:,} pixels with valid data\")\n",
    "    \n",
    "    return {\n",
    "        'season_count': season_count,\n",
    "        'cropping_intensity': cropping_intensity,\n",
    "        'season_types': season_types,\n",
    "        'all_seasons': all_seasons,\n",
    "        'processed_pixels': processed_pixels\n",
    "    }\n",
    "\n",
    "def classify_peak_season(day_of_year):\n",
    "    \"\"\"Classify Indonesian agricultural season\"\"\"\n",
    "    if day_of_year >= 305 or day_of_year <= 90:\n",
    "        return 1  # Nov-Mar\n",
    "    elif 90 < day_of_year <= 180:\n",
    "        return 2  # Apr-Jun\n",
    "    elif 180 < day_of_year <= 270:\n",
    "        return 3  # Jul-Sep\n",
    "    else:\n",
    "        return 0  # Transition\n",
    "\n",
    "def find_flexible_season_boundaries(ndvi_smooth, peak_pos, duration_range):\n",
    "    \"\"\"Find season boundaries\"\"\"\n",
    "    min_duration, max_duration = duration_range\n",
    "    \n",
    "    # Find SOS (valley before peak)\n",
    "    sos_search_window = min(peak_pos, max_duration // 2)\n",
    "    sos_start = max(0, peak_pos - sos_search_window)\n",
    "    pre_peak = ndvi_smooth[sos_start:peak_pos]\n",
    "    \n",
    "    if len(pre_peak) > 5:\n",
    "        sos_pos = sos_start + np.argmin(pre_peak)\n",
    "    else:\n",
    "        sos_pos = max(0, peak_pos - min_duration // 2)\n",
    "    \n",
    "    # Find EOS (valley after peak)\n",
    "    eos_search_window = min(len(ndvi_smooth) - peak_pos, max_duration // 2)\n",
    "    eos_end = min(len(ndvi_smooth), peak_pos + eos_search_window)\n",
    "    post_peak = ndvi_smooth[peak_pos:eos_end]\n",
    "    \n",
    "    if len(post_peak) > 5:\n",
    "        eos_pos = peak_pos + np.argmin(post_peak)\n",
    "    else:\n",
    "        eos_pos = min(len(ndvi_smooth) - 1, peak_pos + min_duration // 2)\n",
    "    \n",
    "    season_length = eos_pos - sos_pos\n",
    "    \n",
    "    if min_duration <= season_length <= max_duration:\n",
    "        return sos_pos, eos_pos, season_length\n",
    "    else:\n",
    "        return None, None, 0\n",
    "\n",
    "# Run detection on your dataset\n",
    "print(\"\\nAdapting to regional variations:\")\n",
    "print(\"‚Ä¢ Season 1: Nov-Mar (flexible 3-4 month duration)\")\n",
    "print(\"‚Ä¢ Season 2: Apr-Jun (flexible timing)\")\n",
    "print(\"‚Ä¢ Season 3: Jul-Sep (optional, region-dependent)\")\n",
    "\n",
    "flexible_results = detect_flexible_seasons_indonesia(\n",
    "    fused_full['S2ndvi_DL'].values,  # Use your corrected NDVI\n",
    "    pd.to_datetime(fused_full.t.values),\n",
    "    season_duration_range=(70, 130),\n",
    "    min_peak_prominence=0.06,\n",
    "    min_peak_distance=35\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä INDONESIAN AGRICULTURAL PATTERNS DETECTED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "season_counts = flexible_results['season_count']\n",
    "season_types = flexible_results['season_types']\n",
    "valid_pixels = flexible_results['processed_pixels']\n",
    "total_pixels = season_counts.size\n",
    "\n",
    "print(f\"\\nüåç Spatial Coverage:\")\n",
    "print(f\"   Total pixels: {total_pixels:,}\")\n",
    "print(f\"   Agricultural pixels: {valid_pixels:,} ({100*valid_pixels/total_pixels:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüåæ Cropping Intensity (Seasons per Year):\")\n",
    "for intensity in [1, 2, 3]:\n",
    "    count = (season_counts == intensity).sum()\n",
    "    pct = 100 * count / valid_pixels if valid_pixels > 0 else 0\n",
    "    print(f\"   {intensity} season(s): {count:,} pixels ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìÖ Seasonal Patterns:\")\n",
    "season_names = ['Nov-Mar (Season 1)', 'Apr-Jun (Season 2)', 'Jul-Sep (Season 3)']\n",
    "for i, name in enumerate(season_names):\n",
    "    count = (season_types[:, :, i] == 1).sum()\n",
    "    pct = 100 * count / valid_pixels if valid_pixels > 0 else 0\n",
    "    print(f\"   {name}: {count:,} pixels ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüó∫Ô∏è  Regional Cropping Patterns:\")\n",
    "print(f\"   Rain-fed areas (1 season): {(season_counts == 1).sum():,} pixels\")\n",
    "print(f\"   Irrigated areas (2 seasons): {(season_counts == 2).sum():,} pixels\")\n",
    "print(f\"   Intensive areas (3 seasons): {(season_counts == 3).sum():,} pixels\")\n",
    "\n",
    "print(\"\\n‚úÖ Multi-season detection completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check why seasons aren't being detected\n",
    "print(\"=\" * 70)\n",
    "print(\"DEBUGGING SEASON DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check a sample pixel to see what's happening\n",
    "def debug_pixel_seasons(iy, ix):\n",
    "    \"\"\"Debug season detection for a single pixel\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Debugging Pixel [{iy}, {ix}]\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    pixel_ndvi = fused_full['S2ndvi_DL'][:, iy, ix].values\n",
    "    time_coords = pd.to_datetime(fused_full.t.values)\n",
    "    day_of_year = np.array([d.dayofyear for d in time_coords])\n",
    "    \n",
    "    print(f\"NDVI range: [{np.nanmin(pixel_ndvi):.4f}, {np.nanmax(pixel_ndvi):.4f}]\")\n",
    "    print(f\"Valid data: {(~np.isnan(pixel_ndvi)).sum()}/{len(pixel_ndvi)}\")\n",
    "    \n",
    "    # Interpolate and smooth\n",
    "    valid_mask = ~np.isnan(pixel_ndvi)\n",
    "    if valid_mask.sum() < 15:\n",
    "        print(\"‚ùå Not enough valid data\")\n",
    "        return\n",
    "    \n",
    "    pixel_ndvi_interp = np.interp(np.arange(len(pixel_ndvi)), \n",
    "                                  np.where(valid_mask)[0], \n",
    "                                  pixel_ndvi[valid_mask])\n",
    "    \n",
    "    pixel_ndvi_smooth = gaussian_filter1d(pixel_ndvi_interp, sigma=1.2)\n",
    "    \n",
    "    # Find peaks with detailed info\n",
    "    threshold = np.nanmean(pixel_ndvi_smooth) + np.nanstd(pixel_ndvi_smooth) * 0.3\n",
    "    print(f\"\\nPeak detection threshold: {threshold:.4f}\")\n",
    "    \n",
    "    peaks, properties = find_peaks(pixel_ndvi_smooth, \n",
    "                                   prominence=0.06,\n",
    "                                   distance=35,\n",
    "                                   height=threshold)\n",
    "    \n",
    "    print(f\"\\nFound {len(peaks)} peaks:\")\n",
    "    for i, peak_pos in enumerate(peaks):\n",
    "        peak_day = day_of_year[peak_pos]\n",
    "        peak_val = pixel_ndvi_smooth[peak_pos]\n",
    "        peak_date = time_coords[peak_pos]\n",
    "        season_type = classify_peak_season(peak_day)\n",
    "        \n",
    "        print(f\"  Peak {i+1}: Day {peak_day:3d} ({peak_date.strftime('%Y-%m-%d')})\")\n",
    "        print(f\"          NDVI: {peak_val:.4f}, Season type: {season_type}\")\n",
    "    \n",
    "    # Plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    ax.plot(time_coords, pixel_ndvi, 'o', alpha=0.5, label='Original', markersize=4)\n",
    "    ax.plot(time_coords, pixel_ndvi_smooth, '-', linewidth=2, label='Smoothed')\n",
    "    ax.axhline(threshold, color='red', linestyle='--', alpha=0.5, label='Peak threshold')\n",
    "    \n",
    "    # Mark peaks\n",
    "    for peak_pos in peaks:\n",
    "        ax.plot(time_coords[peak_pos], pixel_ndvi_smooth[peak_pos], \n",
    "               'r*', markersize=15, label='Peak' if peak_pos == peaks[0] else '')\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('NDVI')\n",
    "    ax.set_title(f'Debug: Pixel [{iy}, {ix}]')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Debug a few sample pixels\n",
    "sample_pixels = [(100, 200), (300, 400), (500, 700)]\n",
    "for iy, ix in sample_pixels:\n",
    "    debug_pixel_seasons(iy, ix)\n",
    "\n",
    "# Check the actual season_count values\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SEASON COUNT DISTRIBUTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "season_counts = flexible_results['season_count']\n",
    "unique_counts, pixel_counts = np.unique(season_counts, return_counts=True)\n",
    "\n",
    "print(\"\\nSeason count distribution:\")\n",
    "for count, pixels in zip(unique_counts, pixel_counts):\n",
    "    pct = 100 * pixels / season_counts.size\n",
    "    print(f\"  {int(count)} seasons: {pixels:,} pixels ({pct:.1f}%)\")\n",
    "\n",
    "# Check if season_count is being set but season_types isn't\n",
    "print(\"\\nChecking season_types array:\")\n",
    "season_types = flexible_results['season_types']\n",
    "print(f\"Season types shape: {season_types.shape}\")\n",
    "print(f\"Season 1 pixels: {(season_types[:,:,0] == 1).sum()}\")\n",
    "print(f\"Season 2 pixels: {(season_types[:,:,1] == 1).sum()}\")\n",
    "print(f\"Season 3 pixels: {(season_types[:,:,2] == 1).sum()}\")\n",
    "\n",
    "# Check all_seasons\n",
    "all_seasons = flexible_results['all_seasons']\n",
    "print(f\"\\nAll seasons shape: {all_seasons.shape}\")\n",
    "print(f\"Non-NaN values: {(~np.isnan(all_seasons)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced debug to check season boundary detection\n",
    "def debug_season_boundaries(iy, ix):\n",
    "    \"\"\"Debug the full season detection pipeline\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FULL DEBUG: Pixel [{iy}, {ix}]\")\n",
    "    print('='*70)\n",
    "    \n",
    "    pixel_ndvi = fused_full['S2ndvi_DL'][:, iy, ix].values\n",
    "    time_coords = pd.to_datetime(fused_full.t.values)\n",
    "    day_of_year = np.array([d.dayofyear for d in time_coords])\n",
    "    \n",
    "    # Prepare data\n",
    "    valid_mask = ~np.isnan(pixel_ndvi)\n",
    "    pixel_ndvi_interp = np.interp(np.arange(len(pixel_ndvi)), \n",
    "                                  np.where(valid_mask)[0], \n",
    "                                  pixel_ndvi[valid_mask])\n",
    "    pixel_ndvi_smooth = gaussian_filter1d(pixel_ndvi_interp, sigma=1.2)\n",
    "    \n",
    "    # Find peaks\n",
    "    threshold = np.nanmean(pixel_ndvi_smooth) + np.nanstd(pixel_ndvi_smooth) * 0.3\n",
    "    peaks, properties = find_peaks(pixel_ndvi_smooth, \n",
    "                                   prominence=0.06,\n",
    "                                   distance=35,\n",
    "                                   height=threshold)\n",
    "    \n",
    "    print(f\"\\nFound {len(peaks)} peak(s)\")\n",
    "    \n",
    "    detected_seasons = []\n",
    "    season_duration_range = (70, 130)\n",
    "    \n",
    "    for i, peak_pos in enumerate(peaks):\n",
    "        peak_day = day_of_year[peak_pos]\n",
    "        peak_val = pixel_ndvi_smooth[peak_pos]\n",
    "        peak_date = time_coords[peak_pos]\n",
    "        season_type = classify_peak_season(peak_day)\n",
    "        \n",
    "        print(f\"\\n--- Peak {i+1} ---\")\n",
    "        print(f\"Position: {peak_pos}, Day: {peak_day}, Date: {peak_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"NDVI: {peak_val:.4f}, Season type: {season_type}\")\n",
    "        \n",
    "        if season_type > 0:\n",
    "            # Try to find boundaries\n",
    "            sos_pos, eos_pos, season_length = find_flexible_season_boundaries(\n",
    "                pixel_ndvi_smooth, peak_pos, season_duration_range)\n",
    "            \n",
    "            print(f\"Boundary detection:\")\n",
    "            print(f\"  SOS position: {sos_pos}\")\n",
    "            print(f\"  EOS position: {eos_pos}\")\n",
    "            print(f\"  Season length: {season_length} days\")\n",
    "            \n",
    "            if sos_pos is not None and eos_pos is not None:\n",
    "                sos_day = day_of_year[sos_pos]\n",
    "                eos_day = day_of_year[eos_pos]\n",
    "                sos_date = time_coords[sos_pos]\n",
    "                eos_date = time_coords[eos_pos]\n",
    "                \n",
    "                print(f\"  SOS: Day {sos_day} ({sos_date.strftime('%Y-%m-%d')})\")\n",
    "                print(f\"  EOS: Day {eos_day} ({eos_date.strftime('%Y-%m-%d')})\")\n",
    "                print(f\"  ‚úÖ Valid season detected!\")\n",
    "                \n",
    "                detected_seasons.append({\n",
    "                    'type': season_type,\n",
    "                    'sos_day': sos_day,\n",
    "                    'eos_day': eos_day,\n",
    "                    'peak_day': peak_day,\n",
    "                    'peak_value': peak_val,\n",
    "                    'season_length': season_length\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  ‚ùå Season boundaries invalid!\")\n",
    "                print(f\"     Duration {season_length} not in range {season_duration_range}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Season type = 0 (unclassified)\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FINAL RESULT: {len(detected_seasons)} season(s) detected\")\n",
    "    if detected_seasons:\n",
    "        for i, season in enumerate(detected_seasons):\n",
    "            print(f\"  Season {i+1}: Type {season['type']}, Length {season['season_length']} days\")\n",
    "    print('='*70)\n",
    "    \n",
    "    return detected_seasons\n",
    "\n",
    "# Test the full pipeline\n",
    "print(\"\\nüîç Testing full season detection pipeline...\")\n",
    "for iy, ix in [(100, 200), (300, 400), (500, 700)]:\n",
    "    seasons = debug_season_boundaries(iy, ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Season detection adapted for 12-day temporal resolution\n",
    "def find_flexible_season_boundaries_adapted(ndvi_smooth, peak_pos, duration_range, temporal_step=12):\n",
    "    \"\"\"\n",
    "    Find season boundaries adapted for coarse temporal resolution\n",
    "    \n",
    "    Parameters:\n",
    "    - temporal_step: days between observations (12 for your data)\n",
    "    \"\"\"\n",
    "    min_duration_days, max_duration_days = duration_range\n",
    "    \n",
    "    # Convert day ranges to number of time steps\n",
    "    min_steps = max(2, min_duration_days // temporal_step // 2)  # Half season before/after peak\n",
    "    max_steps = max_duration_days // temporal_step // 2\n",
    "    \n",
    "    # Find SOS (valley before peak)\n",
    "    sos_search_steps = min(peak_pos, max_steps)\n",
    "    sos_start = max(0, peak_pos - sos_search_steps)\n",
    "    \n",
    "    if peak_pos > sos_start:\n",
    "        pre_peak = ndvi_smooth[sos_start:peak_pos]\n",
    "        if len(pre_peak) > 1:\n",
    "            sos_pos = sos_start + np.argmin(pre_peak)\n",
    "        else:\n",
    "            sos_pos = sos_start\n",
    "    else:\n",
    "        sos_pos = 0\n",
    "    \n",
    "    # Find EOS (valley after peak)\n",
    "    eos_search_steps = min(len(ndvi_smooth) - peak_pos - 1, max_steps)\n",
    "    eos_end = min(len(ndvi_smooth), peak_pos + eos_search_steps + 1)\n",
    "    \n",
    "    if eos_end > peak_pos + 1:\n",
    "        post_peak = ndvi_smooth[peak_pos+1:eos_end]\n",
    "        if len(post_peak) > 1:\n",
    "            eos_pos = peak_pos + 1 + np.argmin(post_peak)\n",
    "        else:\n",
    "            eos_pos = eos_end - 1\n",
    "    else:\n",
    "        eos_pos = min(len(ndvi_smooth) - 1, peak_pos + min_steps)\n",
    "    \n",
    "    # Calculate season length in time steps\n",
    "    season_steps = eos_pos - sos_pos\n",
    "    season_length_days = season_steps * temporal_step\n",
    "    \n",
    "    # Validate (more lenient for coarse temporal resolution)\n",
    "    min_required_steps = 2  # At least 2 observations for SOS and EOS\n",
    "    \n",
    "    if season_steps >= min_required_steps and season_length_days >= min_duration_days * 0.5:\n",
    "        return sos_pos, eos_pos, season_length_days\n",
    "    else:\n",
    "        return None, None, 0\n",
    "\n",
    "# Re-run season detection with adapted function\n",
    "print(\"=\" * 70)\n",
    "print(\"üáÆüá© FIXED MULTI-SEASON DETECTION (12-DAY RESOLUTION)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def detect_seasons_12day_resolution(ndvi_timeseries, time_coords, \n",
    "                                   season_duration_range=(70, 120),\n",
    "                                   min_peak_prominence=0.06, \n",
    "                                   min_peak_distance=2):  # 2 time steps = 24 days\n",
    "    \"\"\"Season detection adapted for 12-day temporal resolution\"\"\"\n",
    "    \n",
    "    nt, ny, nx = ndvi_timeseries.shape\n",
    "    \n",
    "    season_count = np.zeros((ny, nx))\n",
    "    all_seasons = np.full((ny, nx, 6), np.nan)\n",
    "    season_types = np.zeros((ny, nx, 3))\n",
    "    cropping_intensity = np.zeros((ny, nx))\n",
    "    \n",
    "    day_of_year = np.array([d.dayofyear for d in time_coords])\n",
    "    \n",
    "    print(f\"Processing {ny} x {nx} pixels...\")\n",
    "    processed_pixels = 0\n",
    "    \n",
    "    for y in range(ny):\n",
    "        if y % 50 == 0:\n",
    "            print(f\"  Row {y}/{ny}\")\n",
    "            \n",
    "        for x in range(nx):\n",
    "            pixel_ndvi = ndvi_timeseries[:, y, x]\n",
    "            \n",
    "            if np.isnan(pixel_ndvi).sum() > len(pixel_ndvi) * 0.5:\n",
    "                continue\n",
    "                \n",
    "            valid_mask = ~np.isnan(pixel_ndvi)\n",
    "            if valid_mask.sum() < 10:\n",
    "                continue\n",
    "                \n",
    "            # Interpolate\n",
    "            pixel_ndvi_interp = np.interp(np.arange(len(pixel_ndvi)), \n",
    "                                        np.where(valid_mask)[0], \n",
    "                                        pixel_ndvi[valid_mask])\n",
    "            \n",
    "            # Light smoothing\n",
    "            pixel_ndvi_smooth = gaussian_filter1d(pixel_ndvi_interp, sigma=1.0)\n",
    "            \n",
    "            # Find peaks (distance in time steps, not days)\n",
    "            threshold = np.nanmean(pixel_ndvi_smooth) + np.nanstd(pixel_ndvi_smooth) * 0.2\n",
    "            peaks, properties = find_peaks(pixel_ndvi_smooth, \n",
    "                                         prominence=min_peak_prominence,\n",
    "                                         distance=min_peak_distance,\n",
    "                                         height=threshold)\n",
    "            \n",
    "            if len(peaks) == 0:\n",
    "                continue\n",
    "                \n",
    "            processed_pixels += 1\n",
    "            \n",
    "            peak_days = day_of_year[peaks]\n",
    "            peak_values = pixel_ndvi_smooth[peaks]\n",
    "            \n",
    "            detected_seasons = []\n",
    "            \n",
    "            for peak_pos, peak_day, peak_val in zip(peaks, peak_days, peak_values):\n",
    "                season_type = classify_peak_season(peak_day)\n",
    "                \n",
    "                if season_type > 0:\n",
    "                    # Use adapted boundary detection\n",
    "                    sos_pos, eos_pos, season_length = find_flexible_season_boundaries_adapted(\n",
    "                        pixel_ndvi_smooth, peak_pos, season_duration_range, temporal_step=12)\n",
    "                    \n",
    "                    if sos_pos is not None and eos_pos is not None:\n",
    "                        sos_day = day_of_year[sos_pos]\n",
    "                        eos_day = day_of_year[eos_pos]\n",
    "                        \n",
    "                        # Check for duplicates\n",
    "                        is_new_season = True\n",
    "                        for existing in detected_seasons:\n",
    "                            if existing['type'] == season_type:\n",
    "                                if peak_val > existing['peak_value']:\n",
    "                                    detected_seasons.remove(existing)\n",
    "                                else:\n",
    "                                    is_new_season = False\n",
    "                                break\n",
    "                        \n",
    "                        if is_new_season:\n",
    "                            detected_seasons.append({\n",
    "                                'type': season_type,\n",
    "                                'sos_day': sos_day,\n",
    "                                'eos_day': eos_day,\n",
    "                                'peak_day': peak_day,\n",
    "                                'peak_value': peak_val,\n",
    "                                'season_length': season_length\n",
    "                            })\n",
    "            \n",
    "            # Store results\n",
    "            detected_seasons.sort(key=lambda x: x['type'])\n",
    "            num_seasons = len(detected_seasons)\n",
    "            season_count[y, x] = num_seasons\n",
    "            cropping_intensity[y, x] = num_seasons\n",
    "            \n",
    "            for i, season in enumerate(detected_seasons[:3]):\n",
    "                all_seasons[y, x, i*2] = season['sos_day']\n",
    "                all_seasons[y, x, i*2+1] = season['eos_day']\n",
    "                season_types[y, x, season['type']-1] = 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processed {processed_pixels:,} agricultural pixels\")\n",
    "    \n",
    "    return {\n",
    "        'season_count': season_count,\n",
    "        'cropping_intensity': cropping_intensity,\n",
    "        'season_types': season_types,\n",
    "        'all_seasons': all_seasons,\n",
    "        'processed_pixels': processed_pixels\n",
    "    }\n",
    "\n",
    "# Run adapted detection\n",
    "print(\"\\nAdapted for 12-day temporal resolution (62 observations)\")\n",
    "print(\"Detecting seasons with flexible boundaries...\\n\")\n",
    "\n",
    "fixed_results = detect_seasons_12day_resolution(\n",
    "    fused_full['S2ndvi_DL'].values,\n",
    "    pd.to_datetime(fused_full.t.values),\n",
    "    season_duration_range=(60, 150),  # More flexible for coarse resolution\n",
    "    min_peak_prominence=0.05,\n",
    "    min_peak_distance=2  # ~24 days minimum between peaks\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä MULTI-SEASON DETECTION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "season_counts = fixed_results['season_count']\n",
    "season_types = fixed_results['season_types']\n",
    "valid_pixels = fixed_results['processed_pixels']\n",
    "\n",
    "print(f\"\\nüåç Spatial Coverage:\")\n",
    "print(f\"   Agricultural pixels: {valid_pixels:,}\")\n",
    "\n",
    "print(f\"\\nüåæ Cropping Intensity:\")\n",
    "for intensity in [1, 2, 3]:\n",
    "    count = (season_counts == intensity).sum()\n",
    "    pct = 100 * count / valid_pixels if valid_pixels > 0 else 0\n",
    "    print(f\"   {intensity} season(s): {count:,} pixels ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìÖ Seasonal Patterns:\")\n",
    "season_names = ['Nov-Mar', 'Apr-Jun', 'Jul-Sep']\n",
    "for i, name in enumerate(season_names):\n",
    "    count = (season_types[:, :, i] == 1).sum()\n",
    "    pct = 100 * count / valid_pixels if valid_pixels > 0 else 0\n",
    "    print(f\"   {name}: {count:,} pixels ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Season detection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize Indonesian Multi-Season Agriculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Indonesian Multi-Season Agricultural Patterns\n",
    "print(\"=\" * 70)\n",
    "print(\"üó∫Ô∏è  VISUALIZING INDONESIAN AGRICULTURAL PATTERNS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Extract results\n",
    "season_counts = fixed_results['season_count']\n",
    "season_types = fixed_results['season_types']\n",
    "all_seasons = fixed_results['all_seasons']\n",
    "cropping_intensity = fixed_results['cropping_intensity']\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(22, 14))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "fig.suptitle('Demak, Indonesia - Multi-Season Agricultural Analysis (Nov 2024 - Nov 2025)', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "# 1. Cropping Intensity Map\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "im1 = ax1.imshow(cropping_intensity, cmap='RdYlGn', vmin=0, vmax=3, aspect='auto')\n",
    "ax1.set_title('üåæ Cropping Intensity\\n(Seasons per Year)', fontweight='bold', fontsize=13)\n",
    "ax1.set_xlabel('X (893 pixels)')\n",
    "ax1.set_ylabel('Y (671 pixels)')\n",
    "cbar1 = plt.colorbar(im1, ax=ax1, fraction=0.046)\n",
    "cbar1.set_label('Seasons/Year', fontsize=11)\n",
    "cbar1.set_ticks([0, 1, 2, 3])\n",
    "cbar1.set_ticklabels(['None', '1\\n(Rain-fed)', '2\\n(Irrigated)', '3\\n(Intensive)'], fontsize=9)\n",
    "\n",
    "# Add statistics\n",
    "agri_pixels = (season_counts > 0).sum()\n",
    "ax1.text(0.02, 0.98, f'Agricultural:\\n{agri_pixels:,} pixels', \n",
    "        transform=ax1.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 2. Season 1 (Nov-Mar) Distribution\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "season1_map = season_types[:, :, 0]\n",
    "im2 = ax2.imshow(season1_map, cmap='Blues', vmin=0, vmax=1, aspect='auto')\n",
    "ax2.set_title('üå± Season 1: Nov-Mar\\n(Main Wet Season)', fontweight='bold', fontsize=13)\n",
    "ax2.set_xlabel('X (893 pixels)')\n",
    "ax2.set_ylabel('Y (671 pixels)')\n",
    "cbar2 = plt.colorbar(im2, ax=ax2, fraction=0.046)\n",
    "cbar2.set_ticks([0, 1])\n",
    "cbar2.set_ticklabels(['Absent', 'Present'], fontsize=10)\n",
    "\n",
    "s1_count = (season1_map == 1).sum()\n",
    "ax2.text(0.02, 0.98, f'{s1_count:,} pixels\\n({100*s1_count/agri_pixels:.1f}% of agri.)', \n",
    "        transform=ax2.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 3. Season 2 (Apr-Jun) Distribution  \n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "season2_map = season_types[:, :, 1]\n",
    "im3 = ax3.imshow(season2_map, cmap='Greens', vmin=0, vmax=1, aspect='auto')\n",
    "ax3.set_title('üåæ Season 2: Apr-Jun\\n(Dry Season Crop)', fontweight='bold', fontsize=13)\n",
    "ax3.set_xlabel('X (893 pixels)')\n",
    "ax3.set_ylabel('Y (671 pixels)')\n",
    "cbar3 = plt.colorbar(im3, ax=ax3, fraction=0.046)\n",
    "cbar3.set_ticks([0, 1])\n",
    "cbar3.set_ticklabels(['Absent', 'Present'], fontsize=10)\n",
    "\n",
    "s2_count = (season2_map == 1).sum()\n",
    "ax3.text(0.02, 0.98, f'{s2_count:,} pixels\\n({100*s2_count/agri_pixels:.1f}% of agri.)', \n",
    "        transform=ax3.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 4. Season 3 (Jul-Sep) Distribution\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "season3_map = season_types[:, :, 2]\n",
    "im4 = ax4.imshow(season3_map, cmap='Oranges', vmin=0, vmax=1, aspect='auto')\n",
    "ax4.set_title('üåª Season 3: Jul-Sep\\n(Optional Intensive)', fontweight='bold', fontsize=13)\n",
    "ax4.set_xlabel('X (893 pixels)')\n",
    "ax4.set_ylabel('Y (671 pixels)')\n",
    "cbar4 = plt.colorbar(im4, ax=ax4, fraction=0.046)\n",
    "cbar4.set_ticks([0, 1])\n",
    "cbar4.set_ticklabels(['Absent', 'Present'], fontsize=10)\n",
    "\n",
    "s3_count = (season3_map == 1).sum()\n",
    "ax4.text(0.02, 0.98, f'{s3_count:,} pixels\\n({100*s3_count/agri_pixels:.1f}% of agri.)', \n",
    "        transform=ax4.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 5. Agricultural Land Use Classification\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "agri_mask = (season_counts > 0).astype(int)\n",
    "im5 = ax5.imshow(agri_mask, cmap='RdYlBu_r', vmin=0, vmax=1, aspect='auto')\n",
    "ax5.set_title('üó∫Ô∏è  Agricultural vs Non-Agricultural\\nLand Classification', fontweight='bold', fontsize=13)\n",
    "ax5.set_xlabel('X (893 pixels)')\n",
    "ax5.set_ylabel('Y (671 pixels)')\n",
    "cbar5 = plt.colorbar(im5, ax=ax5, fraction=0.046)\n",
    "cbar5.set_ticks([0, 1])\n",
    "cbar5.set_ticklabels(['Non-Agri', 'Agricultural'], fontsize=10)\n",
    "\n",
    "total_pixels = season_counts.size\n",
    "agri_pct = 100 * agri_pixels / total_pixels\n",
    "ax5.text(0.02, 0.98, f'Coverage:\\n{agri_pct:.1f}% agricultural', \n",
    "        transform=ax5.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 6. Season 1 Start Timing (handling Nov-Mar year boundary)\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "season1_sos = all_seasons[:, :, 0]  # First season SOS\n",
    "season1_sos_masked = np.where(season_types[:, :, 0] == 1, season1_sos, np.nan)\n",
    "\n",
    "# Convert to continuous scale for Nov-Mar\n",
    "season1_sos_continuous = season1_sos_masked.copy()\n",
    "for y in range(season1_sos_continuous.shape[0]):\n",
    "    for x in range(season1_sos_continuous.shape[1]):\n",
    "        if not np.isnan(season1_sos_continuous[y, x]):\n",
    "            day = season1_sos_continuous[y, x]\n",
    "            if day >= 305:  # Nov-Dec\n",
    "                season1_sos_continuous[y, x] = (day - 305) + 1\n",
    "            elif day <= 90:  # Jan-Mar\n",
    "                season1_sos_continuous[y, x] = day + 61\n",
    "\n",
    "im6 = ax6.imshow(season1_sos_continuous, cmap='twilight_shifted', vmin=1, vmax=151, aspect='auto')\n",
    "ax6.set_title('üìÖ Season 1 Planting Timing\\n(Nov-Mar)', fontweight='bold', fontsize=13)\n",
    "ax6.set_xlabel('X (893 pixels)')\n",
    "ax6.set_ylabel('Y (671 pixels)')\n",
    "cbar6 = plt.colorbar(im6, ax=ax6, fraction=0.046)\n",
    "cbar6.set_label('Planting Date', fontsize=11)\n",
    "cbar6.set_ticks([1, 32, 62, 92, 121, 151])\n",
    "cbar6.set_ticklabels(['Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'End-Mar'], fontsize=9)\n",
    "\n",
    "# 7. Cropping System Pie Chart\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "system_counts = [\n",
    "    (season_counts == 1).sum(),\n",
    "    (season_counts == 2).sum(),\n",
    "    (season_counts == 3).sum()\n",
    "]\n",
    "labels = ['Single Season\\n(Rain-fed)', 'Double Season\\n(Irrigated)', 'Triple Season\\n(Intensive)']\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "wedges, texts, autotexts = ax7.pie(system_counts, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "                                    startangle=90, textprops={'fontsize': 10})\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "    autotext.set_fontweight('bold')\n",
    "ax7.set_title('ü•ß Cropping System Distribution', fontweight='bold', fontsize=13)\n",
    "\n",
    "# 8. Season Popularity Bar Chart\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "season_names = ['Nov-Mar\\n(Season 1)', 'Apr-Jun\\n(Season 2)', 'Jul-Sep\\n(Season 3)']\n",
    "season_pixels = [(season_types[:, :, i] == 1).sum() for i in range(3)]\n",
    "season_pcts = [100 * p / agri_pixels for p in season_pixels]\n",
    "\n",
    "bars = ax8.bar(range(3), season_pcts, color=['#4472C4', '#70AD47', '#FFC000'], alpha=0.8, edgecolor='black')\n",
    "ax8.set_ylabel('% of Agricultural Pixels', fontsize=11, fontweight='bold')\n",
    "ax8.set_title('üìä Seasonal Pattern Frequency', fontweight='bold', fontsize=13)\n",
    "ax8.set_xticks(range(3))\n",
    "ax8.set_xticklabels(season_names, fontsize=10)\n",
    "ax8.grid(True, alpha=0.3, axis='y')\n",
    "ax8.set_ylim([0, max(season_pcts) * 1.15])\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, pct, count) in enumerate(zip(bars, season_pcts, season_pixels)):\n",
    "    ax8.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "            f'{pct:.1f}%\\n({count:,}px)', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 9. Statistics Summary\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "ax9.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "üìà AGRICULTURAL STATISTICS SUMMARY\n",
    "\n",
    "üåç Spatial Coverage:\n",
    "   ‚Ä¢ Total pixels: {total_pixels:,}\n",
    "   ‚Ä¢ Agricultural: {agri_pixels:,} ({agri_pct:.1f}%)\n",
    "   ‚Ä¢ Non-agricultural: {total_pixels - agri_pixels:,}\n",
    "\n",
    "üåæ Cropping Intensity:\n",
    "   ‚Ä¢ Rain-fed (1 season): {system_counts[0]:,} ({100*system_counts[0]/agri_pixels:.1f}%)\n",
    "   ‚Ä¢ Irrigated (2 seasons): {system_counts[1]:,} ({100*system_counts[1]/agri_pixels:.1f}%)\n",
    "   ‚Ä¢ Intensive (3 seasons): {system_counts[2]:,} ({100*system_counts[2]/agri_pixels:.1f}%)\n",
    "\n",
    "üìÖ Seasonal Patterns:\n",
    "   ‚Ä¢ Season 1 (Nov-Mar): {season_pixels[0]:,} pixels\n",
    "   ‚Ä¢ Season 2 (Apr-Jun): {season_pixels[1]:,} pixels\n",
    "   ‚Ä¢ Season 3 (Jul-Sep): {season_pixels[2]:,} pixels\n",
    "\n",
    "üéØ Key Insights:\n",
    "   ‚Ä¢ Dominant season: {season_names[np.argmax(season_pcts)]}\n",
    "   ‚Ä¢ Main system: {labels[np.argmax(system_counts)]}\n",
    "   ‚Ä¢ Study period: Nov 2024 - Nov 2025\n",
    "\"\"\"\n",
    "\n",
    "ax9.text(0.05, 0.95, summary_text, transform=ax9.transAxes, fontsize=10,\n",
    "        verticalalignment='top', family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.savefig('demak_indonesia_multiseason_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization saved to: demak_indonesia_multiseason_analysis.png\")\n",
    "\n",
    "# Detailed Statistics Output\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìà DETAILED AGRICULTURAL STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Season timing analysis\n",
    "print(\"\\n‚è∞ Season Timing Analysis:\")\n",
    "for season_idx, season_name in enumerate(['Season 1 (Nov-Mar)', 'Season 2 (Apr-Jun)', 'Season 3 (Jul-Sep)']):\n",
    "    season_sos = all_seasons[:, :, season_idx*2]\n",
    "    season_eos = all_seasons[:, :, season_idx*2+1]\n",
    "    season_present = season_types[:, :, season_idx] == 1\n",
    "    \n",
    "    if season_present.sum() > 0:\n",
    "        valid_sos = season_sos[season_present & ~np.isnan(season_sos)]\n",
    "        valid_eos = season_eos[season_present & ~np.isnan(season_eos)]\n",
    "        \n",
    "        if len(valid_sos) > 0:\n",
    "            # Convert to dates\n",
    "            base_date = pd.Timestamp('2024-01-01')\n",
    "            avg_sos_date = base_date + pd.Timedelta(days=int(np.mean(valid_sos)))\n",
    "            \n",
    "            print(f\"\\n  {season_name}:\")\n",
    "            print(f\"    Pixels: {season_present.sum():,}\")\n",
    "            print(f\"    Avg start: Day {np.mean(valid_sos):.0f} ({avg_sos_date.strftime('%b %d')})\")\n",
    "            print(f\"    Start range: Day {np.min(valid_sos):.0f} - {np.max(valid_sos):.0f}\")\n",
    "            print(f\"    Std deviation: ¬±{np.std(valid_sos):.1f} days\")\n",
    "            \n",
    "            if len(valid_eos) > 0:\n",
    "                season_lengths = valid_eos - valid_sos\n",
    "                print(f\"    Avg length: {np.mean(season_lengths):.0f} days\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Indonesian multi-season analysis completed!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.transform import from_bounds\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Step 1: Load the shapefile\n",
    "shapefile_path = '/home/unika_sianturi/work/FuseTS/data/klambu-glapan.shp'\n",
    "paddy_gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "print(f\"Shapefile loaded: {len(paddy_gdf)} features\")\n",
    "print(f\"CRS: {paddy_gdf.crs}\")\n",
    "print(f\"Columns: {paddy_gdf.columns.tolist()}\")\n",
    "print(f\"Bounds: {paddy_gdf.total_bounds}\")\n",
    "\n",
    "# Step 2: Set CRS for your data if it doesn't have one\n",
    "# Assuming your data is also in UTM 49N (EPSG:32649) or UTM 49S (EPSG:32749)\n",
    "# Determine which hemisphere based on your Demak location (Java, Indonesia = Southern Hemisphere)\n",
    "target_crs = \"EPSG:32749\"  # UTM Zone 49S for Southern Hemisphere\n",
    "\n",
    "# If shapefile is in different UTM zone or needs confirmation\n",
    "if paddy_gdf.crs is None:\n",
    "    print(\"Warning: Shapefile has no CRS, setting to UTM 49S\")\n",
    "    paddy_gdf = paddy_gdf.set_crs(target_crs)\n",
    "elif 'UTM zone 49N' in str(paddy_gdf.crs) or '32649' in str(paddy_gdf.crs):\n",
    "    print(\"Shapefile is in UTM 49N, reprojecting to UTM 49S for Java, Indonesia\")\n",
    "    target_crs = \"EPSG:32749\"\n",
    "    paddy_gdf = paddy_gdf.to_crs(target_crs)\n",
    "elif 'UTM zone 49S' in str(paddy_gdf.crs) or '32749' in str(paddy_gdf.crs):\n",
    "    print(\"Shapefile is already in UTM 49S\")\n",
    "    target_crs = str(paddy_gdf.crs)\n",
    "else:\n",
    "    print(f\"Shapefile CRS: {paddy_gdf.crs}\")\n",
    "    # Reproject to UTM 49S if needed\n",
    "    paddy_gdf = paddy_gdf.to_crs(target_crs)\n",
    "\n",
    "print(f\"Using CRS: {target_crs}\")\n",
    "\n",
    "# Step 3: Set CRS for your xarray data\n",
    "# Add CRS information to your data if it doesn't have it\n",
    "if not hasattr(fused_full, 'rio') or fused_full.rio.crs is None:\n",
    "    print(\"Adding CRS to dataset...\")\n",
    "    # You need to set the CRS based on your data's coordinate system\n",
    "    fused_full = fused_full.rio.write_crs(target_crs)\n",
    "    \n",
    "    # If your x,y coordinates are in pixel indices, you need actual UTM coordinates\n",
    "    # Let's check what your coordinates look like\n",
    "    print(f\"X coordinate range: {fused_full.x.min().values} to {fused_full.x.max().values}\")\n",
    "    print(f\"Y coordinate range: {fused_full.y.min().values} to {fused_full.y.max().values}\")\n",
    "\n",
    "# Step 4: Create the mask using the shapefile bounds and resolution\n",
    "ny, nx = fused_full.dims['y'], fused_full.dims['x']\n",
    "\n",
    "# Get spatial bounds from your data\n",
    "x_coords = fused_full.coords['x'].values\n",
    "y_coords = fused_full.coords['y'].values\n",
    "\n",
    "# Calculate pixel resolution\n",
    "x_res = abs((x_coords[-1] - x_coords[0]) / (len(x_coords) - 1))\n",
    "y_res = abs((y_coords[-1] - y_coords[0]) / (len(y_coords) - 1))\n",
    "\n",
    "print(f\"\\nData grid info:\")\n",
    "print(f\"Grid size: {ny} rows x {nx} columns\")\n",
    "print(f\"X resolution: {x_res:.2f} meters\")\n",
    "print(f\"Y resolution: {y_res:.2f} meters\")\n",
    "\n",
    "# Create transform for rasterization\n",
    "# The bounds should be in the format: (left, bottom, right, top)\n",
    "transform = from_bounds(\n",
    "    west=x_coords[0] - x_res/2,\n",
    "    south=y_coords[-1] - y_res/2,  # Assuming y is descending\n",
    "    east=x_coords[-1] + x_res/2,\n",
    "    north=y_coords[0] + y_res/2,\n",
    "    width=nx,\n",
    "    height=ny\n",
    ")\n",
    "\n",
    "print(f\"\\nTransform created:\")\n",
    "print(transform)\n",
    "\n",
    "# Step 5: Rasterize the shapefile\n",
    "try:\n",
    "    paddy_mask_raw = ~geometry_mask(\n",
    "        paddy_gdf.geometry,\n",
    "        out_shape=(ny, nx),\n",
    "        transform=transform,\n",
    "        invert=False\n",
    "    )\n",
    "    print(f\"\\nMask created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating mask: {e}\")\n",
    "    print(\"\\nTrying alternative method...\")\n",
    "    \n",
    "    # Alternative: Manual rasterization using shapely\n",
    "    from shapely.geometry import Point\n",
    "    import pandas as pd\n",
    "    \n",
    "    paddy_mask_raw = np.zeros((ny, nx), dtype=bool)\n",
    "    \n",
    "    # Create grid of points\n",
    "    print(\"Creating spatial grid (this may take a moment)...\")\n",
    "    for i in range(ny):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing row {i}/{ny}...\")\n",
    "        for j in range(nx):\n",
    "            # Get point coordinates\n",
    "            x_coord = x_coords[j]\n",
    "            y_coord = y_coords[i]\n",
    "            \n",
    "            # Create point\n",
    "            point = Point(x_coord, y_coord)\n",
    "            \n",
    "            # Check if point is within any polygon\n",
    "            paddy_mask_raw[i, j] = paddy_gdf.contains(point).any()\n",
    "    \n",
    "    print(\"Manual rasterization complete!\")\n",
    "\n",
    "# Step 6: Convert to xarray DataArray\n",
    "paddy_mask = xr.DataArray(\n",
    "    paddy_mask_raw,\n",
    "    dims=['y', 'x'],\n",
    "    coords={'y': fused_full.coords['y'], 'x': fused_full.coords['x']},\n",
    "    name='paddy_mask'\n",
    ")\n",
    "\n",
    "print(f\"\\nPaddy mask statistics:\")\n",
    "print(f\"Total pixels: {paddy_mask.size:,}\")\n",
    "print(f\"Paddy pixels: {paddy_mask.sum().values:,} ({100*paddy_mask.sum().values/paddy_mask.size:.1f}%)\")\n",
    "print(f\"Mask shape: {paddy_mask.shape}\")\n",
    "\n",
    "# Step 7: Visualize to verify\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Plot 1: Mean NDVI\n",
    "ndvi_mean = fused_full['S2ndvi_DL'].mean(dim='t')\n",
    "im1 = axes[0].imshow(ndvi_mean, cmap='RdYlGn', vmin=-0.2, vmax=0.8)\n",
    "axes[0].set_title('Mean NDVI', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0], label='NDVI')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot 2: Paddy mask\n",
    "im2 = axes[1].imshow(paddy_mask, cmap='YlGn')\n",
    "axes[1].set_title(f'Paddy Mask from Shapefile\\n{paddy_mask.sum().values:,} pixels ({100*paddy_mask.sum().values/paddy_mask.size:.1f}%)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1], label='Paddy area')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Plot 3: NDVI masked to paddy areas only\n",
    "ndvi_masked = ndvi_mean.where(paddy_mask)\n",
    "im3 = axes[2].imshow(ndvi_masked, cmap='RdYlGn', vmin=-0.2, vmax=0.8)\n",
    "axes[2].set_title('Mean NDVI (Paddy Areas Only)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[2], label='NDVI')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('paddy_mask_validation_utm49.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMask validation plot saved as 'paddy_mask_validation_utm49.png'\")\n",
    "print(\"\\nIf the mask looks incorrect, please share:\")\n",
    "print(\"1. The coordinate ranges of your x and y\")\n",
    "print(\"2. Whether coordinates are in UTM meters or pixel indices\")\n",
    "print(\"3. The shapefile bounds shown above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Export Indonesian Multi-Season Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Run masked season detection\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running multi-season detection on paddy areas only...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "paddy_results = detect_seasons_12day_resolution_masked(\n",
    "    fused_full,\n",
    "    paddy_mask,\n",
    "    prominence_threshold=0.05,\n",
    "    distance=3\n",
    ")\n",
    "\n",
    "# Step 7: Analyze results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PADDY AREA MULTI-SEASON ANALYSIS RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "total_paddy = paddy_mask.sum().values\n",
    "print(f\"Total paddy pixels (from shapefile): {total_paddy:,}\")\n",
    "\n",
    "num_seasons_valid = paddy_results['num_seasons'][~np.isnan(paddy_results['num_seasons'])]\n",
    "print(f\"Paddy pixels with detected seasons: {len(num_seasons_valid):,} ({100*len(num_seasons_valid)/total_paddy:.1f}%)\")\n",
    "\n",
    "print(f\"\\nCropping Intensity Distribution:\")\n",
    "for n in [1, 2, 3]:\n",
    "    count = (paddy_results['num_seasons'] == n).sum()\n",
    "    pct_total = 100 * count / total_paddy\n",
    "    pct_detected = 100 * count / len(num_seasons_valid) if len(num_seasons_valid) > 0 else 0\n",
    "    print(f\"  {n} season(s): {count:,} pixels ({pct_total:.1f}% of total, {pct_detected:.1f}% of detected)\")\n",
    "\n",
    "# Step 8: Enhanced visualization for paddy areas only\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
    "\n",
    "# 1. Paddy mask from shapefile\n",
    "im1 = axes[0, 0].imshow(paddy_mask, cmap='YlGn')\n",
    "axes[0, 0].set_title(f'Paddy Areas (Shapefile)\\n{total_paddy:,} pixels', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# 2. Cropping intensity (paddy only)\n",
    "cropping_intensity = paddy_results['num_seasons'].copy()\n",
    "cropping_intensity[~paddy_mask] = np.nan\n",
    "im2 = axes[0, 1].imshow(cropping_intensity, cmap='RdYlGn', vmin=0, vmax=3)\n",
    "axes[0, 1].set_title('Cropping Intensity\\n(Seasons per Year)', fontsize=12, fontweight='bold')\n",
    "cbar2 = plt.colorbar(im2, ax=axes[0, 1])\n",
    "cbar2.set_label('Number of seasons', rotation=270, labelpad=20)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# 3. Mean NDVI for paddy areas\n",
    "ndvi_mean = fused_full['S2ndvi_DL'].mean(dim='t')\n",
    "ndvi_mean_masked = ndvi_mean.where(paddy_mask)\n",
    "im3 = axes[0, 2].imshow(ndvi_mean_masked, cmap='RdYlGn', vmin=-0.2, vmax=0.8)\n",
    "axes[0, 2].set_title('Mean NDVI\\n(Paddy Areas)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[0, 2], label='NDVI')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# 4-6. Season type distribution (only within paddy mask)\n",
    "season_names = ['Season 1 (Nov-Mar)', 'Season 2 (Apr-Jun)', 'Season 3 (Jul-Sep)']\n",
    "for i in range(3):\n",
    "    season_map = (paddy_results['season_types'][:, :, i] == (i+1)).astype(float)\n",
    "    season_map[~paddy_mask] = np.nan\n",
    "    \n",
    "    im = axes[1, i].imshow(season_map, cmap='Blues', vmin=0, vmax=1)\n",
    "    count = np.nansum(season_map)\n",
    "    axes[1, i].set_title(f'{season_names[i]}\\n{int(count):,} pixels', fontsize=11, fontweight='bold')\n",
    "    plt.colorbar(im, ax=axes[1, i])\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# 7. Season 1 planting timing (SOS)\n",
    "sos_s1 = paddy_results['season_sos'][:, :, 0].copy()\n",
    "sos_s1[~paddy_mask] = np.nan\n",
    "sos_s1_adjusted = np.where(sos_s1 > 180, sos_s1 - 365, sos_s1)\n",
    "im7 = axes[2, 0].imshow(sos_s1_adjusted, cmap='twilight', vmin=-180, vmax=180)\n",
    "axes[2, 0].set_title('Season 1 Start Date\\n(Day of Year)', fontsize=12, fontweight='bold')\n",
    "cbar7 = plt.colorbar(im7, ax=axes[2, 0])\n",
    "cbar7.set_label('Day of year', rotation=270, labelpad=20)\n",
    "axes[2, 0].axis('off')\n",
    "\n",
    "# 8. Pie chart of cropping intensity\n",
    "season_counts = {\n",
    "    '1 season': (paddy_results['num_seasons'] == 1).sum(),\n",
    "    '2 seasons': (paddy_results['num_seasons'] == 2).sum(),\n",
    "    '3 seasons': (paddy_results['num_seasons'] == 3).sum()\n",
    "}\n",
    "\n",
    "axes[2, 1].pie(\n",
    "    season_counts.values(),\n",
    "    labels=season_counts.keys(),\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['#90EE90', '#FFD700', '#FF6347'],\n",
    "    startangle=90\n",
    ")\n",
    "axes[2, 1].set_title('Cropping Intensity Distribution\\n(Paddy Areas Only)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 9. Summary statistics table\n",
    "summary_data = [\n",
    "    ['Total Paddy Pixels', f'{total_paddy:,}'],\n",
    "    ['Pixels with Seasons', f'{len(num_seasons_valid):,}'],\n",
    "    ['1 Season', f'{season_counts[\"1 season\"]:,}'],\n",
    "    ['2 Seasons', f'{season_counts[\"2 seasons\"]:,}'],\n",
    "    ['3 Seasons', f'{season_counts[\"3 seasons\"]:,}'],\n",
    "    ['Avg Seasons/Pixel', f'{num_seasons_valid.mean():.2f}' if len(num_seasons_valid) > 0 else 'N/A']\n",
    "]\n",
    "\n",
    "axes[2, 2].axis('tight')\n",
    "axes[2, 2].axis('off')\n",
    "table = axes[2, 2].table(\n",
    "    cellText=summary_data,\n",
    "    colLabels=['Metric', 'Value'],\n",
    "    cellLoc='left',\n",
    "    loc='center',\n",
    "    colWidths=[0.6, 0.4]\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "axes[2, 2].set_title('Summary Statistics', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Multi-Season Paddy Rice Analysis - Demak, Indonesia\\n(Based on Klambu-Glapan Shapefile)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('paddy_multiseason_analysis_masked.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'paddy_multiseason_analysis_masked.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Calculate Planting Indices and Agricultural Patterns\n",
    "\n",
    "### üåæ Advanced Agricultural Analytics\n",
    "\n",
    "This section derives comprehensive planting indices from the multi-season analysis, providing insights for:\n",
    "- **Agricultural Planning**: Understanding planting patterns and timing\n",
    "- **Irrigation Management**: Identifying irrigation-dependent areas\n",
    "- **Food Security**: Assessing agricultural intensification\n",
    "- **Policy Making**: Supporting subsidy and insurance programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåæ CALCULATING PLANTING INDICES AND AGRICULTURAL PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PLANTING PATTERN CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def classify_planting_pattern(season_types):\n",
    "    \"\"\"\n",
    "    Classify agricultural planting patterns based on seasonal presence\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Pattern codes:\n",
    "        0: Non-agricultural\n",
    "        1: Single season (Nov-Mar only) - Traditional rain-fed\n",
    "        2: Double season (Nov-Mar + Apr-Jun) - Standard irrigated\n",
    "        3: Triple season (All three) - Intensive agriculture\n",
    "        4: Dry season focus (Apr-Jun + Jul-Sep) - Alternative pattern\n",
    "        5: Mid-year cropping (Apr-Jun only)\n",
    "        6: Late season only (Jul-Sep only)\n",
    "        7: Other combinations\n",
    "    \"\"\"\n",
    "    patterns = np.zeros(season_types.shape[:2], dtype=int)\n",
    "    \n",
    "    s1 = season_types[:, :, 0]  # Nov-Mar\n",
    "    s2 = season_types[:, :, 1]  # Apr-Jun\n",
    "    s3 = season_types[:, :, 2]  # Jul-Sep\n",
    "    \n",
    "    # Pattern 1: Single season (Nov-Mar) - Traditional rain-fed\n",
    "    patterns[(s1 == 1) & (s2 == 0) & (s3 == 0)] = 1\n",
    "    \n",
    "    # Pattern 2: Double season (Nov-Mar + Apr-Jun) - Standard irrigated\n",
    "    patterns[(s1 == 1) & (s2 == 1) & (s3 == 0)] = 2\n",
    "    \n",
    "    # Pattern 3: Triple season - Intensive agriculture\n",
    "    patterns[(s1 == 1) & (s2 == 1) & (s3 == 1)] = 3\n",
    "    \n",
    "    # Pattern 4: Dry season focus (skip main season)\n",
    "    patterns[(s1 == 0) & (s2 == 1) & (s3 == 1)] = 4\n",
    "    \n",
    "    # Pattern 5: Mid-year only\n",
    "    patterns[(s1 == 0) & (s2 == 1) & (s3 == 0)] = 5\n",
    "    \n",
    "    # Pattern 6: Late season only\n",
    "    patterns[(s1 == 0) & (s2 == 0) & (s3 == 1)] = 6\n",
    "    \n",
    "    # Pattern 7: Other combinations\n",
    "    patterns[(s1 + s2 + s3 > 0) & (patterns == 0)] = 7\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "planting_patterns = classify_planting_pattern(season_types)\n",
    "\n",
    "print(\"‚úÖ Planting Pattern Classification Complete\")\n",
    "print(f\"\\nPattern Distribution:\")\n",
    "pattern_names = {\n",
    "    0: 'Non-agricultural',\n",
    "    1: 'Single season (rain-fed)',\n",
    "    2: 'Double season (irrigated)',\n",
    "    3: 'Triple season (intensive)',\n",
    "    4: 'Dry season focus',\n",
    "    5: 'Mid-year only',\n",
    "    6: 'Late season only',\n",
    "    7: 'Other patterns'\n",
    "}\n",
    "\n",
    "for pattern_code, pattern_name in pattern_names.items():\n",
    "    pixel_count = (planting_patterns == pattern_code).sum()\n",
    "    if pixel_count > 0:\n",
    "        percentage = pixel_count / planting_patterns.size * 100\n",
    "        print(f\"  Pattern {pattern_code} ({pattern_name}): {pixel_count:,} pixels ({percentage:.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. IRRIGATION DEPENDENCY INDEX\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_irrigation_index(cropping_intensity, season_types):\n",
    "    \"\"\"\n",
    "    Calculate irrigation dependency index (0-1)\n",
    "    \n",
    "    Based on:\n",
    "    - Number of cropping seasons (more seasons = more irrigation)\n",
    "    - Presence of dry season crops (Apr-Jun, Jul-Sep)\n",
    "    - Triple cropping capability (requires consistent irrigation)\n",
    "    \"\"\"\n",
    "    irrigation_index = np.zeros(cropping_intensity.shape, dtype=float)\n",
    "    \n",
    "    # Rain-fed areas (single season, only Nov-Mar)\n",
    "    irrigation_index[cropping_intensity == 1] = 0.0\n",
    "    \n",
    "    # Partial irrigation (double cropping)\n",
    "    irrigation_index[cropping_intensity == 2] = 0.5\n",
    "    \n",
    "    # Full irrigation (triple cropping)\n",
    "    irrigation_index[cropping_intensity == 3] = 1.0\n",
    "    \n",
    "    # Adjust based on dry season presence\n",
    "    # If crops in dry season (Apr-Jun or Jul-Sep), increase irrigation dependency\n",
    "    dry_season_crops = (season_types[:, :, 1] == 1) | (season_types[:, :, 2] == 1)\n",
    "    irrigation_index[dry_season_crops & (irrigation_index < 0.5)] = 0.5\n",
    "    \n",
    "    return irrigation_index\n",
    "\n",
    "irrigation_dependency = calculate_irrigation_index(cropping_intensity, season_types)\n",
    "\n",
    "print(\"\\n‚úÖ Irrigation Dependency Index Calculated\")\n",
    "print(f\"\\nIrrigation Dependency Distribution:\")\n",
    "print(f\"  No irrigation (0.0): {(irrigation_dependency == 0.0).sum():,} pixels\")\n",
    "print(f\"  Partial irrigation (0.5): {(irrigation_dependency == 0.5).sum():,} pixels\")\n",
    "print(f\"  Full irrigation (1.0): {(irrigation_dependency == 1.0).sum():,} pixels\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PLANTING DATE STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_planting_dates(season_start_data, season_mask, season_name):\n",
    "    \"\"\"\n",
    "    Analyze planting date distribution for a specific season\n",
    "    \"\"\"\n",
    "    valid_dates = season_start_data[season_mask == 1]\n",
    "    valid_dates_clean = valid_dates[~np.isnan(valid_dates)]\n",
    "    \n",
    "    if len(valid_dates_clean) == 0:\n",
    "        return None\n",
    "    \n",
    "    stats = {\n",
    "        'season': season_name,\n",
    "        'pixel_count': len(valid_dates_clean),\n",
    "        'earliest_planting': int(np.min(valid_dates_clean)),\n",
    "        'latest_planting': int(np.max(valid_dates_clean)),\n",
    "        'median_planting': int(np.median(valid_dates_clean)),\n",
    "        'mean_planting': float(np.mean(valid_dates_clean)),\n",
    "        'std_planting': float(np.std(valid_dates_clean)),\n",
    "        'planting_window_days': int(np.max(valid_dates_clean) - np.min(valid_dates_clean))\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"\\n‚úÖ Planting Date Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "planting_stats = []\n",
    "\n",
    "# Season 1 (Nov-Mar)\n",
    "season1_stats = analyze_planting_dates(\n",
    "    flexible_results['all_seasons'][:, :, 0],\n",
    "    season_types[:, :, 0],\n",
    "    \"Season 1 (Nov-Mar)\"\n",
    ")\n",
    "if season1_stats:\n",
    "    planting_stats.append(season1_stats)\n",
    "    print(f\"\\n{season1_stats['season']}:\")\n",
    "    print(f\"  Pixels with planting: {season1_stats['pixel_count']:,}\")\n",
    "    print(f\"  Earliest planting: Day {season1_stats['earliest_planting']}\")\n",
    "    print(f\"  Latest planting: Day {season1_stats['latest_planting']}\")\n",
    "    print(f\"  Median planting: Day {season1_stats['median_planting']}\")\n",
    "    print(f\"  Planting window: {season1_stats['planting_window_days']} days\")\n",
    "    print(f\"  Variability (std): {season1_stats['std_planting']:.1f} days\")\n",
    "\n",
    "# Season 2 (Apr-Jun)\n",
    "season2_stats = analyze_planting_dates(\n",
    "    flexible_results['all_seasons'][:, :, 2],\n",
    "    season_types[:, :, 1],\n",
    "    \"Season 2 (Apr-Jun)\"\n",
    ")\n",
    "if season2_stats:\n",
    "    planting_stats.append(season2_stats)\n",
    "    print(f\"\\n{season2_stats['season']}:\")\n",
    "    print(f\"  Pixels with planting: {season2_stats['pixel_count']:,}\")\n",
    "    print(f\"  Earliest planting: Day {season2_stats['earliest_planting']}\")\n",
    "    print(f\"  Latest planting: Day {season2_stats['latest_planting']}\")\n",
    "    print(f\"  Median planting: Day {season2_stats['median_planting']}\")\n",
    "    print(f\"  Planting window: {season2_stats['planting_window_days']} days\")\n",
    "    print(f\"  Variability (std): {season2_stats['std_planting']:.1f} days\")\n",
    "\n",
    "# Season 3 (Jul-Sep)\n",
    "season3_stats = analyze_planting_dates(\n",
    "    flexible_results['all_seasons'][:, :, 4],\n",
    "    season_types[:, :, 2],\n",
    "    \"Season 3 (Jul-Sep)\"\n",
    ")\n",
    "if season3_stats:\n",
    "    planting_stats.append(season3_stats)\n",
    "    print(f\"\\n{season3_stats['season']}:\")\n",
    "    print(f\"  Pixels with planting: {season3_stats['pixel_count']:,}\")\n",
    "    print(f\"  Earliest planting: Day {season3_stats['earliest_planting']}\")\n",
    "    print(f\"  Latest planting: Day {season3_stats['latest_planting']}\")\n",
    "    print(f\"  Median planting: Day {season3_stats['median_planting']}\")\n",
    "    print(f\"  Planting window: {season3_stats['planting_window_days']} days\")\n",
    "    print(f\"  Variability (std): {season3_stats['std_planting']:.1f} days\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. AGRICULTURAL INTENSIFICATION INDEX\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_intensification_index(cropping_intensity, season_durations, peak_ndvi):\n",
    "    \"\"\"\n",
    "    Calculate agricultural intensification index (0-1)\n",
    "    \n",
    "    Combines:\n",
    "    - Cropping intensity (40% weight)\n",
    "    - Season duration efficiency (30% weight)\n",
    "    - Productivity (peak NDVI, 30% weight)\n",
    "    \"\"\"\n",
    "    # Normalize cropping intensity (max 3 seasons)\n",
    "    intensity_normalized = cropping_intensity / 3.0\n",
    "    \n",
    "    # Calculate average season duration (longer = more intensive)\n",
    "    avg_duration = np.nanmean(season_durations, axis=2)\n",
    "    duration_normalized = np.clip(avg_duration / 120.0, 0, 1)  # 120 days reference\n",
    "    \n",
    "    # Normalize peak NDVI (higher = more productive)\n",
    "    ndvi_normalized = np.clip(peak_ndvi / 0.9, 0, 1)  # 0.9 NDVI reference\n",
    "    \n",
    "    # Weighted combination\n",
    "    intensification = (\n",
    "        intensity_normalized * 0.4 +\n",
    "        duration_normalized * 0.3 +\n",
    "        ndvi_normalized * 0.3\n",
    "    )\n",
    "    \n",
    "    return intensification\n",
    "\n",
    "# Calculate season durations\n",
    "season_durations = np.zeros((*flexible_results['all_seasons'].shape[:2], 3))\n",
    "for i in range(3):\n",
    "    sos = flexible_results['all_seasons'][:, :, i*2]\n",
    "    eos = flexible_results['all_seasons'][:, :, i*2 + 1]\n",
    "    season_durations[:, :, i] = eos - sos\n",
    "\n",
    "# Get peak NDVI from fused data\n",
    "peak_ndvi = np.nanmax(fused_result['S2ndvi'].values, axis=0)\n",
    "\n",
    "intensification_index = calculate_intensification_index(\n",
    "    cropping_intensity,\n",
    "    season_durations,\n",
    "    peak_ndvi\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Agricultural Intensification Index Calculated\")\n",
    "print(f\"\\nIntensification Statistics:\")\n",
    "print(f\"  Mean intensification: {np.nanmean(intensification_index[cropping_intensity > 0]):.3f}\")\n",
    "print(f\"  High intensification (>0.7): {(intensification_index > 0.7).sum():,} pixels\")\n",
    "print(f\"  Medium intensification (0.4-0.7): {((intensification_index >= 0.4) & (intensification_index <= 0.7)).sum():,} pixels\")\n",
    "print(f\"  Low intensification (<0.4): {(intensification_index < 0.4).sum():,} pixels\")\n",
    "\n",
    "print(\"\\n‚úÖ All planting indices calculated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Visualize Planting Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è  VISUALIZING PLANTING INDICES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comprehensive planting indices visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Agricultural Planting Indices - Indonesia', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Planting Pattern Classification\n",
    "im1 = axes[0, 0].imshow(planting_patterns, cmap='tab10', vmin=0, vmax=7)\n",
    "axes[0, 0].set_title('üåæ Planting Pattern Classification', fontweight='bold')\n",
    "cbar1 = plt.colorbar(im1, ax=axes[0, 0])\n",
    "cbar1.set_label('Pattern Type')\n",
    "cbar1.set_ticks([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "cbar1.set_ticklabels(['Non-agri', 'Single\\n(Rain)', 'Double\\n(Irrig)', 'Triple\\n(Intens)', \n",
    "                      'Dry\\nFocus', 'Mid-yr', 'Late', 'Other'], fontsize=8)\n",
    "\n",
    "# 2. Irrigation Dependency Index\n",
    "im2 = axes[0, 1].imshow(irrigation_dependency, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0, 1].set_title('üíß Irrigation Dependency Index', fontweight='bold')\n",
    "cbar2 = plt.colorbar(im2, ax=axes[0, 1])\n",
    "cbar2.set_label('Irrigation Need (0=rain-fed, 1=full)')\n",
    "cbar2.set_ticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "\n",
    "# 3. Agricultural Intensification Index\n",
    "# Mask non-agricultural areas\n",
    "intensification_masked = np.where(cropping_intensity > 0, intensification_index, np.nan)\n",
    "im3 = axes[0, 2].imshow(intensification_masked, cmap='YlGnBu', vmin=0, vmax=1)\n",
    "axes[0, 2].set_title('üìà Agricultural Intensification Index', fontweight='bold')\n",
    "cbar3 = plt.colorbar(im3, ax=axes[0, 2])\n",
    "cbar3.set_label('Intensification (0=low, 1=high)')\n",
    "cbar3.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "# 4. Season 1 Planting Date Distribution (Nov-Mar)\n",
    "season1_sos = flexible_results['all_seasons'][:, :, 0]\n",
    "season1_sos_masked = np.where(season_types[:, :, 0] == 1, season1_sos, np.nan)\n",
    "\n",
    "# Handle year boundary for visualization\n",
    "season1_sos_viz = season1_sos_masked.copy()\n",
    "for y in range(season1_sos_viz.shape[0]):\n",
    "    for x in range(season1_sos_viz.shape[1]):\n",
    "        if not np.isnan(season1_sos_viz[y, x]):\n",
    "            day = season1_sos_viz[y, x]\n",
    "            if day >= 305:  # Nov-Dec\n",
    "                season1_sos_viz[y, x] = day - 305  # Nov 1 = 0\n",
    "            elif day <= 90:  # Jan-Mar\n",
    "                season1_sos_viz[y, x] = day + 60  # After Dec 31\n",
    "\n",
    "im4 = axes[1, 0].imshow(season1_sos_viz, cmap='RdYlGn', vmin=0, vmax=150)\n",
    "axes[1, 0].set_title('üìÖ Season 1 Planting Dates (Nov-Mar)', fontweight='bold')\n",
    "cbar4 = plt.colorbar(im4, ax=axes[1, 0])\n",
    "cbar4.set_label('Planting Time')\n",
    "cbar4.set_ticks([0, 30, 60, 90, 120, 150])\n",
    "cbar4.set_ticklabels(['Early Nov', 'Early Dec', 'Early Jan', 'Early Feb', 'Early Mar', 'Late Mar'], fontsize=8)\n",
    "\n",
    "# 5. Season 2 Planting Date Distribution (Apr-Jun)\n",
    "season2_sos = flexible_results['all_seasons'][:, :, 2]\n",
    "season2_sos_masked = np.where(season_types[:, :, 1] == 1, season2_sos, np.nan)\n",
    "\n",
    "im5 = axes[1, 1].imshow(season2_sos_masked, cmap='viridis', vmin=90, vmax=180)\n",
    "axes[1, 1].set_title('üìÖ Season 2 Planting Dates (Apr-Jun)', fontweight='bold')\n",
    "cbar5 = plt.colorbar(im5, ax=axes[1, 1])\n",
    "cbar5.set_label('Day of Year')\n",
    "cbar5.set_ticks([90, 105, 120, 135, 150, 165, 180])\n",
    "cbar5.set_ticklabels(['Apr 1', 'Apr 15', 'May 1', 'May 15', 'Jun 1', 'Jun 15', 'Jun 30'], fontsize=8)\n",
    "\n",
    "# 6. Cropping Intensity with Irrigation Overlay\n",
    "im6 = axes[1, 2].imshow(cropping_intensity, cmap='RdYlGn', vmin=0, vmax=3, alpha=0.7)\n",
    "axes[1, 2].set_title('üåæ Cropping Intensity + Irrigation', fontweight='bold')\n",
    "\n",
    "# Overlay irrigation areas with contours\n",
    "high_irrigation = irrigation_dependency > 0.5\n",
    "axes[1, 2].contour(high_irrigation, levels=[0.5], colors='blue', linewidths=2, alpha=0.5)\n",
    "\n",
    "cbar6 = plt.colorbar(im6, ax=axes[1, 2])\n",
    "cbar6.set_label('Seasons per Year')\n",
    "cbar6.set_ticks([0, 1, 2, 3])\n",
    "cbar6.set_ticklabels(['None', '1 Season', '2 Seasons', '3 Seasons'])\n",
    "\n",
    "# Add coordinates to all subplots\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('X coordinate (pixel)')\n",
    "    ax.set_ylabel('Y coordinate (pixel)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('planting_indices_comprehensive.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Planting indices visualization saved to: planting_indices_comprehensive.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create planting date histograms\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Planting Date Distributions by Season', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Season 1 histogram\n",
    "if season1_stats:\n",
    "    season1_dates = flexible_results['all_seasons'][:, :, 0][season_types[:, :, 0] == 1]\n",
    "    season1_dates_clean = season1_dates[~np.isnan(season1_dates)]\n",
    "    \n",
    "    axes[0].hist(season1_dates_clean, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(season1_stats['median_planting'], color='red', linestyle='--', \n",
    "                    linewidth=2, label=f\"Median: Day {season1_stats['median_planting']}\")\n",
    "    axes[0].set_xlabel('Day of Year')\n",
    "    axes[0].set_ylabel('Number of Pixels')\n",
    "    axes[0].set_title('Season 1: Nov-Mar Planting Dates')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Season 2 histogram\n",
    "if season2_stats:\n",
    "    season2_dates = flexible_results['all_seasons'][:, :, 2][season_types[:, :, 1] == 1]\n",
    "    season2_dates_clean = season2_dates[~np.isnan(season2_dates)]\n",
    "    \n",
    "    axes[1].hist(season2_dates_clean, bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "    axes[1].axvline(season2_stats['median_planting'], color='red', linestyle='--',\n",
    "                    linewidth=2, label=f\"Median: Day {season2_stats['median_planting']}\")\n",
    "    axes[1].set_xlabel('Day of Year')\n",
    "    axes[1].set_ylabel('Number of Pixels')\n",
    "    axes[1].set_title('Season 2: Apr-Jun Planting Dates')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Season 3 histogram\n",
    "if season3_stats:\n",
    "    season3_dates = flexible_results['all_seasons'][:, :, 4][season_types[:, :, 2] == 1]\n",
    "    season3_dates_clean = season3_dates[~np.isnan(season3_dates)]\n",
    "    \n",
    "    axes[2].hist(season3_dates_clean, bins=30, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[2].axvline(season3_stats['median_planting'], color='red', linestyle='--',\n",
    "                    linewidth=2, label=f\"Median: Day {season3_stats['median_planting']}\")\n",
    "    axes[2].set_xlabel('Day of Year')\n",
    "    axes[2].set_ylabel('Number of Pixels')\n",
    "    axes[2].set_title('Season 3: Jul-Sep Planting Dates')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('planting_date_histograms.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Planting date histograms saved to: planting_date_histograms.png\")\n",
    "\n",
    "print(\"\\n‚úÖ All planting indices visualized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Export Planting Indices and Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ EXPORTING PLANTING INDICES AND GENERATING REPORTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CREATE COMPREHENSIVE PLANTING INDICES DATASET\n",
    "# ============================================================================\n",
    "\n",
    "planting_indices_dataset = xr.Dataset({\n",
    "    # Planting pattern classification\n",
    "    'planting_pattern': xr.DataArray(\n",
    "        planting_patterns,\n",
    "        dims=['y', 'x'],\n",
    "        coords={'y': y_coords, 'x': x_coords},\n",
    "        attrs={\n",
    "            'long_name': 'Agricultural Planting Pattern Classification',\n",
    "            'description': 'Classification of planting patterns based on seasonal presence',\n",
    "            'units': 'pattern code',\n",
    "            'pattern_codes': {\n",
    "                0: 'Non-agricultural',\n",
    "                1: 'Single season (rain-fed)',\n",
    "                2: 'Double season (irrigated)',\n",
    "                3: 'Triple season (intensive)',\n",
    "                4: 'Dry season focus',\n",
    "                5: 'Mid-year only',\n",
    "                6: 'Late season only',\n",
    "                7: 'Other patterns'\n",
    "            }\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # Irrigation dependency index\n",
    "    'irrigation_dependency': xr.DataArray(\n",
    "        irrigation_dependency,\n",
    "        dims=['y', 'x'],\n",
    "        coords={'y': y_coords, 'x': x_coords},\n",
    "        attrs={\n",
    "            'long_name': 'Irrigation Dependency Index',\n",
    "            'description': 'Level of irrigation dependency (0=rain-fed, 0.5=partial, 1.0=full)',\n",
    "            'units': 'index (0-1)',\n",
    "            'interpretation': '0=rain-fed, 0.5=partial irrigation, 1.0=full irrigation'\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # Agricultural intensification index\n",
    "    'intensification_index': xr.DataArray(\n",
    "        intensification_index,\n",
    "        dims=['y', 'x'],\n",
    "        coords={'y': y_coords, 'x': x_coords},\n",
    "        attrs={\n",
    "            'long_name': 'Agricultural Intensification Index',\n",
    "            'description': 'Composite index combining cropping intensity, season duration, and productivity',\n",
    "            'units': 'index (0-1)',\n",
    "            'components': 'Cropping intensity (40%), Season duration (30%), Peak NDVI (30%)'\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # Cropping intensity (reference)\n",
    "    'cropping_intensity': xr.DataArray(\n",
    "        cropping_intensity,\n",
    "        dims=['y', 'x'],\n",
    "        coords={'y': y_coords, 'x': x_coords},\n",
    "        attrs={\n",
    "            'long_name': 'Cropping Intensity',\n",
    "            'description': 'Number of cropping seasons per year',\n",
    "            'units': 'seasons per year',\n",
    "            'valid_range': [0, 3]\n",
    "        }\n",
    "    )\n",
    "})\n",
    "\n",
    "# Add global attributes\n",
    "planting_indices_dataset.attrs.update({\n",
    "    'title': 'Agricultural Planting Indices for Indonesia',\n",
    "    'description': 'Comprehensive planting indices derived from MOGPR S1+S2 fusion and multi-season analysis',\n",
    "    'methodology': 'Multi-sensor satellite data fusion with flexible season detection',\n",
    "    'spatial_coverage': f'{len(y_coords)} x {len(x_coords)} pixels',\n",
    "    'processing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'software': 'FuseTS with MOGPR algorithm',\n",
    "    'data_source': 'Sentinel-1 VV/VH + Sentinel-2 NDVI',\n",
    "    'country': 'Indonesia',\n",
    "    'agricultural_calendar': 'Season 1 (Nov-Mar), Season 2 (Apr-Jun), Season 3 (Jul-Sep)'\n",
    "})\n",
    "\n",
    "# Save dataset\n",
    "planting_indices_file = \"planting_indices_indonesia.nc\"\n",
    "planting_indices_dataset.to_netcdf(planting_indices_file)\n",
    "print(f\"‚úÖ Planting indices dataset saved to: {planting_indices_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CREATE DETAILED STATISTICS REPORT\n",
    "# ============================================================================\n",
    "\n",
    "planting_indices_stats = {\n",
    "    'summary': {\n",
    "        'total_pixels': int(planting_patterns.size),\n",
    "        'agricultural_pixels': int((planting_patterns > 0).sum()),\n",
    "        'agricultural_percentage': float((planting_patterns > 0).sum() / planting_patterns.size * 100)\n",
    "    },\n",
    "    \n",
    "    'planting_patterns': {},\n",
    "    \n",
    "    'irrigation_dependency': {\n",
    "        'rain_fed_pixels': int((irrigation_dependency == 0.0).sum()),\n",
    "        'partial_irrigation_pixels': int((irrigation_dependency == 0.5).sum()),\n",
    "        'full_irrigation_pixels': int((irrigation_dependency == 1.0).sum()),\n",
    "        'rain_fed_percentage': float((irrigation_dependency == 0.0).sum() / (irrigation_dependency >= 0).sum() * 100),\n",
    "        'irrigated_percentage': float((irrigation_dependency > 0).sum() / (irrigation_dependency >= 0).sum() * 100)\n",
    "    },\n",
    "    \n",
    "    'intensification': {\n",
    "        'mean_index': float(np.nanmean(intensification_index[cropping_intensity > 0])),\n",
    "        'high_intensification_pixels': int((intensification_index > 0.7).sum()),\n",
    "        'medium_intensification_pixels': int(((intensification_index >= 0.4) & (intensification_index <= 0.7)).sum()),\n",
    "        'low_intensification_pixels': int((intensification_index < 0.4).sum())\n",
    "    },\n",
    "    \n",
    "    'planting_dates': {}\n",
    "}\n",
    "\n",
    "# Add pattern statistics\n",
    "for pattern_code, pattern_name in pattern_names.items():\n",
    "    pixel_count = int((planting_patterns == pattern_code).sum())\n",
    "    if pixel_count > 0:\n",
    "        planting_indices_stats['planting_patterns'][pattern_name] = {\n",
    "            'pixel_count': pixel_count,\n",
    "            'percentage': float(pixel_count / planting_patterns.size * 100)\n",
    "        }\n",
    "\n",
    "# Add planting date statistics\n",
    "for stats in planting_stats:\n",
    "    planting_indices_stats['planting_dates'][stats['season']] = {\n",
    "        'pixel_count': stats['pixel_count'],\n",
    "        'earliest_planting_doy': stats['earliest_planting'],\n",
    "        'latest_planting_doy': stats['latest_planting'],\n",
    "        'median_planting_doy': stats['median_planting'],\n",
    "        'mean_planting_doy': stats['mean_planting'],\n",
    "        'planting_variability_days': stats['std_planting'],\n",
    "        'planting_window_days': stats['planting_window_days']\n",
    "    }\n",
    "\n",
    "# Save statistics\n",
    "stats_file = \"planting_indices_statistics.json\"\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(planting_indices_stats, f, indent=2)\n",
    "print(f\"‚úÖ Planting indices statistics saved to: {stats_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CREATE CSV EXPORT FOR GIS/ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìä Creating detailed CSV for GIS integration...\")\n",
    "\n",
    "# Extract per-pixel data (sample every 3rd pixel for manageable file size)\n",
    "planting_indices_csv_data = []\n",
    "\n",
    "for i in range(0, len(y_coords), 3):\n",
    "    for j in range(0, len(x_coords), 3):\n",
    "        if planting_patterns[i, j] > 0:  # Only agricultural pixels\n",
    "            row_data = {\n",
    "                'pixel_y': i,\n",
    "                'pixel_x': j,\n",
    "                'latitude': float(y_coords[i]),\n",
    "                'longitude': float(x_coords[j]),\n",
    "                'planting_pattern': int(planting_patterns[i, j]),\n",
    "                'pattern_name': pattern_names[int(planting_patterns[i, j])],\n",
    "                'cropping_intensity': int(cropping_intensity[i, j]),\n",
    "                'irrigation_dependency': float(irrigation_dependency[i, j]),\n",
    "                'intensification_index': float(intensification_index[i, j]),\n",
    "                'season1_present': int(season_types[i, j, 0]),\n",
    "                'season2_present': int(season_types[i, j, 1]),\n",
    "                'season3_present': int(season_types[i, j, 2])\n",
    "            }\n",
    "            \n",
    "            # Add planting dates if available\n",
    "            if season_types[i, j, 0] == 1:\n",
    "                row_data['season1_planting_doy'] = flexible_results['all_seasons'][i, j, 0]\n",
    "            if season_types[i, j, 1] == 1:\n",
    "                row_data['season2_planting_doy'] = flexible_results['all_seasons'][i, j, 2]\n",
    "            if season_types[i, j, 2] == 1:\n",
    "                row_data['season3_planting_doy'] = flexible_results['all_seasons'][i, j, 4]\n",
    "            \n",
    "            planting_indices_csv_data.append(row_data)\n",
    "\n",
    "planting_indices_df = pd.DataFrame(planting_indices_csv_data)\n",
    "csv_file = \"planting_indices_detailed.csv\"\n",
    "planting_indices_df.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Detailed planting indices CSV saved to: {csv_file}\")\n",
    "print(f\"   üìä Contains {len(planting_indices_df)} sample agricultural pixels\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. GENERATE SUMMARY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã PLANTING INDICES SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüåæ PLANTING PATTERN DISTRIBUTION:\")\n",
    "for pattern_name, stats in planting_indices_stats['planting_patterns'].items():\n",
    "    print(f\"  {pattern_name}: {stats['pixel_count']:,} pixels ({stats['percentage']:.2f}%)\")\n",
    "\n",
    "print(\"\\nüíß IRRIGATION DEPENDENCY:\")\n",
    "print(f\"  Rain-fed agriculture: {planting_indices_stats['irrigation_dependency']['rain_fed_percentage']:.1f}%\")\n",
    "print(f\"  Irrigated agriculture: {planting_indices_stats['irrigation_dependency']['irrigated_percentage']:.1f}%\")\n",
    "\n",
    "print(\"\\nüìà AGRICULTURAL INTENSIFICATION:\")\n",
    "print(f\"  Mean intensification index: {planting_indices_stats['intensification']['mean_index']:.3f}\")\n",
    "print(f\"  High intensification areas: {planting_indices_stats['intensification']['high_intensification_pixels']:,} pixels\")\n",
    "\n",
    "print(\"\\nüìÖ PLANTING DATE ANALYSIS:\")\n",
    "for season_name, date_stats in planting_indices_stats['planting_dates'].items():\n",
    "    print(f\"\\n  {season_name}:\")\n",
    "    print(f\"    Planted pixels: {date_stats['pixel_count']:,}\")\n",
    "    print(f\"    Median planting: Day {date_stats['median_planting_doy']}\")\n",
    "    print(f\"    Planting window: {date_stats['planting_window_days']} days\")\n",
    "    print(f\"    Variability: ¬±{date_stats['planting_variability_days']:.1f} days\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÅ EXPORTED FILES:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  1. {planting_indices_file} - NetCDF dataset (all indices)\")\n",
    "print(f\"  2. {stats_file} - JSON statistics\")\n",
    "print(f\"  3. {csv_file} - Detailed CSV for GIS\")\n",
    "print(f\"  4. planting_indices_comprehensive.png - Index maps\")\n",
    "print(f\"  5. planting_date_histograms.png - Temporal distributions\")\n",
    "\n",
    "print(\"\\nüöÄ APPLICATIONS:\")\n",
    "print(\"  ‚Ä¢ Agricultural extension services - Target interventions\")\n",
    "print(\"  ‚Ä¢ Irrigation planning - Identify high-need areas\")\n",
    "print(\"  ‚Ä¢ Crop insurance - Risk assessment and premium calculation\")\n",
    "print(\"  ‚Ä¢ Food security monitoring - Track intensification trends\")\n",
    "print(\"  ‚Ä¢ Policy making - Evidence-based subsidy allocation\")\n",
    "print(\"  ‚Ä¢ Climate adaptation - Understand planting shifts\")\n",
    "\n",
    "print(\"\\n‚úÖ Planting indices analysis complete and ready for use!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ GPU-Accelerated MOGPR (Optional - 10-100x faster!)\n",
    "\n",
    "If you have a GPU (NVIDIA CUDA or Apple Silicon), you can use the GPU-accelerated version for massive speedup!\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "# Install PyTorch with GPU support\n",
    "# For NVIDIA GPU:\n",
    "pip install torch gpytorch\n",
    "\n",
    "# For Apple Silicon (M1/M2/M3):\n",
    "pip install torch gpytorch\n",
    "\n",
    "# Verify GPU:\n",
    "python -c \"import torch; print(f'GPU: {torch.cuda.is_available() or torch.backends.mps.is_available()}')\"\n",
    "```\n",
    "\n",
    "**Expected Performance:**\n",
    "- **CPU (current)**: 50√ó50 pixels = 11 minutes ‚Üí Full dataset = 21 hours\n",
    "- **GPU**: 50√ó50 pixels = ~1 minute ‚Üí Full dataset = ~2 hours\n",
    "- **Speedup**: 10-100x depending on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Use GPU-accelerated MOGPR (requires torch + gpytorch)\n",
    "# This can provide 10-100x speedup!\n",
    "\n",
    "USE_GPU = False  # Set to True if you have GPU and installed torch + gpytorch\n",
    "\n",
    "if USE_GPU:\n",
    "    try:\n",
    "        import torch\n",
    "        import gpytorch\n",
    "        from fusets.mogpr_gpu import MOGPRTransformerGPU, mogpr_gpu\n",
    "        \n",
    "        # Check GPU availability\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üöÄ NVIDIA GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            print(\"üöÄ Apple Silicon GPU (MPS) detected\")\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No GPU detected, falling back to CPU\")\n",
    "            device = torch.device(\"cpu\")\n",
    "        \n",
    "        print(f\"‚úÖ GPU-accelerated MOGPR ready on {device}\")\n",
    "        print(\"Expected speedup: 10-100x faster than CPU version!\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå GPU libraries not installed: {e}\")\n",
    "        print(\"Install with: pip install torch gpytorch\")\n",
    "        USE_GPU = False\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Using CPU version (slower but works everywhere)\")\n",
    "    print(\"To enable GPU: Set USE_GPU = True and install torch + gpytorch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MOGPR H100)",
   "language": "python",
   "name": "mogpr_h100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
