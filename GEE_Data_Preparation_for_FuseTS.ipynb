{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Earth Engine Data Preparation for FuseTS\n",
    "\n",
    "This notebook extracts Sentinel-1 and Sentinel-2 data from Google Earth Engine and prepares it for FuseTS MOGPR processing.\n",
    "\n",
    "## Temporal Compositing Strategy\n",
    "- **Total periods**: 31 periods for 2024\n",
    "- **Period length**: 12 days each\n",
    "- **Period 1**: Jan 1-12, 2024\n",
    "- **Period 2**: Jan 13-25, 2024  \n",
    "- **Period 3**: Jan 26 - Feb 7, 2024\n",
    "- **... and so on**\n",
    "\n",
    "## Output Format\n",
    "Data will be exported in FuseTS-compatible xarray format with proper band naming:\n",
    "- S1: `VV`, `VH` bands\n",
    "- S2: `S2ndvi` band\n",
    "- Dimensions: `(time, y, x)` with `t` coordinate name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"Earth Engine initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Earth Engine: {e}\")\n",
    "    print(\"Please run: ee.Authenticate() first if this is your first time\")\n",
    "    # ee.Authenticate()  # Uncomment this line for first-time setup\n",
    "    # ee.Initialize()\n",
    "\n",
    "print(f\"Earth Engine version: {ee.__version__}\")\n",
    "print(f\"geemap version: {geemap.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Study Area and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your study area (modify coordinates as needed)\n",
    "# Example: Agricultural area in Belgium\n",
    "study_area = ee.Geometry.Rectangle([\n",
    "    5.0,   # min longitude\n",
    "    50.8,  # min latitude  \n",
    "    5.4,   # max longitude\n",
    "    51.2   # max latitude\n",
    "])\n",
    "\n",
    "# Alternative: Define study area from shapefile or other geometry\n",
    "# study_area = geemap.shp_to_ee('path/to/your/shapefile.shp')\n",
    "\n",
    "# Processing parameters\n",
    "YEAR = 2024\n",
    "SCALE = 10  # meters per pixel (10m for S2, will be resampled for S1)\n",
    "MAX_CLOUD_COVER = 20  # Maximum cloud cover percentage for S2\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = 'gee_fusets_data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Study area bounds: {study_area.bounds().getInfo()}\")\n",
    "print(f\"Processing year: {YEAR}\")\n",
    "print(f\"Spatial resolution: {SCALE}m\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate 12-Day Composite Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_12day_periods(year):\n",
    "    \"\"\"\n",
    "    Generate 31 periods of 12 days each for the specified year\n",
    "    \"\"\"\n",
    "    start_date = datetime(year, 1, 1)\n",
    "    periods = []\n",
    "    \n",
    "    for period_num in range(31):\n",
    "        period_start = start_date + timedelta(days=period_num * 12)\n",
    "        period_end = period_start + timedelta(days=11)  # 12 days inclusive\n",
    "        \n",
    "        # Ensure we don't go beyond the year\n",
    "        if period_end.year > year:\n",
    "            period_end = datetime(year, 12, 31)\n",
    "            \n",
    "        periods.append({\n",
    "            'period': period_num + 1,\n",
    "            'start_date': period_start,\n",
    "            'end_date': period_end,\n",
    "            'start_str': period_start.strftime('%Y-%m-%d'),\n",
    "            'end_str': period_end.strftime('%Y-%m-%d'),\n",
    "            'center_date': period_start + timedelta(days=6),  # Middle of period\n",
    "            'doy_center': (period_start + timedelta(days=6)).timetuple().tm_yday\n",
    "        })\n",
    "        \n",
    "        if period_end.year > year:\n",
    "            break\n",
    "            \n",
    "    return periods\n",
    "\n",
    "# Generate periods\n",
    "periods = generate_12day_periods(YEAR)\n",
    "\n",
    "print(f\"Generated {len(periods)} periods for {YEAR}:\")\n",
    "print(\"\\nFirst 5 periods:\")\n",
    "for i, period in enumerate(periods[:5]):\n",
    "    print(f\"Period {period['period']:2d}: {period['start_str']} to {period['end_str']} (center: DOY {period['doy_center']:3d})\")\n",
    "\n",
    "print(\"\\nLast 5 periods:\")\n",
    "for i, period in enumerate(periods[-5:]):\n",
    "    print(f\"Period {period['period']:2d}: {period['start_str']} to {period['end_str']} (center: DOY {period['doy_center']:3d})\")\n",
    "\n",
    "# Create a DataFrame for easier handling\n",
    "periods_df = pd.DataFrame(periods)\n",
    "print(f\"\\nTotal temporal coverage: {periods[0]['start_str']} to {periods[-1]['end_str']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentinel1_data(geometry, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Load Sentinel-1 GRD data for a specific time period\n",
    "    \"\"\"\n",
    "    s1_collection = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "                    .filterBounds(geometry)\n",
    "                    .filterDate(start_date, end_date)\n",
    "                    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "                    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "                    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "                    .select(['VV', 'VH']))\n",
    "    \n",
    "    return s1_collection\n",
    "\n",
    "def load_sentinel2_data(geometry, start_date, end_date, max_cloud_cover=20):\n",
    "    \"\"\"\n",
    "    Load Sentinel-2 data and calculate NDVI for a specific time period\n",
    "    \"\"\"\n",
    "    def calculate_ndvi(image):\n",
    "        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "        return image.addBands(ndvi)\n",
    "    \n",
    "    def mask_clouds(image):\n",
    "        # Use SCL band for cloud masking\n",
    "        scl = image.select('SCL')\n",
    "        # Keep vegetation, soil, water, snow classes (4,5,6,11)\n",
    "        good_pixels = scl.eq(4).Or(scl.eq(5)).Or(scl.eq(6)).Or(scl.eq(11))\n",
    "        return image.updateMask(good_pixels)\n",
    "    \n",
    "    s2_collection = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "                    .filterBounds(geometry)\n",
    "                    .filterDate(start_date, end_date)\n",
    "                    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', max_cloud_cover))\n",
    "                    .map(mask_clouds)\n",
    "                    .map(calculate_ndvi)\n",
    "                    .select(['NDVI']))\n",
    "    \n",
    "    return s2_collection\n",
    "\n",
    "def create_composite(collection, method='median'):\n",
    "    \"\"\"\n",
    "    Create a composite from an image collection\n",
    "    \"\"\"\n",
    "    if method == 'median':\n",
    "        return collection.median()\n",
    "    elif method == 'mean':\n",
    "        return collection.mean()\n",
    "    elif method == 'max':\n",
    "        return collection.max()\n",
    "    else:\n",
    "        return collection.median()\n",
    "\n",
    "print(\"Data loading functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Data for All Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_period(period_info, geometry, scale=10):\n",
    "    \"\"\"\n",
    "    Process S1 and S2 data for a single 12-day period\n",
    "    \"\"\"\n",
    "    start_date = period_info['start_str']\n",
    "    end_date = period_info['end_str']\n",
    "    period_num = period_info['period']\n",
    "    \n",
    "    print(f\"Processing Period {period_num}: {start_date} to {end_date}\")\n",
    "    \n",
    "    try:\n",
    "        # Load Sentinel-1 data\n",
    "        s1_collection = load_sentinel1_data(geometry, start_date, end_date)\n",
    "        s1_count = s1_collection.size().getInfo()\n",
    "        \n",
    "        # Load Sentinel-2 data\n",
    "        s2_collection = load_sentinel2_data(geometry, start_date, end_date, MAX_CLOUD_COVER)\n",
    "        s2_count = s2_collection.size().getInfo()\n",
    "        \n",
    "        print(f\"  Found {s1_count} S1 images, {s2_count} S2 images\")\n",
    "        \n",
    "        # Create composites\n",
    "        if s1_count > 0:\n",
    "            s1_composite = create_composite(s1_collection, 'median')\n",
    "        else:\n",
    "            # Create empty image with correct bands\n",
    "            s1_composite = ee.Image.constant([0, 0]).rename(['VV', 'VH']).updateMask(ee.Image.constant(0))\n",
    "            \n",
    "        if s2_count > 0:\n",
    "            s2_composite = create_composite(s2_collection, 'median')\n",
    "        else:\n",
    "            # Create empty NDVI image\n",
    "            s2_composite = ee.Image.constant(0).rename('NDVI').updateMask(ee.Image.constant(0))\n",
    "        \n",
    "        # Combine S1 and S2 data\n",
    "        combined_image = s1_composite.addBands(s2_composite.rename('S2ndvi'))\n",
    "        \n",
    "        # Add metadata\n",
    "        combined_image = combined_image.set({\n",
    "            'period': period_num,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'center_date': period_info['center_date'].strftime('%Y-%m-%d'),\n",
    "            'doy_center': period_info['doy_center'],\n",
    "            's1_count': s1_count,\n",
    "            's2_count': s2_count\n",
    "        })\n",
    "        \n",
    "        return combined_image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing period {period_num}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process all periods\n",
    "print(\"Starting data processing for all periods...\\n\")\n",
    "\n",
    "processed_images = []\n",
    "successful_periods = []\n",
    "\n",
    "for i, period in enumerate(periods):\n",
    "    result = process_single_period(period, study_area, SCALE)\n",
    "    if result is not None:\n",
    "        processed_images.append(result)\n",
    "        successful_periods.append(period)\n",
    "    \n",
    "    # Progress update every 5 periods\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Completed {i + 1}/{len(periods)} periods\\n\")\n",
    "\n",
    "print(f\"Successfully processed {len(processed_images)} out of {len(periods)} periods\")\n",
    "\n",
    "# Create ImageCollection from processed images\n",
    "if processed_images:\n",
    "    time_series_collection = ee.ImageCollection(processed_images)\n",
    "    print(f\"Created time series collection with {time_series_collection.size().getInfo()} images\")\n",
    "else:\n",
    "    print(\"No images were successfully processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Data from GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_timeseries_to_drive(collection, geometry, scale, output_name):\n",
    "    \"\"\"\n",
    "    Export the time series collection to Google Drive as a multi-band image\n",
    "    \"\"\"\n",
    "    # Convert collection to multi-band image\n",
    "    # Each period becomes a separate set of bands\n",
    "    image_list = collection.toList(collection.size())\n",
    "    \n",
    "    def rename_bands_with_period(image):\n",
    "        image = ee.Image(image)\n",
    "        period = ee.Number(image.get('period')).format('%02d')\n",
    "        \n",
    "        # Rename bands to include period number\n",
    "        old_names = image.bandNames()\n",
    "        new_names = old_names.map(lambda name: ee.String(name).cat('_P').cat(period))\n",
    "        \n",
    "        return image.rename(new_names)\n",
    "    \n",
    "    # Rename bands with period numbers\n",
    "    renamed_collection = collection.map(rename_bands_with_period)\n",
    "    \n",
    "    # Convert to single multi-band image\n",
    "    multi_band_image = renamed_collection.toBands()\n",
    "    \n",
    "    # Export task\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "        image=multi_band_image,\n",
    "        description=output_name,\n",
    "        folder='GEE_FuseTS_Data',\n",
    "        fileNamePrefix=output_name,\n",
    "        scale=scale,\n",
    "        region=geometry,\n",
    "        maxPixels=1e9,\n",
    "        crs='EPSG:4326',\n",
    "        fileFormat='GeoTIFF'\n",
    "    )\n",
    "    \n",
    "    return task\n",
    "\n",
    "def export_individual_periods(collection, geometry, scale, base_name):\n",
    "    \"\"\"\n",
    "    Export each period as a separate GeoTIFF file\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    image_list = collection.toList(collection.size())\n",
    "    \n",
    "    for i in range(len(successful_periods)):\n",
    "        image = ee.Image(image_list.get(i))\n",
    "        period_num = successful_periods[i]['period']\n",
    "        \n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=image,\n",
    "            description=f'{base_name}_Period_{period_num:02d}',\n",
    "            folder='GEE_FuseTS_Data',\n",
    "            fileNamePrefix=f'{base_name}_Period_{period_num:02d}',\n",
    "            scale=scale,\n",
    "            region=geometry,\n",
    "            maxPixels=1e9,\n",
    "            crs='EPSG:4326',\n",
    "            fileFormat='GeoTIFF'\n",
    "        )\n",
    "        \n",
    "        tasks.append(task)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "# Choose export method\n",
    "EXPORT_METHOD = 'individual'  # 'combined' or 'individual'\n",
    "\n",
    "if time_series_collection:\n",
    "    if EXPORT_METHOD == 'combined':\n",
    "        # Export as single multi-band file\n",
    "        print(\"Preparing export as single multi-band GeoTIFF...\")\n",
    "        export_task = export_timeseries_to_drive(\n",
    "            time_series_collection, \n",
    "            study_area, \n",
    "            SCALE, \n",
    "            f'S1_S2_TimeSeries_{YEAR}'\n",
    "        )\n",
    "        \n",
    "        print(f\"Starting export task: {export_task.config['description']}\")\n",
    "        export_task.start()\n",
    "        \n",
    "        print(f\"Export task submitted. Monitor progress at: https://code.earthengine.google.com/tasks\")\n",
    "        \n",
    "    else:\n",
    "        # Export individual period files\n",
    "        print(\"Preparing export as individual period GeoTIFFs...\")\n",
    "        export_tasks = export_individual_periods(\n",
    "            time_series_collection,\n",
    "            study_area,\n",
    "            SCALE,\n",
    "            f'S1_S2_{YEAR}'\n",
    "        )\n",
    "        \n",
    "        print(f\"Starting {len(export_tasks)} export tasks...\")\n",
    "        for i, task in enumerate(export_tasks[:5]):  # Start first 5 tasks\n",
    "            task.start()\n",
    "            print(f\"  Started: {task.config['description']}\")\n",
    "        \n",
    "        if len(export_tasks) > 5:\n",
    "            print(f\"\\nRemaining {len(export_tasks) - 5} tasks can be started manually or in batches\")\n",
    "            print(\"Monitor all tasks at: https://code.earthengine.google.com/tasks\")\n",
    "\n",
    "else:\n",
    "    print(\"No data to export!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Local Processing Function (Alternative to Export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timeseries_locally(collection, geometry, scale, max_pixels=1e6):\n",
    "    \"\"\"\n",
    "    Extract time series data directly to memory for small areas\n",
    "    This is faster than export/download for small study areas\n",
    "    \"\"\"\n",
    "    print(\"Extracting time series data locally...\")\n",
    "    \n",
    "    # Get the region bounds\n",
    "    region = geometry.bounds()\n",
    "    \n",
    "    # Extract data for each period\n",
    "    image_list = collection.toList(collection.size())\n",
    "    \n",
    "    periods_data = []\n",
    "    \n",
    "    for i in range(len(successful_periods)):\n",
    "        print(f\"Extracting period {i+1}/{len(successful_periods)}...\")\n",
    "        \n",
    "        image = ee.Image(image_list.get(i))\n",
    "        period_info = successful_periods[i]\n",
    "        \n",
    "        try:\n",
    "            # Sample the image\n",
    "            if scale * scale * 10000 < max_pixels:  # Rough estimate\n",
    "                # Use geemap for efficient extraction\n",
    "                data_array = geemap.ee_to_xarray(\n",
    "                    image, \n",
    "                    region=region, \n",
    "                    scale=scale,\n",
    "                    crs='EPSG:4326'\n",
    "                )\n",
    "                \n",
    "                # Add period information\n",
    "                data_array = data_array.assign_coords(\n",
    "                    period=period_info['period'],\n",
    "                    center_date=period_info['center_date'],\n",
    "                    doy_center=period_info['doy_center']\n",
    "                )\n",
    "                \n",
    "                periods_data.append(data_array)\n",
    "                \n",
    "            else:\n",
    "                print(f\"  Area too large for local extraction, use export method instead\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error extracting period {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if periods_data:\n",
    "        # Combine all periods into a single xarray Dataset\n",
    "        print(\"Combining periods into time series...\")\n",
    "        \n",
    "        # Concatenate along a new time dimension\n",
    "        combined_data = xr.concat(periods_data, dim='time')\n",
    "        \n",
    "        # Create proper time coordinates\n",
    "        time_coords = [p['center_date'] for p in successful_periods[:len(periods_data)]]\n",
    "        combined_data = combined_data.assign_coords(time=time_coords)\n",
    "        \n",
    "        return combined_data\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Try local extraction for small areas\n",
    "area_size = study_area.area().getInfo()  # in square meters\n",
    "area_km2 = area_size / 1e6\n",
    "\n",
    "print(f\"Study area size: {area_km2:.2f} km²\")\n",
    "\n",
    "if area_km2 < 100:  # Less than 100 km²\n",
    "    print(\"Area is small enough for local extraction. Attempting direct download...\")\n",
    "    \n",
    "    try:\n",
    "        local_data = extract_timeseries_locally(\n",
    "            time_series_collection, \n",
    "            study_area, \n",
    "            SCALE, \n",
    "            max_pixels=1e6\n",
    "        )\n",
    "        \n",
    "        if local_data is not None:\n",
    "            print(\"Local extraction successful!\")\n",
    "            print(f\"Data shape: {local_data.dims}\")\n",
    "            print(f\"Variables: {list(local_data.data_vars)}\")\n",
    "            \n",
    "            # Save locally\n",
    "            output_file = os.path.join(OUTPUT_DIR, f'S1_S2_timeseries_{YEAR}_local.nc')\n",
    "            local_data.to_netcdf(output_file)\n",
    "            print(f\"Data saved to: {output_file}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Local extraction failed, use export method instead\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Local extraction error: {e}\")\n",
    "        print(\"Use export method instead\")\n",
    "        \n",
    "else:\n",
    "    print(\"Area is too large for local extraction. Use the export method above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Metadata and Processing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processing summary\n",
    "processing_summary = {\n",
    "    'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'year': YEAR,\n",
    "    'total_periods': len(periods),\n",
    "    'successful_periods': len(successful_periods),\n",
    "    'study_area_bounds': study_area.bounds().getInfo(),\n",
    "    'spatial_resolution': SCALE,\n",
    "    'max_cloud_cover': MAX_CLOUD_COVER,\n",
    "    'composite_method': 'median',\n",
    "    'output_bands': ['VV', 'VH', 'S2ndvi']\n",
    "}\n",
    "\n",
    "# Create detailed period information\n",
    "period_details = []\n",
    "for period in successful_periods:\n",
    "    period_details.append({\n",
    "        'period': period['period'],\n",
    "        'start_date': period['start_str'],\n",
    "        'end_date': period['end_str'],\n",
    "        'center_date': period['center_date'].strftime('%Y-%m-%d'),\n",
    "        'doy_center': period['doy_center']\n",
    "    })\n",
    "\n",
    "# Save metadata\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'summary': processing_summary,\n",
    "    'periods': period_details\n",
    "}\n",
    "\n",
    "metadata_file = os.path.join(OUTPUT_DIR, f'processing_metadata_{YEAR}.json')\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(\"Processing Summary:\")\n",
    "print(f\"  Year: {YEAR}\")\n",
    "print(f\"  Total periods: {len(periods)}\")\n",
    "print(f\"  Successful periods: {len(successful_periods)}\")\n",
    "print(f\"  Spatial resolution: {SCALE}m\")\n",
    "print(f\"  Output bands: {processing_summary['output_bands']}\")\n",
    "print(f\"  Metadata saved to: {metadata_file}\")\n",
    "\n",
    "# Create period visualization\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot period timeline\n",
    "period_dates = [p['center_date'] for p in successful_periods]\n",
    "period_numbers = [p['period'] for p in successful_periods]\n",
    "\n",
    "ax.scatter(period_dates, period_numbers, alpha=0.7, s=50)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Period Number')\n",
    "ax.set_title(f'12-Day Composite Periods for {YEAR}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add month boundaries\n",
    "for month in range(1, 13):\n",
    "    month_start = datetime(YEAR, month, 1)\n",
    "    ax.axvline(month_start, color='red', alpha=0.3, linestyle='--')\n",
    "    ax.text(month_start, max(period_numbers) * 0.9, \n",
    "           month_start.strftime('%b'), rotation=90, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, f'period_timeline_{YEAR}.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPeriod timeline saved to: {os.path.join(OUTPUT_DIR, f'period_timeline_{YEAR}.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Conversion for FuseTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fusets_format(data_path_or_array, metadata_path=None):\n",
    "    \"\"\"\n",
    "    Convert GEE-exported data to FuseTS-compatible format\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(data_path_or_array, str):\n",
    "        # Load from file\n",
    "        print(f\"Loading data from: {data_path_or_array}\")\n",
    "        \n",
    "        if data_path_or_array.endswith('.nc'):\n",
    "            data = xr.open_dataset(data_path_or_array)\n",
    "        else:\n",
    "            # Assume GeoTIFF\n",
    "            import rioxarray\n",
    "            data = rioxarray.open_rasterio(data_path_or_array)\n",
    "            \n",
    "    else:\n",
    "        # Use provided array\n",
    "        data = data_path_or_array\n",
    "    \n",
    "    print(\"Converting to FuseTS format...\")\n",
    "    \n",
    "    # Ensure proper dimension naming\n",
    "    if 'time' in data.dims:\n",
    "        data = data.rename({'time': 't'})\n",
    "    \n",
    "    # Ensure proper band naming for FuseTS\n",
    "    if 'NDVI' in data.data_vars:\n",
    "        data = data.rename({'NDVI': 'S2ndvi'})\n",
    "    \n",
    "    # Ensure coordinate order is (t, y, x)\n",
    "    expected_dims = ['t', 'y', 'x']\n",
    "    \n",
    "    for var in data.data_vars:\n",
    "        if set(data[var].dims) == set(expected_dims):\n",
    "            data[var] = data[var].transpose('t', 'y', 'x')\n",
    "    \n",
    "    # Add FuseTS-specific attributes\n",
    "    data.attrs.update({\n",
    "        'title': f'Sentinel-1/2 Time Series for FuseTS Processing',\n",
    "        'description': '12-day composite periods extracted from Google Earth Engine',\n",
    "        'bands': 'VV (S1), VH (S1), S2ndvi (S2 NDVI)',\n",
    "        'temporal_resolution': '12-day composites',\n",
    "        'processing_software': 'Google Earth Engine + Python',\n",
    "        'fusets_ready': True\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_example_usage_script():\n",
    "    \"\"\"\n",
    "    Create a script showing how to use the exported data with FuseTS\n",
    "    \"\"\"\n",
    "    \n",
    "    script_content = '''\n",
    "# Example script to use GEE-exported data with FuseTS\n",
    "# Run this after downloading the exported data from Google Drive\n",
    "\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from fusets.mogpr import MOGPRTransformer\n",
    "from fusets.analytics import phenology\n",
    "from fusets import whittaker\n",
    "\n",
    "# Load the exported data\n",
    "# Option 1: If you exported as individual periods\n",
    "# data_files = ['S1_S2_2024_Period_01.tif', 'S1_S2_2024_Period_02.tif', ...]\n",
    "# data = combine_period_files(data_files)  # You'll need to implement this\n",
    "\n",
    "# Option 2: If you exported as single multi-band file\n",
    "data_path = 'S1_S2_TimeSeries_2024.tif'\n",
    "data = rioxarray.open_rasterio(data_path)\n",
    "\n",
    "# Convert to FuseTS format\n",
    "fusets_data = prepare_fusets_format(data)\n",
    "\n",
    "# Apply MOGPR fusion\n",
    "mogpr = MOGPRTransformer()\n",
    "fused_data = mogpr.fit_transform(fusets_data)\n",
    "\n",
    "# Extract phenological metrics\n",
    "phenology_metrics = phenology(fused_data['S2ndvi'])\n",
    "\n",
    "# Access results\n",
    "sos_times = phenology_metrics.da_sos_times\n",
    "eos_times = phenology_metrics.da_eos_times\n",
    "\n",
    "print(\"FuseTS processing completed!\")\n",
    "'''\n",
    "    \n",
    "    script_file = os.path.join(OUTPUT_DIR, 'fusets_processing_example.py')\n",
    "    with open(script_file, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    return script_file\n",
    "\n",
    "# Create example script\n",
    "example_script = create_example_usage_script()\n",
    "print(f\"Example FuseTS processing script created: {example_script}\")\n",
    "\n",
    "# If we have local data, prepare it for FuseTS\n",
    "if 'local_data' in locals() and local_data is not None:\n",
    "    print(\"\\nPreparing local data for FuseTS...\")\n",
    "    fusets_ready_data = prepare_fusets_format(local_data)\n",
    "    \n",
    "    # Save FuseTS-ready data\n",
    "    fusets_output = os.path.join(OUTPUT_DIR, f'S1_S2_timeseries_{YEAR}_fusets_ready.nc')\n",
    "    fusets_ready_data.to_netcdf(fusets_output)\n",
    "    print(f\"FuseTS-ready data saved to: {fusets_output}\")\n",
    "    \n",
    "    # Display data structure\n",
    "    print(\"\\nFuseTS-ready data structure:\")\n",
    "    print(fusets_ready_data)\n",
    "    \n",
    "    print(\"\\nThis data is now ready for the MOGPR fusion notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What this notebook accomplishes:\n",
    "\n",
    "1. **Temporal Strategy**: Creates exactly 31 periods of 12-day composites for 2024\n",
    "2. **Data Collection**: Extracts S1 (VV, VH) and S2 (NDVI) data from Google Earth Engine\n",
    "3. **Cloud Processing**: Uses GEE's computational power for large-scale data processing\n",
    "4. **Export Options**: Provides both individual period files and combined multi-band exports\n",
    "5. **Local Processing**: For small areas, extracts data directly without export/download\n",
    "6. **FuseTS Preparation**: Converts data to the exact format needed for MOGPR processing\n",
    "\n",
    "### Temporal Coverage:\n",
    "- **Period 1**: 2024-01-01 to 2024-01-12\n",
    "- **Period 2**: 2024-01-13 to 2024-01-25  \n",
    "- **Period 3**: 2024-01-26 to 2024-02-07\n",
    "- **...**\n",
    "- **Period 31**: 2024-12-18 to 2024-12-29\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download Data**: Monitor exports at https://code.earthengine.google.com/tasks\n",
    "2. **Load in FuseTS**: Use the exported GeoTIFF files with the MOGPR fusion notebook\n",
    "3. **Apply MOGPR**: Run the S1+S2 fusion using the prepared time series\n",
    "4. **Phenological Analysis**: Extract seasonal metrics from the fused data\n",
    "\n",
    "### File Outputs:\n",
    "- **Data**: S1_S2_TimeSeries_2024.tif (or individual period files)\n",
    "- **Metadata**: processing_metadata_2024.json\n",
    "- **Timeline**: period_timeline_2024.png\n",
    "- **Example Script**: fusets_processing_example.py\n",
    "\n",
    "The exported data is now ready for the FuseTS MOGPR processing workflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}